{"pages":[{"title":"关于","text":"网站使用​ 听到微笑的博客仅用于个人学习分享，不从事任何商业活动。 本站主要分享务实、能看懂、读者可复现 的原创文章。 版权声明 据我国法律，在因特网上发表的作品仍然受到同等的著作权保护。 本博客为编辑作品，整体著作权属于本人（听到微笑）所有。本博客许可他人建立友情链接，但链接所指向的内容应是本博客首页（https://www.bigcoder.cn）。 本博客为非营利性个人原创专业网站，除部分有明确署名的作品外，所刊登的所有作品的著作权均为本人（听到微笑）所拥有，本人（听到微笑）保留所有法定权利。任何个人或组织如欲转载本人在博客所刊登的作品，需遵守以下规定： 在因特网上转载，如转载人为非营利性网站，在遵守下列要求的前提下，可以不经本人同意而免费转载：保持作品完整性、署本人名（听到微笑）、在页面明显处标明转载来源（即本博客域名：https://www.bigcoder.cn）; 在因特网上转载，如转载人为营利性网站，可在遵守下列要求的前提下转载，并应支付相应稿酬：告知本人（可以使用Email、微博形式）、保持作品完整性、署本人名、在页面明显处标明转载来源（即本博客域名：https://www.bigcoder.cn）; 平面媒体（包括但不限于报纸、杂志、书籍、印刷品）则需在经本人同意（可以使用Email、微博形式）后转载或进行编辑后使用，并应署本人名、支付相应稿酬。 本博客中有明确署名的作品的著作权为署名人拥有。 留言如需留言可在下方评论区进行。 联系作者 Email：tianjindong0804@163.com 微博：https://weibo.com/HearingSmile","link":"/about.html"},{"title":"音乐欣赏","text":"","link":"/music.html"}],"posts":[{"title":"Object类中的clone方法","text":"在日常的编码中，我们获取大多数情况下都是通过new关键字来创建对象的，但是Object中提供了clone()方法，让我们复制一个已有对象的副本进行使用。 clone()方法的作用 克隆方法用于创建对象的拷贝，为了使用clone方法，类必须实现java.lang.Cloneable接口，如果没有实现Clonebale接口，调用父类的clone()方法时会抛出CloneNotSupportedException，Cloneable接口只是一个标识，和Serializable接口类似，接口中没有任何方法。 源码类似于下面这样： 123456protected Object clone() throws CloneNotSupportedException { if (!(this instanceof Cloneable)) { throw new CloneNotSupportedException(\"Class doesn't implement Cloneable\"); } return internalClone((Cloneable) this); } Object中的clone方法是protected，也就是说这个方法只能在子类内部调用，所以我们需要在子类中写一个public方法，调用Object的clone()，使这个方法暴露出来。 在克隆java对象的时候不会调用构造器。 java提供一种叫浅拷贝（shallow copy）的默认方式实现clone，创建好对象的副本后然后通过赋值拷贝内容，意味着如果你的类包含引用类型，那么原始对象和克隆都将指向相同的引用内容，这是很危险的，因为发生在可变的字段上任何改变将反应到他们所引用的共同内容上。为了避免这种情况，需要对引用的内容进行深度克隆。 浅拷贝12345678910public class User implements Cloneable{ private String name; private int age; private int[] arr=new int[10]; public Object clone() throws CloneNotSupportedException { return super.clone(); } //省略Getter和Setter...} 123456789101112131415public class TestClone { @Test public void testClone(){ User user = new User(); user.setName(\"zhangsan\"); user.setAge(18); user.getArr()[0]=12; try { User clone = (User) user.clone(); System.out.println(clone.getArr()==user.getArr());//返回true，表示两个引用都指向同一个数组 } catch (CloneNotSupportedException e) { e.printStackTrace(); } }} 深拷贝12345678910111213public class User implements Cloneable{ private String name; private int age; private int[] arr=new int[10]; public Object clone() throws CloneNotSupportedException { User clone = (User) super.clone(); clone.setName(new String(this.name)); clone.setArr(Arrays.copyOf(this.arr,this.arr.length)); return clone; } //省略Getter和Setter...} 12345678910111213141516public class TestClone { @Test public void testClone(){ User user = new User(); user.setName(\"zhangsan\"); user.setAge(18); user.getArr()[0]=12; try { User clone = (User) user.clone(); System.out.println(clone.getArr()==user.getArr());//返回false，表示两个引用分别指向两个不同的数组 } catch (CloneNotSupportedException e) { e.printStackTrace(); } }}","link":"/article/43547.html"},{"title":"深入解析LinkedHashMap","text":"LinkedHashMap是HashMap的一个子类，保存了记录的插入顺序，在用Iterator遍历LinkedHashMap时，先得到的记录肯定是先插入的，也可以在构造时带参数，按照访问次序排序。 12345678910public void test(){ Map&lt;String,String&gt; map=new LinkedHashMap&lt;&gt;(); map.put(\"a\",\"1\"); map.put(\"b\",\"2\"); map.put(\"c\",\"3\"); Set&lt;Map.Entry&lt;String, String&gt;&gt; entries = map.entrySet(); for (Map.Entry&lt;String, String&gt; entry : entries) { System.out.println(entry); }} 可以看到，通过遍历Entry发现LinkedHashMap是有序的。在上面的案例中我们展示了LinkedHashMap默认的顺序维持方式（维持插入的顺序），通过重载的构造函数，我们可以将LinkedHashMap设置为维持访问的顺序： 123456789101112public void test(){ Map&lt;String,String&gt; map=new LinkedHashMap&lt;&gt;(16,0.75f,true); map.put(\"a\",\"1\"); map.put(\"b\",\"2\"); map.put(\"c\",\"3\"); //获取b后，b节点就会移动到链表的尾部 map.get(\"b\"); Set&lt;Map.Entry&lt;String, String&gt;&gt; entries = map.entrySet(); for (Map.Entry&lt;String, String&gt; entry : entries) { System.out.println(entry); }} LinkedHashMap维持插入顺序的原理想要知道LinkedHashMap是如何维持插入顺序的，就需要从其内部类入手解决： 123456static class Entry&lt;K,V&gt; extends HashMap.Node&lt;K,V&gt; { Entry&lt;K,V&gt; before, after; Entry(int hash, K key, V value, Node&lt;K,V&gt; next) { super(hash, key, value, next); }} 可以看到，LinkedHashMap中Entry内部类继承与HashMap.Node内部类，LinkedHashMap.Entry类在HashMap.Node的基础上增加了两个指针：before、after。没错，LinkedHashMap就是采用双向链表来维持插入顺序的。LinkedHashMap也提供了两个字段来保存双向链表的头尾的引用。 12345678/** * The head (eldest) of the doubly linked list. */transient LinkedHashMap.Entry&lt;K,V&gt; head;/** * The tail (youngest) of the doubly linked list. */transient LinkedHashMap.Entry&lt;K,V&gt; tail; 如上图，我们依次插入A、B、C、D、E五个Entry，而每次插入时，我们都按照插入顺序维持一个双向链表。我们从head指针开始，顺着after指针走（也就是图中的红色箭头），就可以还原我们的插入顺序。 LinkedHashMap源码解析在了解完LinkedHashMap基本原理后，我们就来看看它的源码，我们先从它的构造器入手。 构造函数1234public LinkedHashMap(int initialCapacity,float loadFactor,boolean accessOrder) { super(initialCapacity, loadFactor); this.accessOrder = accessOrder;} 其中initialCapacity和loadFactor在 《JDK8HashMap源码深度解析》一文中详细介绍过了，这里不再赘述。需要注意的是accessOrder参数，它决定了LinkedHashMap的顺序维持策略，当accessOrder=true时，采用访问顺序维持模式，而accessOrder=false时采用插入顺序维持模式。 1234public LinkedHashMap() { super(); accessOrder = false;} 可以看到LinkedHashMap无参构造器，将accessOrder属性设置为了false。 put方法根据前面的描述知道了LinkedHashMap在插入Entry时会不断维持一个双向链表，那么我们有必要对put方法进行一些分析，需要注意的是LinkedHashMap并没实现自己的put方法，而是继承至HashMap的put方法。下面是HashMap中的put方法的源码： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182public V put(K key, V value) { return putVal(hash(key), key, value, false, true);}/*** 计算key的hash值，该hash算法调用了Obejct的hashcode* 返回的是key.hashCode()&amp;(key.hashCode()&gt;&gt;&gt;16),其中&gt;&gt;&gt;代表无符号右移**/static final int hash(Object key) { int h; return (key == null) ? 0 : (h = key.hashCode()) ^ (h &gt;&gt;&gt; 16);}final V putVal(int hash, K key, V value, boolean onlyIfAbsent, boolean evict) { Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; int n, i; //将Map内部的table数组赋给局部变量tab，如果table为空或者大小为0，则使用resize进行扩容 if ((tab = table) == null || (n = tab.length) == 0) n = (tab = resize()).length; /** * n-1&amp;hash的效果就是 hash%n （因为HashMap中封装的数组的长度都是2的幂(默认16)） * 如果数组对应位置没有元素（没有发生Hash冲突），则新建一个Node元素，放入该数组位置 */ if ((p = tab[i = (n - 1) &amp; hash]) == null) // 重点****************************************** tab[i] = newNode(hash, key, value, null); // 重点****************************************** /** * 发生Hash冲突后的处理 */ else { Node&lt;K,V&gt; e; K k; if (p.hash == hash &amp;&amp; ((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))) e = p; else if (p instanceof TreeNode) e = ((TreeNode&lt;K,V&gt;)p).putTreeVal(this, tab, hash, key, value); else { //如果此时解决Hash冲突的数据结构为链表，则遍历到链表尾部 for (int binCount = 0; ; ++binCount) { if ((e = p.next) == null) { //向链表中添加新元素 p.next = newNode(hash, key, value, null); //如果新元素未加入之前，链表长度大于等于7了则需要将链表转换为红黑树了，换句话说加入新元素后链表长度大于等于8了，就转成红黑树。 if (binCount &gt;= TREEIFY_THRESHOLD - 1) // -1 for 1st treeifyBin(tab, hash);//将链表转换为红黑树 //跳出循环 break; } //判断key是否相等 //这里的条件判断显示出HashMap允许一个key==null的键值对存储 if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) break; p = e; } } //如果找到了一个相同的key，则根据onlyIfAbsent判断是否需要替换旧的value。 //onlyIfAbsent为true时代表不替换原先元素。 if (e != null) { // existing mapping for key V oldValue = e.value; if (!onlyIfAbsent || oldValue == null) e.value = value; afterNodeAccess(e); return oldValue; } } //被修改的次数，fast-fail机制 ++modCount; //如果HashMap中存储的节点数量是否到达了扩容的阈值 if (++size &gt; threshold) //进行扩容 resize(); afterNodeInsertion(evict); return null;} 可能有人就糊涂了，既然是使用的父类的put方法，那么LinkedHashMap是如何维持双向链表的呢？实际上真正的玄机在第29行中，tab[i] = newNode(hash, key, value, null);掉用的是LinkedHashMap的newNode方法，就是在这个方法中实现了维持插入顺序的功能（不得不感叹设计的精妙）。 123456789101112131415161718192021Node&lt;K,V&gt; newNode(int hash, K key, V value, Node&lt;K,V&gt; e) { //创建Entry节点 LinkedHashMap.Entry&lt;K,V&gt; p = new LinkedHashMap.Entry&lt;K,V&gt;(hash, key, value, e); //将新增节点放在链表尾部 linkNodeLast(p); return p;}private void linkNodeLast(LinkedHashMap.Entry&lt;K,V&gt; p) { LinkedHashMap.Entry&lt;K,V&gt; last = tail; //将tail指针指向该元素（tail指针永远指向链表的尾部节点） tail = p; //原先的尾部节点若为空，则代表当前Map中没有存储数据，则将head指针也指向新增节点p if (last == null) head = p; else { //将before指针指向原先的队尾 p.before = last; //将原先队尾的next指针指向新增元素 last.after = p; }} get方法前面我们提到了accessOrder属性，如果accessOrder=true就会使得LinkedHashMap维持访问顺序，一说到访问那就肯定是get方法了，我们就来看看它是如何维持访问顺序的。LinkedHashMap实现了自己的get方法： 123456789public V get(Object key) { Node&lt;K,V&gt; e; if ((e = getNode(hash(key), key)) == null) return null; if (accessOrder) //如果accessOrder为true则将访问的元素移到双向链表的尾部 afterNodeAccess(e); return e.value;} 1234567891011121314151617181920212223242526272829303132void afterNodeAccess(Node&lt;K,V&gt; e) { // move node to last LinkedHashMap.Entry&lt;K,V&gt; last; // accessOrder=true且当前元素不处于链表的尾部 if (accessOrder &amp;&amp; (last = tail) != e) { LinkedHashMap.Entry&lt;K,V&gt; p = (LinkedHashMap.Entry&lt;K,V&gt;)e, b = p.before, a = p.after; // 因为马上要到链表尾部去了，所以要将当前元素的after指针置为空 p.after = null; if (b == null) //如果前一个节点为空，那么将头指针指向下一个节点 head = a; else //前一个节点不为空，那么将前一个节点的after指针指向下一个节点 b.after = a; if (a != null) //如果下一个节点不为空，则将下一个节点的before指针设为前一个节点 a.before = b; else //如果没有下一个节点，则将last指向前一个节点，实际上这一步正常情况下不会发生，因为前面已经验证了当前元素不是尾节点 last = b; if (last == null) head = p; else { //将当前元素插入链表尾部 p.before = last; last.after = p; } //此时当前元素已经移到了链表尾部，将tail指针指向当前元素 tail = p; //modeCount用于迭代器的快速失败机制（fail-fast） ++modCount; }} LinkedHashMap用途浅析​ 我们在使用缓存的时候，需要采用特定的缓存淘汰机制，而LRU（Least Recently Used 最近最少使用）淘汰机制也是最常使用的。它会淘汰最久没有使用过的缓存，而借助LinkedHashMap可以非常容易的实现这一策略： 12345678910111213141516import java.util.LinkedHashMap;import java.util.Map;public class LRUCache&lt;K, V&gt; extends LinkedHashMap&lt;K, V&gt; { private int maxEntries; public LRUCache(int maxEntries) { super(16, 0.75f, true); this.maxEntries = maxEntries; } @Override protected boolean removeEldestEntry(Map.Entry&lt;K, V&gt; eldest) { return size() &gt; maxEntries; }} 为什么重写父类的removeEldestEntry就能实现LRU策略呢？这仍然需要分析LinkedHashMap的源码，在该类中put方法（HashMap中的方法）会调用putVal()方法（HashMap的方法），而在putVal()方法的尾部会调用afterNodeInsertion()方法（LinkedHashMap中的方法），afterNodeInsertion方法就是淘汰策略的实现代码： 123456789101112/*** 可能移除最少使用的元素**/void afterNodeInsertion(boolean evict) { LinkedHashMap.Entry&lt;K,V&gt; first; //如果removeEldestEntry(first)返回true就会触发淘汰机制，淘汰的最久没有使用过的元素 if (evict &amp;&amp; (first = head) != null &amp;&amp; removeEldestEntry(first)) { K key = first.key; //删除双向链表的头节点 removeNode(hash(key), key, null, false, true); }}","link":"/article/59694.html"},{"title":"深入解析HashMap源码","text":"​ HashMap是Java程序员使用频率最高的用于映射(键值对)处理的数据类型。随着JDK（Java Developmet Kit）版本的更新，JDK1.8对HashMap底层的实现进行了优化，例如引入红黑树的数据结构和扩容的优化等。 ​ Java为数据结构中的映射定义了一个接口java.util.Map，此接口主要有四个常用的实现类，分别是HashMap、Hashtable、LinkedHashMap和TreeMap，类继承关系如下图所示： 下面针对各个实现类的特点做一些说明： (1) HashMap：它根据键的hashCode值存储数据，大多数情况下可以直接定位到它的值，因而具有很快的访问速度，但遍历顺序却是不确定的。 HashMap最多只允许一条记录的键为null，允许多条记录的值为null。HashMap非线程安全，即任一时刻可以有多个线程同时写HashMap，可能会导致数据的不一致。如果需要满足线程安全，可以用 Collections的synchronizedMap方法使HashMap具有线程安全的能力，或者使用ConcurrentHashMap。 (2) Hashtable：Hashtable是遗留类，很多映射的常用功能与HashMap类似，不同的是它承自Dictionary类，并且是线程安全的，任一时间只有一个线程能写Hashtable，并发性不如ConcurrentHashMap，因为ConcurrentHashMap引入了分段锁。Hashtable不建议在新代码中使用，不需要线程安全的场合可以用HashMap替换，需要线程安全的场合可以用ConcurrentHashMap替换。 (3) LinkedHashMap：LinkedHashMap是HashMap的一个子类，保存了记录的插入顺序，在用Iterator遍历LinkedHashMap时，先得到的记录肯定是先插入的，也可以在构造时带参数，按照访问次序排序。 (4) TreeMap：TreeMap实现SortedMap接口，能够把它保存的记录根据键排序，默认是按键值的升序排序，也可以指定排序的比较器，当用Iterator遍历TreeMap时，得到的记录是排过序的。如果使用排序的映射，建议使用TreeMap。在使用TreeMap时，key必须实现Comparable接口或者在构造TreeMap传入自定义的Comparator，否则会在运行时抛出java.lang.ClassCastException类型的异常。 对于上述四种Map类型的类，要求映射中的key是不可变对象。不可变对象是该对象在创建后它的哈希值不会被改变。如果对象的哈希值发生变化，Map对象很可能就定位不到映射的位置了。 HashMap的基本原理​ 如果小伙伴们对HashMap的基本原理还不熟悉，建议大家参考 漫画：什么是HashMap？一文。这里对基本原理进行简单的梳理： ​ HashMap是一个用于存储Key-Value键值对的集合，每一个键值对也叫做Entry。这些个键值对（Entry）分散存储在一个数组当中（这个数组被称为Hash桶数组），这个数组就是HashMap的基础。 ​ 那么我们如何将对象按照key的值放入到数组中去呢？这里我们就需要借助Hash函数，Hash函数可以将任何一个对象转换为一个int类型的值，但是这还不能将元素插入到数组中，因为数组的容量有限，而int类型的值非常大，我们不可能将int类型的Hash值与数组的元素一一对应，那么解决这种问题最简单的方法就是取模（%），将Hash值%数组容量就会得到一个不大于数组容量的一个数值，就可以将元素插入到数组中。 ​ 但是，因为HashMap中数组的长度是有限的，当插入的Entry越来越多时，再完美的Hash函数也难免会出现index冲突的情况（Hash冲突就是两个不同Entry计算出来的数组索引值相同）。那么解决Hash冲突最简单的方法采用拉链法，也就是将冲突的Entry以链表的形式存在。 ​ ​ 在JDK7之前，HashMap解决Hash冲突时都是采用拉链法解决的；而在JDK8开始，HashMap采用链表+红黑树相结合的方式解决的，具体实现原理在后序源码讲解中体现。 ​ 当HashMap中的数组中保存的Entry太多后，Hash冲突的可能性不断增大，这样会大大降低HashMap的执行效率，所以HashMap引入了扩容机制。这里就需要了解加载因子（负载因子）的概念，当元素个数大&gt;数组容量*加载因子时就会触发扩容，扩容时HashMap中的数组增加至原来两倍，将原Map中的数组重新散列到新的数组中去，至此完成了扩容操作。 HashMap源码分析HashMap中的常量值123456789101112131415161718192021222324/** * 默认的数组容量为16 */ static final int DEFAULT_INITIAL_CAPACITY = 1 &lt;&lt; 4; // aka 16 /** * 数组的最大容量。在后面我们会将每一个数组元素看成一个桶，因为数组元素在后面可能连接的是一个链表或者是一颗树。 */ static final int MAXIMUM_CAPACITY = 1 &lt;&lt; 30; /** * 默认的加载因子（负载因子）为0.75 */ static final float DEFAULT_LOAD_FACTOR = 0.75f; /** * 当链表长度到达TREEIFY_THRESHOLD时将链表转换为红黑树，也就是说链表最长为7。 */ static final int TREEIFY_THRESHOLD = 8; /** * . 桶的链表还原阈值：即 红黑树转为链表的阈值，当在扩容（resize（））时（此时HashMap的数据存储位置会重新计算），在重新计算存储位置后，当原有的红黑树内数量 &lt; 6时，则将 红黑树转换成链表 */ static final int UNTREEIFY_THRESHOLD = 6; ​ 上面部分常量值有可能不能完全理解，我们这里只做一些了解即可。 HashMap中几个关键字段1234int threshold; // 所能容纳的key-value对极限 (阈值)final float loadFactor; // 负载因子 int modCount; //用于迭代的快速失败机制int size; //HashMap中实际存在的键值对数量 threshold和loadFactor：Node[] table的初始化长度length(默认值是16)，Load factor为负载因子(默认值是0.75)，threshold是HashMap所能容纳的最大数据量的Node(键值对)个数。threshold = length * Load factor。也就是说，在数组定义好长度之后，负载因子越大，所能容纳的键值对个数越多。 size：这个字段其实很好理解，就是HashMap中实际存在的键值对数量。注意和table的长度length、容纳最大键值对数量threshold的区别。 modCount：这个字段主要用来记录HashMap内部结构发生变化的次数，主要用于迭代的快速失败。强调一点，内部结构发生变化指的是结构发生变化，例如put新键值对，但是某个key对应的value值被覆盖不属于结构变化 构造函数分析12345678910111213141516171819/** * @param initialCapacity HashMap初始化桶的个数（数组的容量） * @param loadFactor 加载因子小于0抛出异常 */ public HashMap(int initialCapacity, float loadFactor) { //如果initialCapacity小于0抛出异常 if (initialCapacity &lt; 0) throw new IllegalArgumentException(\"Illegal initial capacity: \" + initialCapacity); //如果initialCapacity大于最大容量，则将初始化容量设为最大值 if (initialCapacity &gt; MAXIMUM_CAPACITY) initialCapacity = MAXIMUM_CAPACITY; //当加载因子小于等于0 if (loadFactor &lt;= 0 || Float.isNaN(loadFactor)) throw new IllegalArgumentException(\"Illegal load factor: \" + loadFactor); this.loadFactor = loadFactor; //如果用户传入的initialCapacity值不是2的幂，那么返回一个不小于它的2的幂 this.threshold = tableSizeFor(initialCapacity); } ​ 构造函数中传入了initialCapacity，loadFactor。 ​ initialCapacity表示HashMap中桶的个数。 ​ loadFactor是HashMap的加载因子（负载因子），负载因子决定HashMap的扩容时机，如果HashMap中存储的Entry数量大于等于 当前数组容量*加载因子时就会触发数组的扩容操作。举例来说：假如当前数组容量为16，加载因子为0.75，那么此时数组中最大的元素个数为16*0.75=12。 tableSizeFor方法的作用​ 在构造函数中调用了一个名为tableSizeFor方法，它将用户传入的初始容量转换为一个不小该值的2的幂值。例如：传入initialCapacity=11，他返回的是initialCapacity=16。这样做的目的到底是为什么呢？ ​ 前面说到了元素定位时会使用取模运算，实际上这种算法效率非常低，为了实现更加高效的算法，HashMap采用位运算的方式： ​ index = HashCode（Key） &amp; （Length - 1） 下面我们以值为“book”的Key来演示整个过程： 计算book的hashcode，结果为十进制的3029737，二进制的101110001110101110 1001。 假定HashMap长度是默认的16，计算Length-1的结果为十进制的15，二进制的1111。 把以上两个结果做与运算，101110001110101110 1001 &amp; 1111 = 1001，十进制是9，所以 index=9。 采用位运算得到的结果与取模运算的效果完全相同，但是这样做的前提就是Length必须是2的幂。这也就是为什么HashMap会定义tableSizeFor方法返回容量的2次幂值。 静态内部类12345678910111213141516171819202122232425262728293031323334353637383940// Node实现了Map.Entry&lt;K,V&gt;接口，也就是说Node实际上就是我们前面提到的Entrystatic class Node&lt;K,V&gt; implements Map.Entry&lt;K,V&gt; { final int hash;//保存当前Entry key的hash值，由于后面会多次使用该Hash值，避免重复计算 final K key; V value; Node&lt;K,V&gt; next;//指向链表下一个节点 Node(int hash, K key, V value, Node&lt;K,V&gt; next) { this.hash = hash; this.key = key; this.value = value; this.next = next; } public final K getKey() { return key; } public final V getValue() { return value; } public final String toString() { return key + \"=\" + value; } public final int hashCode() { return Objects.hashCode(key) ^ Objects.hashCode(value); } public final V setValue(V newValue) { V oldValue = value; value = newValue; return oldValue; } public final boolean equals(Object o) { if (o == this) return true; if (o instanceof Map.Entry) { Map.Entry&lt;?,?&gt; e = (Map.Entry&lt;?,?&gt;)o; if (Objects.equals(key, e.getKey()) &amp;&amp; Objects.equals(value, e.getValue())) return true; } return false; }} ​ 看完构造函数和类的常量值以及Entry的结构后，我们就以HashMap中put方法入手，看看HashMap中的底层原理。 put方法源码1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980public V put(K key, V value) { return putVal(hash(key), key, value, false, true); }/*** 计算key的hash值，该hash算法调用了Obejct的hashcode* 返回的是key.hashCode()&amp;(key.hashCode()&gt;&gt;&gt;16),其中&gt;&gt;&gt;代表无符号右移**/static final int hash(Object key) { int h; return (key == null) ? 0 : (h = key.hashCode()) ^ (h &gt;&gt;&gt; 16); } final V putVal(int hash, K key, V value, boolean onlyIfAbsent, boolean evict) { Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; int n, i; //将Map内部的table数组赋给局部变量tab，如果table为空或者大小为0，则使用resize进行扩容 if ((tab = table) == null || (n = tab.length) == 0) n = (tab = resize()).length; /** * n-1&amp;hash的效果就是 hash%n （因为HashMap中封装的数组的长度都是2的幂(默认16)） * 如果数组对应位置没有元素（没有发生Hash冲突），则新建一个Node元素，放入该数组位置 */ if ((p = tab[i = (n - 1) &amp; hash]) == null) tab[i] = newNode(hash, key, value, null); /** * 发生Hash冲突后的处理 */ else { Node&lt;K,V&gt; e; K k; if (p.hash == hash &amp;&amp; ((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))) e = p; else if (p instanceof TreeNode) e = ((TreeNode&lt;K,V&gt;)p).putTreeVal(this, tab, hash, key, value); else { //如果此时解决Hash冲突的数据结构为链表，则遍历到链表尾部 for (int binCount = 0; ; ++binCount) { if ((e = p.next) == null) { //向链表中添加新元素 p.next = newNode(hash, key, value, null); //如果新元素未加入之前，链表长度大于等于7了则需要将链表转换为红黑树了，换句话说加入新元素后链表长度大于等于8了，就转成红黑树。 if (binCount &gt;= TREEIFY_THRESHOLD - 1) // -1 for 1st treeifyBin(tab, hash);//将链表转换为红黑树 //跳出循环 break; } //判断key是否相等 //这里的条件判断显示出HashMap允许一个key==null的键值对存储 if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) break; p = e; } } //如果找到了一个相同的key，则根据onlyIfAbsent判断是否需要替换旧的value。 //onlyIfAbsent为true时代表不替换原先元素。 if (e != null) { // existing mapping for key V oldValue = e.value; if (!onlyIfAbsent || oldValue == null) e.value = value; afterNodeAccess(e); return oldValue; } } //被修改的次数，fast-fail机制 ++modCount; //如果HashMap中存储的节点数量是否到达了扩容的阈值 if (++size &gt; threshold) //进行扩容 resize(); afterNodeInsertion(evict); return null; } resize方法123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105/** * 初始化Map中的Node数组，如果已经初始化则进行扩容操作 */ final Node&lt;K,V&gt;[] resize() { Node&lt;K,V&gt;[] oldTab = table; //获取数组的原始大小 int oldCap = (oldTab == null) ? 0 : oldTab.length; //获取原始阈值 int oldThr = thresholds; //用于记录新容量和新阈值 int newCap, newThr = 0; //如果原始容量大于0，则代表当前Map已经初始化过了，则应该进行扩容操作 if (oldCap &gt; 0) { if (oldCap &gt;= MAXIMUM_CAPACITY) { threshold = Integer.MAX_VALUE; return oldTab; } //将新的容量设为原先的两倍（oldCap&lt;&lt;1） else if ((newCap = oldCap &lt;&lt; 1) &lt; MAXIMUM_CAPACITY &amp;&amp; oldCap &gt;= DEFAULT_INITIAL_CAPACITY) //将阈值也设为原先的两倍 newThr = oldThr &lt;&lt; 1; // double threshold } // 初始化数组走这里 // 如果构造函数定义了数组初始容量 else if (oldThr &gt; 0) // initial capacity was placed in threshold newCap = oldThr; // 如果构造函数没有定义初始容量 else { // zero initial threshold signifies using defaults //初始化数组容量（16） newCap = DEFAULT_INITIAL_CAPACITY; //初始化阈值（16*0.75 = 12） newThr = (int)(DEFAULT_LOAD_FACTOR * DEFAULT_INITIAL_CAPACITY); } // 构造函数中定义了初始容量 在这里计算阈值 if (newThr == 0) { float ft = (float)newCap * loadFactor; newThr = (newCap &lt; MAXIMUM_CAPACITY &amp;&amp; ft &lt; (float)MAXIMUM_CAPACITY ? (int)ft : Integer.MAX_VALUE); } // 阈值 threshold = newThr; @SuppressWarnings({\"rawtypes\",\"unchecked\"}) //创建一个新的数组，可用于初始化，也可用于扩容 Node&lt;K,V&gt;[] newTab = (Node&lt;K,V&gt;[])new Node[newCap]; table = newTab; if (oldTab != null) { //遍历旧数组，将就数组中的元素，重新散列到新数组中去 for (int j = 0; j &lt; oldCap; ++j) { //e表示数组上的节点 Node&lt;K,V&gt; e; if ((e = oldTab[j]) != null) { //把头节点置为空 oldTab[j] = null; //当就数组中头元素没有链表子节点时，直接散列该元素 if (e.next == null) newTab[e.hash &amp; (newCap - 1)] = e; //如果数组头元素下面还有其他子节点，且子节点是树结构 else if (e instanceof TreeNode) ((TreeNode&lt;K,V&gt;)e).split(this, newTab, j, oldCap); //如果是一个链表 else { // preserve order Node&lt;K,V&gt; loHead = null, loTail = null; Node&lt;K,V&gt; hiHead = null, hiTail = null; Node&lt;K,V&gt; next; do { next = e.next; // 还是原来的索引值 if ((e.hash &amp; oldCap) == 0) { if (loTail == null) loHead = e; else loTail.next = e; loTail = e; } // 不是原来的索引值了 需要迁移 else { if (hiTail == null) hiHead = e; else hiTail.next = e; hiTail = e; } } while ((e = next) != null); if (loTail != null) { //将尾节点的next指针置为空 loTail.next = null; //将链接的不移动链表放到原索引位置 newTab[j] = loHead; } if (hiTail != null) { hiTail.next = null; //将链接的移动链表放到新的索引位置 newTab[j + oldCap] = hiHead; } } } } } return newTab; } 从66行开始代码进入了链表元素的迁移工作，loHead和loTail两个节点分别记录不需要移动的链表的头部和尾部，hiHead和hiTail分别记录需要移动的链表头部和尾部。 假设在扩容的时候某个数组下有这样一个链表 : 其中，假设天蓝色部分的不需要挪动，红色部分的需要挪动 第一步 : 建立loHead loTail hiHead hiTail四个节点 第二步 ： 第三步 : …第N步： 总结下图是美团技术团队的put函数的流程总结 ： 参考文章： 漫画：什么是HashMap HashMap源码分析——put和get（一） 美团HashMap技术博客 红黑联盟，Java类集框架之HashMap(JDK1.8)源码剖析，2015 CSDN博客频道， 教你初步了解红黑树，2010","link":"/article/2006.html"},{"title":"推荐一款过滤百度广告的神器","text":"打开度娘，随便搜索一个关键字，得到的搜索结果第一页基本上都是百度推广的广告，所以博主平时搜索基本上都是使用Google或者必应，Google由于国内网络环境原因不能访问，所以今天推荐一个过滤百度广告推广的插件，TamperMonkey（油泼猴）。 话不多说，先上一个效果图 安装TamperMonkey插件及去广告脚本TamperMonkey插件可以运行在市面上Google系和Firefox系浏览器上的，而绝大多数国产浏览器（360，UC等）都是采用谷歌浏览器的内核，所以理论上来说，这些国产浏览器都可以使用TamperMonkey插件，本文以Firefox浏览器为例，讲解安装流程： 第一步：打开浏览器扩展中心 第二步：搜索并安装TamperMonkey插件 第三步：安装去广告脚本进入https://greasyfork.org/zh-CN/scripts，下载并安装AC-baidu:重定向优化百度搜狗谷歌搜索_去广告_favicon_双列脚本： 脚本的卸载 到这里我们去广告插件就安装完成了。其实在https://greasyfork.org/zh-CN/scripts还有很多功能强大的脚本，有兴趣的小伙伴可以自己安装使用哦。","link":"/article/58628.html"},{"title":"事务的隔离级别（转）","text":"前面我们介绍了事务在并发情况下会出现读问题的情况 《事务并发所带来的问题》，而数据库事务的隔离级别就是用来解决相应读问题而产生的的。事务的隔离级别有4种，由低到高分别为Read uncommitted 、Read committed 、Repeatable read 、Serializable 。而且，在事务的并发操作中可能会出现脏读，不可重复读，幻读。下面通过事例一一阐述它们的概念与联系。 读未提交（Read uncommitted）读未提交，顾名思义，就是一个事务可以读取另一个未提交事务的数据。 事例：老板要给程序员发工资，程序员的工资是3.6万/月。但是发工资时老板不小心按错了数字，按成3.9万/月，但是事务还没有提交，就在这时，程序员去查看自己这个月的工资，发现比往常多了3千元，以为涨工资了非常高兴。但是老板及时发现了不对，马上回滚差点就提交了的事务，将数字改成3.6万再提交。 分析：实际程序员这个月的工资还是3.6万，但是程序员看到的是3.9万。他看到的是老板还没提交事务时的数据。这就是脏读。 读已提交（Read committed）读提交，顾名思义，就是一个事务要等另一个事务提交后才能读取数据。 事例：程序员拿着信用卡去享受生活（卡里当然是只有3.6万），当他埋单时（程序员事务开启），收费系统事先检测到他的卡里有3.6万，就在这个时候！！程序员的妻子要把钱全部转出充当家用，并提交。当收费系统准备扣款时，再检测卡里的金额，发现已经没钱了（第二次检测金额当然要等待妻子转出金额事务提交完）。程序员就会很郁闷，明明卡里是有钱的… 分析：这就是读提交，若有事务对数据进行更新（UPDATE）操作时，读操作事务要等待这个更新操作事务提交后才能读取数据，可以解决脏读问题。但在这个事例中，出现了一个事务范围内两个相同的查询却返回了不同数据，这就是不可重复读。 可重复读（Repeatable read）重复读，就是在开始读取数据（事务开启）时，不再允许修改操作. 事例：程序员拿着信用卡去享受生活（卡里当然是只有3.6万），当他埋单时（事务开启，不允许其他事务的UPDATE修改操作），收费系统事先检测到他的卡里有3.6万。这个时候他的妻子不能转出金额了。接下来收费系统就可以扣款了。 分析：重复读可以解决不可重复读问题。写到这里，应该明白的一点就是，不可重复读对应的是修改，即UPDATE操作。但是可能还会有幻读问题。因为幻读问题对应的是插入INSERT操作，而不是UPDATE操作。 序列化（Serializable）Serializable 是最高的事务隔离级别，在该级别下，事务串行化顺序执行，可以避免脏读、不可重复读与幻读。但是这种事务隔离级别效率低下，比较耗数据库性能，一般不使用。 脏读 不可重复读 幻读 Read uncommitted（读未提交） √ √ √ Read committed（读已提交） × √ √ Repeatable read（可重复读） × × √ Serializable（序列化） × × × 值得一提的是：大多数数据库默认的事务隔离级别是Read committed，比如SQL Server , Oracle。Mysql的默认隔离级别是Repeatable read。 本文转载至：https://www.cnblogs.com/ubuntu1/p/8999403.html","link":"/article/37211.html"},{"title":"事务并发所带来的问题（转）","text":"一个数据库可能拥有多个访问客户端，这些客户端都可以并发方式访问数据库。数据库中的相同数据可能同时被多个事务访问，如果没有采取必要的隔离措施，就会导致各种并发问题，破坏数据的完整性。这些问题可以归结为5类，包括3类数据读问题（ 脏读、 不可重复读和 幻读）以及2类数据更新问题（ 第一类丢失更新和 第二类丢失更新）。下面，我们分别通过实例讲解引发问题的场景。 脏读（dirty read）​ A事务读取B事务尚未提交的更改数据，并在这个数据的基础上操作。如果恰巧B事务回滚，那么A事务读到的数据根本是不被承认的。来看取款事务和转账事务并发时引发的脏读场景： 在这个场景中，B希望取款500元而后又撤销了动作，而A往相同的账户中转账100元，就因为A事务读取了B事务尚未提交的数据，因而造成账户白白丢失了500元。 不可重复读（unrepeatable read）不可重复读是指 A事务读取了B事务已经提交的更改数据。假设A在取款事务的过程中，B往该账户转账100元，A两次读取账户的余额发生不一致： 在同一事务中，T4时间点和T7时间点读取账户存款余额不一样。 幻读（phantom read）A事务读取B事务提交的新增数据，这时A事务将出现幻读的问题。幻读一般发生在计算统计数据的事务中，举一个例子，假设银行系统在同一个事务中，两次统计存款账户的总金额，在两次统计过程中，刚好新增了一个存款账户，并存入100元，这时，两次统计的总金额将不一致： 幻读和不可重复读是两个容易混淆的概念，前者是指读到了其他已经提交事务的新增数据，而后者是指读到了已经提交事务的更改数据（更改或删除），为了避免这两种情况，采取的对策是不同的，防止读取到更改数据，只需要对操作的数据添加行级锁，阻止操作中的数据发生变化，而防止读取到新增数据，则往往需要添加表级锁——将整个表锁定，防止新增数据（Oracle使用多版本数据的方式实现）。 第一类丢失更新A事务撤销时，把已经提交的B事务的更新数据覆盖了。这种错误可能造成很严重的问题，通过下面的账户取款转账就可以看出来： A事务在回滚时，“不小心”将B事务已经转入账户的金额给抹去了。 第二类丢失更新A事务覆盖B事务已经提交的数据，造成B事务所做操作丢失： 上面的例子里由于支票转账事务覆盖了取款事务对存款余额所做的更新，导致银行最后损失了100元，相反如果转账事务先提交，那么用户账户将损失100元。 本文转载至：https://blog.csdn.net/dingguanyi/article/details/80888441","link":"/article/34137.html"},{"title":"单例模式","text":"单例模式，是一种常用的软件设计模式。在它的核心结构中只包含一个被称为单例的特殊类。通过单例模式可以保证系统中，应用该模式的一个类只有一个实例。即一个类只有一个对象实例。 饿汉式所谓的饿汉式和懒汉式所指的是单例对象的创建时机，饿汉式是在这个单例类被加载进JVM时就创建单例对象；而懒汉式的单例类即便是被加载进JVM后也不会创建单例对象，而是在用户第一获取单例对象时才创建单例对象。 12345678910111213141516/** * 饿汉式 * * 缺点：只要这个类被加载进JVM，那么这个单例对象就会被创建，在很多时候我们需要在第一次使用时创建这个单例对象，从而节省内存开销 **/public class Singleton1 { private static Singleton1 instance = new Singleton1(); public static Singleton1 getInstance() { return instance; } private Singleton1() { }} 懒汉式1234567891011121314151617/** * 懒汉式 * 缺点，在并发环境下，线程不安全 **/public class Singleton2 { private static Singleton2 instance = null; private Singleton2() { } public static Singleton2 getInstance() { if (instance == null) { instance=new Singleton2(); } return instance; }} 这是懒汉式单例最简单的写法，但是在生产过程中不会使用，因为上述代码存在线程安全问题。 在上图中，如果两个线程同时调用getInstance方法,由于线程调度问题有可能两个线程都能通过if( instance==null )的校验，此时两个线程都能创建一个实例这就破坏了单例规则。 线程安全版12345678910111213141516171819202122232425/** * 懒汉式(方法同步) * 优点：解决了Singleton2中线程不安全的问题 * 缺点：由于使用synchronized的关键字，所以性能不佳 **/public class Singleton3 { private static Singleton3 instance = null; private Singleton3() { } public static synchronized Singleton3 getInstance() { if (instance == null) { instance=new Singleton3(); } return instance; } public static synchronized Singleton3 getInstance() { if (instance == null) { instance=new Singleton3(); } return instance; }} 静态内部类形式在懒汉式中虽然解决了饿汉式中内存占用问题，但是由于需要保证线程安全需要用synchronized关键字修饰，这就导致这种方式的单例模式在性能上达不到最佳，那么我们利用静态内部类的加载机制，可以达到懒汉式的效果，并且不用担心线程安全问题。 1234567891011121314151617181920/** * 静态内部类完成单例模式 * * 优点：既保证了线程安全，又解决了性能问题 * * 在类加载器加载Singleton4时并不会加载LazyHolder内部类，只有在调用getInstance * 方法时才会加载LazyHolder类，从而创建单例的Singleton4 **/public class Singleton4 { private static Singleton4 instance = null; private static class LazyHolder { private static final Singleton4 singleton4 = new Singleton4(); } private Singleton4() { } public static Singleton4 getInstance() { return LazyHolder.singleton4; }}","link":"/article/46132.html"},{"title":"委派模式","text":"在常用的23种设计模式中其实面没有委派模式（delegate）的影子，但是在Spring中委派模式确实用的比较多的一种模式。在spring中的体现：Spring MVC框架中的DispatcherServlet其实就用到了委派模式。 委派模式的作用： 基本作用就是负责任务的调用和分配任务，跟代理模式很像，可以看做是一种特殊情况下的静态代理的全权代理，但是代理模式注重过程，而委派模式注重结果。 如何理解委派模式 在上图中，项目经理将一个任务分发给具体的执行者去执行（委派），但是BOSS是不关注这个过程，BOSS只关心在规定时间内项目经理能否完成，所以委派模式注重的是结果而不是过程。 委派模式Demo定义任务执行的接口： 123public interface Executer { void execute(String command);} 执行者A： 123456public class ExecuterA implements Executer { @Override public void execute(String command) { System.out.println(\"执行者A执行：\"+command); }} 执行者B: 123456public class ExecuterB implements Executer { @Override public void execute(String command) { System.out.println(\"执行者B执行：\"+command); }} 模拟项目经理分配任务： 1234567891011public class Leader implements Executer { private Map&lt;String, Executer&gt; targets = new HashMap&lt;&gt;(); public Leader() { targets.put(\"修改密码\",new ExecuterA()); targets.put(\"登录\",new ExecuterB()); } @Override public void execute(String command) { targets.get(command).execute(command); }} 模拟BOSS给项目经理下达任务： 123456789public class Boss { public static void main(String[] args) { Leader leader = new Leader(); //BOSS为项目经理布置任务（Leader），然而项目经理不会具体去执行，而是将任务分配给执行者。 // 这就是典型的“干活是你的，功劳是我的（站在Leader的角度）” leader.execute(\"登录\"); leader.execute(\"修改密码\"); }} 总结委派模式最关键的就是委派类的编写，也就是上面的Leader类。委派类负责任务的调用和分配任务。","link":"/article/29971.html"},{"title":"策略模式","text":"在软件领域中，设计模式作为一种经典的开发实践常常需要我们去深入的理解，而策略模式作为设计模式的一种，使用频率也是相对来说比较高的，在Java中，Collections类的sort方法就是策略模式的典型应用。 策略模式的思想其思想是针对一组算法，将每一种算法都封装到具有共同接口的独立的类中，从而是它们可以相互替换。策略模式的最大特点是使得算法可以在不影响客户端的情况下发生变化，从而改变不同的功能。 策略模式的组成 抽象策略角色：这个是一个抽象的角色，通常情况下使用接口或者抽象类去实现。对比来说，就是我们的Comparator接口。 具体策略角色：包装了具体的算法和行为。对比来说，就是实现了Comparator接口的实现一组实现类。 环境角色：内部会持有一个抽象角色的引用，给客户端调用。 示例定义抽象策略角色123public interface Operation { int doOperation(int a, int b);} 定义具体策略角色加法策略 123456public class AddOperation implements Operation { @Override public int doOperation(int a, int b) { return a + b; }} 减法策略 123456public class SubtractionOperation implements Operation { @Override public int doOperation(int a, int b) { return a - b; }} 定义环境角色1234567891011121314/*** 该类用于进行运算操作，传入不同的算术策略得到不同的结果**/public class Arithmetics { private Operation operation; public Arithmetics(Operation oper) { this.operation = oper; } public int doOperation(int a, int b) { return operation.doOperation(a, b); }} 测试123456789101112public class StrategyTest { public static void main(String[] args) { int a = 5, b = 3; //做加法 Arithmetics addOper = new Arithmetics(new AddOperation()); System.out.println(addOper.doOperation(a, b)); //做减法 Arithmetics subtractionOper = new Arithmetics(new SubtractionOperation()); System.out.println(subtractionOper.doOperation(a, b)); }} 总结意图：定义一系列的算法,把它们一个个封装起来, 并且使它们可相互替换。 主要解决：在有多种算法相似的情况下，使用 if…else 所带来的复杂和难以维护。 何时使用：一个系统有许多许多类，而区分它们的只是他们直接的行为。","link":"/article/58639.html"},{"title":"模板模式","text":"​ 在面向对象程序设计过程中，程序员常常会遇到这种情况：设计一个系统时知道了算法所需的关键步骤，而且确定了这些步骤的执行顺序，但某些步骤的具体实现还未知，或者说某些步骤的实现与具体的环境相关。 模板模式生活中的体现​ 在日常生活中，我们去银行办理业务一般要经过以下4个流程：取号、排队、办理具体业务、对银行工作人员进行评分等，其中取号、排队和对银行工作人员进行评分的业务对每个客户是一样的，可以在父类中实现，但是办理具体业务却因人而异，它可能是存款、取款或者转账等，可以延迟到子类中实现。 ​ 这样的例子在生活中还有很多，例如，一个人每天会起床、吃饭、做事、睡觉等，其中“做事”的内容每天可能不同。我们把这些规定了流程或格式的实例定义成模板，允许使用者根据自己的需求去更新它，例如，简历模板、论文模板、Word 中模板文件等。 模板模式的代码示例在下列代码中我们将模拟”奶茶”和“咖啡”的冲泡过程，我们冲泡饮品的过程可以简化为一个公有的模板： 第一步：将原料放入杯中 第二步：导入开水进行冲泡 第三步：放入辅料 定义饮料模板类：1234567891011121314151617181920212223242526public abstract class BeverageTemplate { /** * 饮料冲泡的具体流程 **/ public final void createBeverage() { step1(); step2(); step3(); } /** * 将原料放入杯中（由于不同的饮料需要放入的原料不一样，所以这个步骤我们放在子类中执行） */ public abstract void step1(); public void step2() { System.out.println(\"将开水导倒入杯中冲泡\"); } /** * 加入辅料（由于不同的饮料需要放入的辅料不一样，所以这个步骤我们放在子类中执行） */ public abstract void step3();} 定义奶茶类：1234567891011public class Cofee extends BeverageTemplate { @Override public void step1() { System.out.println(\"准备好咖啡和方糖\"); } @Override public void step3() { System.out.println(\"加入方糖\"); }} 定义咖啡类：1234567891011public class MilkTea extends BeverageTemplate { @Override public void step1() { System.out.println(\"将奶茶粉放入杯中\"); } @Override public void step3() { System.out.println(\"将珍珠放入杯中\"); }} 测试：123456789public class TemplatePatternTest { public static void main(String[] args) { //我们冲泡一杯咖啡 new Cofee().createBeverage(); //冲泡一杯奶茶 new MilkTea().createBeverage(); }} 总结模板模式的意图：定义一个操作中的算法的骨架，而将一些步骤延迟到子类中。模板方法使得子类可以不改变一个算法的结构即可重定义该算法的某些特定步骤。 主要解决：一些方法通用，却在每一个子类都重新写了这一方法。","link":"/article/61129.html"},{"title":"原型模式","text":"原型模式（Prototype Pattern）是用于创建重复的对象，同时又能保证性能。这种类型的设计模式属于创建型模式，它提供了一种创建对象的最佳方式。 原型模式的意义假如我们右一个Person类: 12345678public class Person implements Cloneable { private String name; private int age; private Date birthday; //省略Getter和Setter...} 在很多时候，我们创建一个对象需要给这个对象赋予很多属性，但是每次我们创建的时候都需要赋值一个属性： 12345678public class Test{ public static void main(String[] args){ Person person = new Person(); person.setName(\"张三\"); person.setAge(18); person.setBirthday(new Date()); }} 我们每次创建这个对象的时候都需要手动赋值这三个属性，但是如果这个类的属性有50个，那我们就需要进行50次操作才能创建一个操作，但如果这50个属性中绝大多数属性都是公用的，那么此时我们很多操作就是无用的，我们就可以使用原型模式来解决这个问题。 原型模式核心思想：我们创建一个原型对象，然后克隆该原型对象则可得到一个新的对象。 示例1234567891011121314151617181920212223242526272829303132public class Person implements Cloneable { private String name; private int age; private Date birthday; private static PersonPrototype instance; static { instance = new PersonPrototype(); instance.setName(\"张三\"); instance.setAge(18); instance.setBirthday(new Date()); } @Override protected Object clone() throws CloneNotSupportedException { return super.clone(); } //获取原型对象的克隆体 public static PersonPrototype getClone() { try { return ((PersonPrototype) instance.clone()); } catch (CloneNotSupportedException e) { e.printStackTrace(); } return null; } //省略Getter和Setter...} 测试： 1234567public class PrototypeTest { public static void main(String[] args) { //这里获取就是原型对象的一个克隆 PersonPrototype clone = PersonPrototype.getClone(); System.out.println(clone.getBirthday()); }} 引出浅克隆与深克隆在使用clone()方法的时候我们需要注意浅克隆与深克隆，从clone的本质来说，克隆就是对属性值的一种直接拷贝，对于基本数据类型来说这没有任何问题，但是对于引用类型来说，由于值拷贝，所以只是复制了对象的引用，而并没有克隆属性引用的对象。 具体如何达到深克隆的效果可以参考博主的另一篇博文：《Object类中clone方法的使用》 总结原型模式的意图：用原型实例指定创建对象的种类，并且通过拷贝这些原型创建新的对象。 何时使用： 当一个系统应该独立于它的产品创建，构成和表示时。 当要实例化的类是在运行时刻指定时，例如，通过动态装载。 为了避免创建一个与产品类层次平行的工厂类层次时。 当一个类的实例只能有几个不同状态组合中的一种时。建立相应数目的原型并克隆它们可能比每次用合适的状态手工实例化该类更方便一些。","link":"/article/22550.html"},{"title":"cglib代理","text":"​ 在此之前，我们学习了JDK动态代理，而JDK动态代理有一定的局限性，因为使用JDK动态代理时，被代理类必须实现接口，然后动态代理生成的代理类同时实现该接口实现代理模式，但在特定情况下没办法让被代理类实现接口，那么此时我们就需要使用cglib代理。 代理模式的三要素 两个成员：被代理对象、执行者（类似于Spring中切面的概念） 使用场景：当某件事情不方便自己做，但是必须要做时使用代理模式。 代理对象持有被代理对象的引用。 ​ 在第一点中，执行者指的是代理对象的执行模板，例如在JDK动态代理中，实现InvocationHandler接口的类就是代理类中方法的执行模板。而在cglib代理中执行模板需要实现MethodInterceptor。 使用cglib需要做的准备JDK动态代理由于是JDK自带的，所以我们不需要在项目中引入第三方jar，但是cglib需要引入两个jar包： cglib代理具体实例创建被代理类12345678910111213package _6代理模式.CGlib代理;public class UserService { public void addUser(){ System.out.println(\"添加用户\"); } public void deleteUser() { System.out.println(\"删除用户\"); }} 创建执行者在JDK动态代理中，执行者需要继承InvocationHandler接口；而在cglib中需要实现MethodInterceptor接口。 12345678910111213141516171819202122232425262728293031323334package _6代理模式.CGlib代理;import net.sf.cglib.proxy.MethodInterceptor;import net.sf.cglib.proxy.MethodProxy;import java.lang.reflect.Method;/** * 执行者 */public class Executent implements MethodInterceptor { /** * * @param o 代表代理对象本身，可以它调用代理对象的其他方法 * @param method 代理对象对应方法的字节码对象 * @param objects 传入用户调用“代理对象”对应方法的参数数组 * @param methodProxy 被代理对象方法的引用（通过它调用父类方法，从而达到代理的效果） * @return * @throws Throwable */ @Override public Object intercept(Object o, Method method, Object[] objects, MethodProxy methodProxy) throws Throwable { System.out.println(\"开启事务\"); Object result= null; try { result = methodProxy.invokeSuper(o,objects); } catch (Throwable throwable) { throwable.printStackTrace(); System.out.println(\"回滚事务\"); } System.out.println(\"提交事务\"); return result; }} 通过cglib生成代理对象123456789101112public class TestCglib { public static void main(String[] args) { Enhancer enhancer = new Enhancer(); //设置父类 enhancer.setSuperclass(UserService.class); //设置执行者 enhancer.setCallback(new Executent()); //创建代理对象 UserService userService = (UserService) enhancer.create(); userService.addUser(); }} 执行结果：","link":"/article/57476.html"},{"title":"JDK动态代理的深入理解","text":"​ 代理模式是应用框架中经常使用的一种模式，动态代理是AOP（面向切面编程）思想的一种重要的实现方式，在我们常用的框架中也经常遇见代理模式的身影，例如在Spring中事务管理就运用了动态代理，它将Service层原先应该进行的事务管理交给了Spring框架，大大简化了开发流程。在Hibernate中对象的懒加载模式，也运用了JDK的动态代理以及cglib代理。 静态代理​ 在说动态代理之前，我们需要先了解一下静态代理。 ​ 静态代理通常用于对原有业务逻辑的扩充。比如持有第三方jar包中的某个类，并调用了其中的某些方法。然后出于某种原因，比如记录日志、打印方法执行时间，但是又不好将这些逻辑写入第三方jar包的方法里。所以可以创建一个代理类实现和这个类方法相同的方法，通过让代理类持有真实对象，然后在原代码中调用代理类方法，来达到添加我们需要业务逻辑的目的。 ​ 这其实也就是代理模式的一种实现，通过对真实对象的封装，来实现扩展性。 满足代理模式应用场景 两个角色：被代理者、执行者 注重过程：被代理对象不想做或者不方便做，但是必须要做时使用代理模式 代理对象：执行者必须持有被代理对象的引用 这里我们提出一个需求：我们在编写的Service类的每个方法中没有进行事务处理，但根据需要我们必须进行实物处理，那此时我们进行静态代理。 ServiceInterface接口: 1234public interface ServiceInterface { void a(); void b();} 接口的实现类： 123456789101112public class ServiceImpl implements ServiceInterface{ @Override public void a() { System.out.println(\"执行a方法\"); } @Override public void b() { System.out.println(\"执行b方法\"); }} 代理类: 123456789101112131415161718192021public class ServiceImplProxy implements ServiceInterface{ private ServiceInterface service; public ServiceImplProxy(ServiceInterface service) { super(); this.service = service; } @Override public void a() { System.out.println(\"开启事务\"); service.a(); System.out.println(\"提交事务\"); } @Override public void b() { System.out.println(\"开启事务\"); service.b(); System.out.println(\"提交事务\"); }} 测试： 1234567891011public class TestProxy { public static void main(String[] args) { //创建目标对象 ServiceInterface service=new ServiceImpl(); //创建代理对象，将目标对象传入代理对象 ServiceInterface serviceProxy =new ServiceImplProxy(service); serviceProxy.a(); serviceProxy.b(); }} 静态代理的优点及缺点​ 上面演示了静态代理的完整过程，现在我们来看看静态代理的优缺点。 ​ 优点：扩展原功能，不侵入原代码。 ​ 缺点： 如果我们只需要调用代理对象的某一个方法，换句话说我们只需要代理对象代理委托类的某一个方法时，我们仍然需要实现接口中所有的方法，这样显得很浪费。当然在这里我们可以通过继承来实现静态代理，使用继承时，我们只需要覆写需要代理的方法即可。 如果在接口中定义了很多方法，而这些方法都需要被代理，并且代理的逻辑都是相同的，比如说在WEB开发中Service层所有方法执行之前都需要打开事务，结束后关闭事务。那么此时我们使用静态代理，会导致大量代码重复（想想如果有50个方法，你去一个一个手敲吧）。 动态代理动态代理的原理：​ 动态代理由程序在内存中动态生成一个对象，不需要我们手写代理对象，我们只需要指定代理方法的模板即可。 JDK自带动态代理的核心类：java.lang.reflect.Proxy类：​ 该类中的newInstance的静态方法，用于创建动态代理对象，该类也是所有生成的动态代理对象的父类。 ​ 核心方法： ​ Proxy.newProxyInstance(ClassLoader loader, Class&lt;?&gt;[] interfaces, InvocationHandler h) ​ 参数： ClassLoader：传入被代理类的类加载器，实际上只需要传入类加载器即可，不一定必须传入“被代理类”的类加载器，也可以传入别的实例的类加载器，因为java中类加载器是一个对象。这个类加载器用于将生成的字节码文件加载进方法区，并生成字节码对象。 Class&lt;?&gt;[] interfaces：这个参数是指定代理对象实现的接口，可以实现多个接口，这些接口中的所有方法都会按照invoke模板中的代码进行加强。 InvocationHandler h：传入InvocationHandler接口的实现类，该类中的invoke方法是代理对象中所有方法的逻辑处理模板。 java.lang.reflect.InvocationHandler接口：​ 该接口有一个invoke方法，我们需要实现invoke方法，这个方法就是代理方法的模板。 ​ 接口中需要实现的方法： ​ invoke(Object proxy, Method method, Object[] args)； ​ 参数： Object proxy：代表代理对象本身，可以它调用代理对象的其他方法 Method method：代表“被代理对象”对应方法的字节码对象 Object[] args：传入“代理对象”对应方法的参数数组 动态代理的示例代码：12345678910111213141516171819202122232425public class ServiceInvocationHandler implements InvocationHandler { private Object instance;//这是目标对象（被代理对象） public Object getInstance() { return instance; } public void setInstance(Object instance) { this.instance = instance; } /*** * proxy:代表代理对象本身,可以通过它调用代理对象的其他方法 * method:代表目标对象对应方法的字节码对象 * args：代表目标对象相应的参数 */ @Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable { System.out.println(\"开启事务\"); method.invoke(instance,args); System.out.println(\"提交事务\"); return null; }} ​ 通过Proxy类创建动态代理: 12345678910111213141516public class TestMyProxy { public static void main(String[] args) throws Exception{ //创建目标对象 ServiceInterface sale = new ServiceImpl(); //创建模板对象 ServiceInvocationHandler invocationHandler = new ServiceInvocationHandler(); //将目标对象注入模板对象中 invocationHandler.setInstance(sale); Class serviceClazz = sale.getClass(); ServiceInterface proxy = (ServiceInterface) Proxy.newProxyInstance(serviceClazz.getClassLoader(), serviceClazz.getInterfaces(), invocationHandler); proxy.a(); proxy.b(); }} 通过测试，动态代理对象与前面的静态代理结果相同: JDK动态代理的原理​ 看到这里肯定很多人都会另一头雾水，动态代理底层到底是怎么实现的，我们想办法通过程序将代理对象字节码文件输出到磁盘上，然后通过jdgui反编译工具，查看动态代理对象的源码结构。 1234567891011public class TestMyProxy { public static void main(String[] args) throws Exception{ byte[] buffer = ProxyGenerator.generateProxyClass(\"$Proxy0\", new Class[]{ServiceInterface.class}); try { FileOutputStream output = new FileOutputStream(\"C:/Users/Lenovo/Desktop/$Proxy0.class\"); output.write(buffer); } catch (Exception e) { e.printStackTrace(); } }} 注：使用ProxyGenerator类需要将JRE中lib目录下的一个jar包导入到项目中 通过反编译工具（推荐jd-gui）可以看到代理对象源码结构如下（下列代码对源码进行了部分删减，如果有需要可以自行生成源代码查看）： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273public final class $Proxy0 extends Proxy implements ServiceInterface{ private static Method m1; private static Method m4; private static Method m3; private static Method m2; private static Method m0; public $Proxy0(InvocationHandler paramInvocationHandler) { super(paramInvocationHandler); } public final void b() { try { this.h.invoke(this, m4, null); return; } catch (Error|RuntimeException localError) { throw localError; } catch (Throwable localThrowable) { throw new UndeclaredThrowableException(localThrowable); } } public final void a() { try { this.h.invoke(this, m3, null); return; } catch (Error|RuntimeException localError) { throw localError; } catch (Throwable localThrowable) { throw new UndeclaredThrowableException(localThrowable); } } static { try { m1 = Class.forName(\"java.lang.Object\").getMethod(\"equals\", new Class[] { Class.forName(\"java.lang.Object\") }); m4 = Class.forName(\"com.proxy.ServiceInterface\").getMethod(\"b\", new Class[0]); m3 = Class.forName(\"com.proxy.ServiceInterface\").getMethod(\"a\", new Class[0]); m2 = Class.forName(\"java.lang.Object\").getMethod(\"toString\", new Class[0]); m0 = Class.forName(\"java.lang.Object\").getMethod(\"hashCode\", new Class[0]); return; } catch (NoSuchMethodException localNoSuchMethodException) { throw new NoSuchMethodError(localNoSuchMethodException.getMessage()); } catch (ClassNotFoundException localClassNotFoundException) { throw new NoClassDefFoundError(localClassNotFoundException.getMessage()); } }} 我们分析动态生成的代理类源码可知，代理对象中的代理方法，都执行了this.h.invoke()方法，this.h实际上是代理类的父类中（Proxy类）中的属性，它保存了在调用Proxy.newInstance()方法时传入的InvocationHandler实例，而InvocationHandler中的invoke方法就是，代理类的代理逻辑。 也就是说在生成的一个动态代理对象中，对目标对象中所有方法都进行了相同的前处理和后处理过程，因为都执行相同的invoke“模板”方法，也就是说如果我们想对目标对象中每个方法进行不同的前处理和后处理，我们需要编写不同的InvocationHandler实现类，然后创建不同的动态代理对象才能实现这一需求。 总结动态代理实际上就是在内存中直接生成字节码文件（通常程序员写的java文件编译后生成的字节码文件都在硬盘中），然后将字节码加载进方法区生成字节码对象，生成字节码对象后通过反射就可以创建代理对象的实例了。生成字节码文件就必定需要生成这个代理类的方法，那程序是怎么知道这个代理对象需要实现哪些方法呢，这就是为什么在Proxy.newProxyInstance()需要传入接口的数组，传入几个接口，这个代理就会实现这些接口，程序自然就知道它需要实现哪些方法了。此时就会开始循环生成代理类中的方法，那这个方法的具体实现代码又是什么呢？由于生成的代理类继承了Proxy类，在Proxy类中有一个h字段，保存的是一个InvocationHandler实现类的对象，而我们通过Proxy.newInstance()方法时，传入了一个InvocationHandler实例，所以在代理对象中存放着一个InvocationHandler对象，代理对象的每个方法都会调用父类中存放的InvocationHandler实例中的invoke方法。 说明本文由听到微笑CSDN博客移植而来，原文地址： 原文","link":"/article/43606.html"},{"title":"什么是一致性Hash算法","text":"一致性Hash算法是为了解决分布式缓存中服务器伸缩问题，在传统的取余运算中，普通的余数Hash会存在算法伸缩性差的问题，一旦增加或删除节点，会导致大面积的缓存失效。 余数Hash代来的问题假设我们现在有5台缓存服务器： 现在我们下列数据： 1234{id:45321,value=\"AAAAAAA\"}{id:58945,value=\"BBBBBBB\"}{id:45243,value=\"CCCCCCC\"}{id:15243,value=\"DDDDDDD\"} 在分布式缓存下，确定缓存服务器编号最简单的方式就是模运算： 例如：id=45321的数据对应的服务器是：45321%5=1 此时看上去这种算法似乎非常合理，但是由于业务的发展，现有的服务器数量无法满足需求，我们在5台的基础上加了两台服务器： 此时，如果我们想取出刚刚缓存的数据，就会发生找不到缓存的问题。例如此时我们尝试获取id=58945的数据，根据ID我们确定服务器ID：58945%7=5，而在5号服务器中并没有这个数据。 采用传统的余数Hash来确定缓存服务器的算法会带来伸缩性差的问题，一旦服务器集群增加或删除节点就会导致原先缓存的数据大面积失效，而一致性Hash算法就是用来解决传统余数Hash算法伸缩性差的问题。 一致性Hash算法什么是一致性Hash算法首先，我们把全量的缓存空间当做一个环形存储结构。环形空间总共分成2^32个缓存区， 每一个缓存key都可以通过Hash算法转化为一个32位的二进制数，也就对应着环形空间的某一个缓存区。我们把所有的缓存key映射到环形空间的不同位置。 我们将每一个缓存服务器也遵循同样的Hash算法，比如利用IP做Hash，映射到环形空间当中去： 如何让key和节点对应起来呢？很简单，每一个key的顺时针方向最近节点，就是key所归属的存储节点。所以图中key1存储于Cache1，key2存储于Cache2，key3、key4存储于Cache3。 一致性Hash算法的优势如果我们在3个缓存服务器的基础上增加一个节点（Cache4）： 当缓存集群的节点有所增加的时候，整个环形空间的映射仍然会保持一致性哈希的顺时针规则，所以有一小部分key的归属会受到影响。 有哪些key会受到影响呢？图中加入了新节点Cache4，处于Cache3和Cache2之间，按照顺时针规则，Cache4和Cache2之间的key3不再归属于Cache3，而属于Cache4。可以看到在增加节点后，采用一致性Hash算法的缓存系统只有一小部分缓存数据失效。 虚拟节点有时候如果节点数量太少，就会发生分布不均匀的情况，这样分布式缓存的压力会大大增加，影响服务器性能: 为了解决因节点太少导致而产生的不均匀情况，一致性Hash算法引入了虚拟节点的概念。所谓虚拟节点，就是基于原来的物理节点，产生N个虚拟节点，最后将虚拟节点映射到环上。","link":"/article/62369.html"},{"title":"什么是B-树","text":"B-树是一种多路平衡查找树。需要明确的是，B-Tree完整翻译是Balance Tree，“-”只是一个连接符，而不是读“减”；B-树也并不是很多人以为的二叉树，二叉树的英文名称为Binary Tree，二叉搜索树是Binary Search Tree（BST）。 In computer science, a B-tree is a self-balancing tree data structure that maintains sorted data and allows searches, sequential access, insertions, and deletions in logarithmic time. The B-tree is a generalization of a binary search tree in that a node can have more than two children.[1] Unlike other self-balancing binary search trees, the B-tree is well suited for storage systems that read and write relatively large blocks of data, such as discs. It is commonly used in databases and file systems.——维基百科 在计算机科学中，b树是一种自平衡树数据结构，它维护排序的数据，并允许在对数时间内进行搜索、顺序访问、插入和删除。b树是二叉搜索树的泛化，即一个节点可以有两个以上的子节点。与其他自平衡二叉搜索树不同，b -树非常适合于读写相对较大数据块(如磁盘)的存储系统。它通常用于数据库和文件系统。 使用B-树的意义B-树诞生的意义还得从二叉搜索树来说起。在传统的二叉搜索树中，如果在最理想的状态下，二叉搜索树的查找时间复杂度为O(logN)，但是由于插入顺序的不同会导致二叉搜索树变成一个瘸子，最坏的情况下就变成了线性查找，时间复杂度变成了O(N)： 至此，平衡二叉树这个新品种就诞生，平衡二叉树的实现方式分为很多种，例如：AVL、红黑树、替罪羊树等。但这些算法的最终目的就是保住树不会成为一个瘸子，以保证树的查询效率维持一个较高水平。 但是平衡二叉树就真的能保证实际的查询效率吗？答案是否定的。因为我们传统的理解中并没有考虑磁盘的IO性能，如果平衡二叉树存储在内存中，那么平衡二叉树的效率自然非常高，只需要考虑数据的比较次数即可；但是如果一个非常大的平衡二叉树放在磁盘中，我们不可能将整棵树加载进内存，能做的只有逐一加载每一个磁盘页（这里的磁盘页对应着树的节点），那么此时影响查询性能就不是比较次数了，而是磁盘的IO了。 假如我们现在要查找节点为6的元素，就分别需要进行四次磁盘IO操作： 第一次磁盘IO： 第二次磁盘IO： 第三次磁盘IO： 第四次磁盘IO： 不难发现，树的高度决定着查询的IO次数，而B-Tree就是为了降低树的高度，减少磁盘IO次数而产生的一种数据。 B-树的定义 每一个节点最多有 m 个子节点 每一个非叶子节点（除根节点）最少有 ⌈m/2⌉ 个子节点 如果根节点不是叶子节点，那么它至少有两个子节点 有 k 个子节点的非叶子节点拥有 k − 1 个键 所有的叶子节点都在同一层 B-树的意义B-树在查询过程中比较次数不必二叉搜索树少，尤其是单一节点中元素数量很多时。可是相比磁盘IO的速度，内存中的比较耗时几乎是可以忽略的，所以只要树的高度足够低，IO次数足够少就可以提供查找性能。相比之下节点内部元素多一些也没有关系，只要不超过磁盘页的大小即可。这就是B-树的优势之一。 B-树主要用于文件系统以及部分数据库索引，大部分关系型数据库则使用B+树作为索引。","link":"/article/8567.html"},{"title":"什么是B+树","text":"在上文中 什么是B-树，我们讲解了B-树的基本原理，而B+树是基于B-树的一种变体，有着比B-树更高的查询性能。在说B+树之前，我们先回顾一下B-树的特点： 一个M阶B-树有如下特点： 每一个节点最多有 m 个子节点 每一个非叶子节点（除根节点）最少有 ⌈m/2⌉ 个子节点 如果根节点不是叶子节点，那么它至少有两个子节点 有 k 个子节点的非叶子节点拥有 k − 1 个键 所有的叶子节点都在同一层 B+树的特点一个M阶B+树具有一下特点： k个子树的中间节点包含有k个元素（B树中是k-1个元素），每个元素不保存数据，只用来索引，所有数据都保存在叶子节点。 所有的叶子结点中包含了全部元素的信息，及指向含这些元素记录的指针，且叶子结点本身依关键字的大小自小而大顺序链接。 所有的中间节点元素都同时存在于子节点，在子节点元素中是最大（或最小）元素。 咱们举个例子来看看： 在B+树中每一个父节点的元素都出现在子节点中，是子节点最大的（或最小的）元素。 在上图中，根节点元素8是子节点(2,5,8)的最大元素，也是叶子节点(6,8)的最大元素。根节点15是子节点(11,15)的最大元素，也是(13,15)的最大元素。需要注意的是，根节点中的最大元素，也就等同于整颗B+树的最大元素。以后无论插入删除多少元素，始终要保持最大元素在根节点中。 至于叶子节点，由于父节点的元素都出现子节点，因此所有叶子节点包含了全量元素信息，并且每个叶子节点都带着指向下一个节点的指针，形成了一个有序链表： B+树还具有一个特点，这个特点是在索引之外，确实至关重要的特点，那就是卫星数据的位置。 在B-树中卫星数据： 而在B+树中，只有叶子节点带有卫星数据，其余中间节点仅仅是索引，没有任何数据关联。 需要补充的是，在数据库的聚集索引（Clustered Index）中，叶子节点直接包含卫星数据。在非聚集索引（NonClustered Index）中，叶子节点带有指向卫星数据的指针。 B+树的性能优势B+树的好处主要体现在查询性能上，下面我们就以单行查询和范围查询来做分析。 单行查询在单行元素查询的时候，B+树会自顶向下逐层查找节点，最终匹配的叶子节点。假如我们要查找节点为3的节点： 看起来与B树一样都需要执行三次磁盘IO，那么实际的性能优化体现在哪里呢？ 实际上我们在学习B树时，有一个重要概念：B树的阶数越高，树的高度就越低，查询所执行磁盘IO次数就越少，但是每个节点数据不能超过磁盘页的大小。在B+树中由于中间节点没有卫星数据，所以同样大小的磁盘页可以拥有更多的节点元素，从而降低树的高度。 另外由于B+树的数据每一次查找都必须找到叶子节点才能得到数据，所以相对于B树的查找，B+树每一次查找都是稳定的。 范围查询可能在单行查询时B+树的性能优势并不明显，但是一旦到了范围查询，B+树的性能优势就充分发挥。我们先来看看B-树的范围查找过程。 B-树的范围查找假如我们需要查找[3,11]范围的节点，首先我们需要找到范围的下限3： 中序变量6： 中序遍历到8： 中序遍历到9： 中序遍历到11，遍历结束： B+树的范围查找现在来看B-树的范围查找确实很繁琐，但是B+树就非常简单了： B+树我们只需要找到下限节点(3,5)，然后通过链表指针，遍历到(9,11)即可。 总结B+树是对B-树的一种改进，它相对于B-树有如下优势： 它使得单一节点能够存储更过的元素，最终降低了磁盘IO次数。 所有查询都要查找到叶子节点，查询更加稳定。 所有叶子节点形成有序链表，方便范围查询。 本文主体转载至： 漫画：什么是B+树？，但对部分细节有所更改。","link":"/article/21035.html"},{"title":"B树和B+树与磁盘读写之间的联系","text":"在前面的文章中我们分别介绍了B-树和B+树的应用原理，B树实际上就是一颗多路查找树，每一个结点都保存多个元素。保存多个元素是为了尽可能降低树的高度，以降低查询时磁盘IO次数。但是有人就会问了，我们将所有元素放在一个节点中那不就最大限度的降低树的高度吗？😃😃😃 说到这里我们就需要来讲解一下局部性原理与磁盘预读。 局部性原理与磁盘预读由于存储介质的特性，磁盘本身存取就比主存慢很多，再加上机械运动耗费，磁盘的存取速度往往是主存的几百分分之一，因此为了提高效率，要尽量减少磁盘I/O。为了达到这个目的，磁盘往往不是严格按需读取，而是每次都会预读，即使只需要一个字节，磁盘也会从这个位置开始，顺序向后读取一定长度的数据放入内存。这样做的理论依据是计算机科学中著名的局部性原理： 当一个数据被用到时，其附近的数据也通常会马上被使用。 程序运行期间所需要的数据通常比较集中。 由于磁盘顺序读取的效率很高（不需要寻道时间，只需很少的旋转时间），因此对于具有局部性的程序来说，预读可以提高I/O效率。 预读的长度一般为页（page）的整倍数。页是计算机管理存储器的逻辑块，硬件及操作系统往往将主存和磁盘存储区分割为连续的大小相等的块，每个存储块称为一页（在许多操作系统中，页得大小通常为4k），主存和磁盘以页为单位交换数据。当程序要读取的数据不在主存中时，会触发一个缺页异常，此时系统会向磁盘发出读盘信号，磁盘会找到数据的起始位置并向后连续读取一页或几页载入内存中，然后异常返回，程序继续运行。 B树与B+树与磁盘读写之间的联系内存和磁盘既然是以页为单位交换数据，那么B树的节点大小保持页的大小是最优选择，既尽量的降低了节点读取后的内存占用，又尽可能利用了磁盘一次IO所读取的数据。 B树： 我们假设如果是4阶的，那么每个结点最多3个关键字，最少两个（根节点最少1个），也就是说，我们最多也就要访问3次磁盘就可以完成访存，而传统的访存需要每一个关键字都进行访存，可以看出B树的优势。注意：B树的非叶结点不单单只有key值，还有key对应数据在磁盘的具体地址。 B+树： 相对与B树而言，B+树的非叶结点值只存有key值，不含有卫星数据，比较而言就会有更大的空间，就可以存更多的key值，就会显得更加“矮胖”，矮了磁盘IO次数就相对会更少一些。同时由于B+树增加了一个最小关键字的根结点，所以顺序访问更加便捷。 本文参考至： B树与B+树与磁盘读取的联系 硬盘的读写原理","link":"/article/54308.html"},{"title":"二十二.Git树对象","text":"​ 在Git对象中我们了解到，Git仓库相当于一个Key—Value数据库，我们可以通过hash-object命令向“数据库”中存入文件，然后通过存入文件时返回的SHA-1码取出文件。但是在这个数据库中有一个致命的问题，那就是并没保存原先的文件名，所以Git就因此产生树对象。 了解树对象​ Git 以一种类似于 UNIX 文件系统的方式存储内容，但作了些许简化。 所有内容均以树对象和数据对象的形式存储。 一个树对象包含了一条或多条树对象记录（tree entry），每条记录含有一个指向数据对象或者子树对象的 SHA-1 指针，以及相应的模式、类型、文件名信息。 例如，某项目当前对应的最新树对象可能是这样的： 这个项目根目录下包含README、Rakefile文件和lib文件夹，在lib文件夹中有一个simplegit.rb文件。 我们可以通过将树对象理解为Git的“文件夹”，每一个树对象可以引用若干个文件对象（blob），和若干个树对象（tree）。 查看Git的树对象12$ git cat-file -p master^{tree}# master^{tree} 语法表示 master 分支上最新的提交所指向的树对象 ​ 我们可以看到，在git_learning这个仓库中，master分支最新的tree有三条记录，第一条和第三条指向一个文件对象，而第二条记录指向一个树对象，所以css是一个文件夹。 ​ 可以看到这个树对象中只有一条记录，指向一个文件对象，文件名叫style.css","link":"/article/34569.html"},{"title":"GitHub如何向别人的项目发送Pull Request请求","text":"Pull Request 是社会化编程的象征。GitHub 创造的这一功能，可说给开源开发世界带来了一场革命。不会用这个功能，就等于不会用GitHub。不过，刚刚接触 GitHub 的人在发送 Pull Request 时，往往会遇到找不到对方的项目或者不知道该如何发送等问题。 本文就将详细讲述Pull Request的全过程，并为大家提供一个练习Pull Request的机会。 到底什么是Pull Request 很多新手小白在使用GitHub时经常看到这个词，但对这个词的真正含义并不理解。Pull Request实际上就是自己修改别人项目的源代码后，请求对方仓库采纳该修改时采取的一种行为。所有对仓库有读权限的用户都能发送Pull Request。 例如，我们使用GitHub上面托管的开源软件后，发现了其中的某个BUG，我们通过自己的努力解决了这个BUG（修改源码），此时我们可以向该仓库发送Pull Request，如果该修改被仓库管理员采纳，那么别人在使用该开源软件就不会出现该BUG了。 发送Pull Request的具体步骤第一步：进入任意仓库页面 这里博主为大家提供一个现成的测试仓库 https://github.com/HearingSmile/TestPullRequest大家可以按照文章所讲解的步骤，向这个仓库提交自己的Pull Request，练习Pull Request具体操作。 第二步：Fork该仓库 点击图片中右上角的Fork按钮，将TestPullRequest仓库Fork到自己账户中： 第三步：clone仓库我们需要将自己账户中的TestPullRequest仓库克隆到本地。 我进入系统中任意目录下，执行下列命令完成克隆操作： 1git clone https://github.com/xxx/TestPullRequest.git #URL获取，见下图 第四步：新建分支，在分支上完成修改当前 Git 的主流开发模式都会使用特性分支，所以我们修改之前需要创建一个分支，然后在master分支上执行这次修改。 在创建分支之前我们可以先确定一下远程分支： 1234$ git branch -a* master remotes/origin/HEAD -&gt; origin/master remotes/origin/master 克隆后的仓库有一个master本地分支，但是我们最好不要在主分支上进行修改，我们需要创建一个特性分支： 创建分支： 1git checkout -b work origin/master 我们特性分支的名字叫做work，它跟踪远程分支origin/master。 再次确认当前仓库的分支状况： 12345$ git branch -avv master e31200d [origin/master] Init Reporsitory* work e31200d [origin/master] Init Reporsitory remotes/origin/HEAD -&gt; origin/master remotes/origin/master e31200d Init Reporsitory 第五步：修改代码 我添加14行中的代码 第六步：提交更新，并Push到GitHub上用diff命令查看修改是否成功： 12345678910111213$ git diff index.htmldiff --git a/index.html b/index.htmlindex 2fc45aa..7088d2a 100644--- a/index.html+++ b/index.html@@ -11,5 +11,6 @@ &lt;/head&gt; &lt;body id=\"page-index\"&gt; &lt;p&gt;1.第一次提交&lt;/p&gt;+ &lt;p&gt;2.Pull Request测试&lt;/p&gt; &lt;/body&gt; &lt;/html&gt;\\ No newline at end of file 提交修改，并push到GitHub上去： 123$ git add index.html$ git commit -m 'Update index.html'$ git push origin work #work表示push到远端的work分支上去，如果远端没有则自动创建远端分支 第七步：发送Pull Request切换到work分支： 点击New pull request按钮： 发送Pull Request： 提交成功：","link":"/article/17497.html"},{"title":"二十一.Git文件对象","text":"Git对象​ Git 是一个内容寻址文件系统。 这意味着，Git 的核心部分是一个简单的键值对数据库（key-value data store）。 你可以向该数据库插入任意类型的内容，它会返回一个键值，通过该键值可以在任意时刻再次检索（retrieve）该内容。 ​ 我们可以通过hash-object这个底层命令向Git这个“数据库”中保存数据了。 准备工作​ 初始化一个git仓库： 1$ git init test ​ 我们进入到.git目录下，查看objects文件夹的结构： 12$ cd .git$ find objects ​ 我们可以看到objects目录下，只有两个空的文件夹：info、pack 向”数据库“中存入数据​ 我们先在git的工作目录中创建一个文件，例如hello.txt，然后我们能通过hash-object命令将这个文件写入“数据库”。 12$ git hash-object -w ./helle.txt# -w 选项表示 hash-object此时用于存储数据对象；若不指定此项，则该命令仅返回对应的键值。 ​ 或者我们可以使用标准输入存入数据 12$ echo 'Hello World!' | git hash-object -w --stdin# --stdin 该选项指示 hash-object 命令从标准输入读取内容；若不指定此选项，则须在命令尾部给出待存储文件的路径。 ​ 我们可以看到将文件存入后，系统会返回一个SHA-1的哈希值。一个文件对应一条内容，以该内容加上特定头部信息一起的 SHA-1 校验和为文件命名。 校验和的前两个字符用于命名子目录，余下的 38 个字符则用作文件名。 ​ 我们再次进入.git目录执行find objects即可看到刚刚我们保存的hello.txt文件（第一次初始化的git仓库，.git/objects目录下只有pack、info这两个文件夹） 从”数据库“中取出数据​ 可以通过 cat-file 命令从 Git 那里取回数据。 这个命令简直就是一把剖析 Git 对象的瑞士军刀。 为 cat-file 指定 -p 选项可指示该命令自动判断内容的类型，并为我们显示格式友好的内容： 12$ git cat-file -p 802992c4220de19a90767f3000a79a31b98d0df7# -p 指示该命令自动判断内容的类型 Git objects的理解​ 在将文件存入Git objects中时，文件名并没有被保存，仅保存了文件的内容。 上述类型的对象我们称之为数据对象（blob object）。 利用 cat-file -t 命令，可以让 Git 告诉我们其内部存储的任何对象类型，只要给定该对象的 SHA-1 值： 12$ git cat-file -t 802992c4220de19a90767f3000a79a31b98d0df7# -t 指示cat-file命令返回该对象的类型（type）","link":"/article/3483.html"},{"title":"二十四.git目录探秘","text":".git目录是Git仓库文件存放的目录，它里面存放着仓库的版本历史、分支等一切与当前仓库相关的信息。 我们先进入.git目录，这个目录是一个隐藏目录，需要使用ls -a才能看到。 1$ cd .git ​ HEAD文件​ HEAD 文件指示目前被检出的分支 ​ 该文件使用与指定当前工作目录正工作在哪个分支中，我们通过cat或其他命令查看这个文件内容： ​ ​ 通过上面的HEAD我们可以看到，当前工作目录正工作在master分支下，如果我们更换分支，HEAD文件的内容也会随之改变。 refs文件夹​ refs 目录存储指向数据（分支）的提交对象的指针 ​ 前面HEAD文件记录，当前工作的分支，就是引用refs文件夹中的文件。refs文件夹中有两个文件夹: ​ tags文件夹：保存当前仓库的标签 heads：表示Git仓库的分支（每一个分支，都在这个目录中有相对应的文件夹） config文件​ 在文件内容之前，我们先执行下面的命令 1$ git config --local user.name \"tjd\" #local参数可以省略，默认就是local作用域 ​ 然后我们再使用cat config命令查看config文件中的内容： ​ ​ 可以看到，由于我们为当前工作目录设置与全局user.name不同的值，所以系统就会在当前工作目录的config文件中记录用户设置的-local作用域的属性。 ​ 我们可以直接更改这个文件中的内容，也能达到$ git config --local user.name &quot;xxx&quot;命令的效果。也就是说，我们在使用git config --local user.name命令查看-local作用域下的user.name就是读取这个文件中的内容。 objects文件夹​ 这个文件夹保存Git的对象。","link":"/article/11704.html"},{"title":"二十三.Git提交对象","text":"​ 前面我们提到了tree的概念，它解决了文件名的保存以及文件夹的层次结构的问题。但是我们每次提交都相当于保存了一个树对象，而每一次的提交都不一定修改了项目下的所有文件，那么就意味着总有一些树对象是可以在下次提交中重用的。若想重用这些快照树，那么就必须记住以往创建的所有树对象。 并且，你也完全不知道是谁保存了这些快照，在什么时刻保存的，以及为什么保存这些快照。 而以上这些，正是提交对象（commit object）能为你保存的基本信息。 ​ 下面我们给出一个提交对象、tree对象、blob对象的基本关系图： 在上面的这个commit对象中，保存了作者、提交者、提交信息、父提交对象、此次提交的根树对象。 Git对象的理解问题：新建一个仓库，在仓库中创建doc/readme文件，然后放入暂存区，放入暂存区后，此时Git仓库中有多少个对象？提交后Git仓库有多少个对象？ 第一步：创建一个仓库1$ git init test 第二步：新建doc/readme12$ mkdir doc$ touch readme 在doc放入暂存区之前，查看.git/objects：发现并没有任何对象。 第三步：将doc/readme放入暂存区，查看创建了几个对象1$ git add doc/ 此时我们查看.git/objects： 1$ find .git/bojects 我们发现有一个对象，通过git cat-file -t 980a0d5f19可以看出这个对象是一个blob对象。 第四步：提交此次变更，然后查看此次变更后一共产生了多少个对象12$ git commit -m 'create doc/readme'$ find .git/objects 此时我们看到一共创建了4个对象，我们使用cat-file命令查看对象的内容： 下图就是这四个对象的继承图：","link":"/article/35041.html"},{"title":"十三.分支的基本操作","text":"前言有人把 Git 的分支模型称为它的必杀技特性，也正因为这一特性，使得 Git 从众多版本控制系统中脱颖而出。 为何 Git 的分支模型如此出众呢？ Git 处理分支的方式可谓是难以置信的轻量，创建新分支这一操作几乎能在瞬间完成，并且在不同分支之间的切换操作也是一样便捷。 与许多其它版本控制系统不同，Git 鼓励在工作流程中频繁地使用分支与合并，哪怕一天之内进行许多次。 理解和精通这一特性，你便会意识到 Git 是如此的强大而又独特，并且从此真正改变你的开发方式。 查看分支12345git branch checkout* master #打星号的分支为当前所在分支 test 如果需要查看每一个分支的最后一次提交，可以运行 git branch -v 命令： 1git branch -v 如果需要查看分支跟踪的远程分支就需要使用-vv选项了： 1git branch -vv --merged 与 --no-merged 这两个有用的选项可以过滤这个列表中已经合并或尚未合并到当前分支的分支。 如果要查看哪些分支已经合并到当前分支，可以运行 git branch --merged： 12git branch --mergedgit branch --no-merged 创建分支只创建分支： 12git branch &lt;branch-name&gt; #在当前分支最新的commit上新建分支git branch &lt;branch-name&gt; &lt;SHA-1-Code&gt; #创建一个分支，指向一个指定的commit对象 ​ 这个命令创建一个分支指针，指向HEAD所指向的commit对象（ HEAD 想象为当前分支的别名），分支实际上就是一种指针，指向当前分支上最新的commit对象（提交对象总是指向它自己上一次的提交对象），具体分支原理参考 分支简介。 创建分支，并切换到新分支上： 12345git checkout -b &lt;branch-name&gt;#上面的命令等同于：git branch &lt;branch-name&gt;git checkout &lt;branch-name&gt; 删除分支1git branch -d &lt;branch-name&gt; 分支切换1git checkout &lt;branch-name&gt; #更换分支后，当前工作目录的文件内容也会回到指定分支的状态 例如： 1git checkout master ​ 这条命令做了两件事。 一是使 HEAD 指回 master 分支，二是将工作目录恢复成 master 分支所指向的快照内容。 也就是说，你现在做修改的话，项目将始于一个较旧的版本。 本质上来讲，这就是忽略 testing分支所做的修改，以便于向另一个方向进行开发。 ​ 在切换分支时，一定要注意你工作目录里的文件会被改变。 如果是切换到一个较旧的分支，你的工作目录会恢复到该分支最后一次提交时的样子。 如果 Git 不能干净利落地完成这个任务，它将禁止切换分支，例如当前分支中有文件被修改但为提交，此时如果系统将分支切换过去就会导致数据丢失，显然在以数据为重的Git系统中是不会这样做的。 分支合并无分叉分支的合并1git merge &lt;branch-name&gt; #无分叉的分支，直接合并（快进） 两个分支无冲突指：一个分支所指向的commit对象是另个一个分支的直接父commit节点。 ​ 在上图中，C0、C1、C2、C3、C4都是commit对象，master、hotfix、iss53分别是三个分支（分支的实质是指向commit对象的一个指针），其中master分支指向的commit对象是hotfix和iss53的直接父节点，所以此时master与gotfix、iss53这两个分支中任何一个进行合并都是不会产生冲突的。 在进行无冲突分支合并时，要以较早的版本为基准进行合并： 12git checkout master #切换到master分支，因为matser为早期版本git merge hotfix #与hotfix进行合并 ​ 换句话说，当你试图合并两个分支时，如果顺着一个分支走下去能够到达另一个分支，那么 Git 在合并两者的时候，只会简单的将指针向前推进（指针右移），因为这种情况下的合并操作没有需要解决的分歧——这就叫做 “快进（fast-forward）”。 分叉分支的合并在很多时候，分支可能会产生分叉，也就是下面这个情况： 此种情况，依旧是使用merge命令 1git merge iss53 #在master分支中使用该命令 ​ 在这种情况下，你的开发历史从一个更早的地方开始分叉开来（diverged）。 因为，master 分支所在提交并不是 iss53 分支所在提交的直接祖先，Git 不得不做一些额外的工作。 出现这种情况的时候，Git 会使用两个分支的末端所指的快照（C4 和 C5）以及这两个分支的工作祖先（C2），做一个简单的三方合并。 ​ 和之前将分支指针向前推进所不同的是，Git 将此次三方合并的结果做了一个新的快照并且自动创建一个新的提交指向它。 这个被称作一次合并提交，它的特别之处在于他有不止一个父提交。 冲突解决​ 有时候合并操作不会如此顺利。 如果你在两个不同的分支中，对同一个文件的同一个部分进行了不同的修改，Git 就没法干净的合并它们。 此时 Git 做了合并，但是没有自动地创建一个新的合并提交。 Git 会暂停下来，等待你去解决合并产生的冲突。 你可以在合并冲突后的任意时刻使用 git status 命令来查看那些因包含合并冲突而处于未合并（unmerged）状态的文件： 1234567891011$ git statusOn branch masterYou have unmerged paths. (fix conflicts and run \"git commit\")Unmerged paths: (use \"git add &lt;file&gt;...\" to mark resolution) both modified: index.htmlno changes added to commit (use \"git add\" and/or \"git commit -a\") ​ 如果因为冲突被暂停，我们需要手动解决冲突后，然后通过git add将修改放入暂存区，然后提交即可 若冲突无法解决，取消此次合并1git reset --merge","link":"/article/60933.html"},{"title":"十四.分离头指针","text":"HEAD指针和分支实质上是一样，都是一个指针指针，HEAD指针通常是指向分支的。但是我们使用checkout命令,检出到一个历史commit对象，这样就处于分离头指针状态。所谓的分离头指针状态就是指：HEAD指针指向的不是任何一个分支，而是一个历史版本的Commit对象： ​ 通过log命令，查看历史版本的校验码： 12345678910111213141516171819git log commit c689ddfc8fdc6b689a13d3a7dd861df3230d0947 (HEAD -&gt; master)#从括号中可以看到，通常情况下，HEAD是指向一个分支的（这里指向master分支）Author: tianjindong &lt;tianjindong98@qq.com&gt;Date: Tue Jun 4 09:52:42 2019 +0800 dsacommit d21c01b28f1b21bffab3a64286fdc180405d6120Merge: 86984d0 8b43fc7Author: tianjindong &lt;tianjindong98@qq.com&gt;Date: Tue Jun 4 09:28:36 2019 +0800 dsadascommit 8b43fc7441ced8d19b4edd1aaa333444b931b075 #我们进入的历史版本Author: tianjindong &lt;tianjindong98@qq.com&gt;Date: Tue Jun 4 09:26:41 2019 +0800 dasdsad 通过checkout检出到历史版本： 12345678910111213git checkout 8b43fc74Note: checking out '8b43fc74'.You are in 'detached HEAD' state. You can look around, make experimentalchanges and commit them, and you can discard any commits you make in thisstate without impacting any branches by performing another checkout.If you want to create a new branch to retain commits you create, you maydo so (now or later) by using -b with the checkout command again. Example: git checkout -b &lt;new-branch-name&gt;HEAD is now at 8b43fc7 dasdsad ​ 在分离头状态下，我们查看历史信息： 12345678910111213git logcommit 8b43fc7441ced8d19b4edd1aaa333444b931b075 (HEAD) #可以看到HEAD没有指向任何分支Author: tianjindong &lt;tianjindong98@qq.com&gt;Date: Tue Jun 4 09:26:41 2019 +0800 dasdsadcommit fefbd33515900407c6cb53631a1b5818adc77698Merge: 932a3bf 1d04d68Author: tianjindong &lt;tianjindong98@qq.com&gt;Date: Tue Jun 4 09:13:54 2019 +0800 merge master and test branch ​ 在分离头状态下，仍然可以进行工作区的修改和提交，但是一旦切回到其它分支，就可能会导致在分离头指针状态下提交的commit的对象丢失。","link":"/article/4964.html"},{"title":"十五.变基操作","text":"使用变基操作合并分支 ​ 在 Git 中整合来自不同分支的修改主要有两种方法：merge 以及 rebase。 ​ 在使用merge命令时，我们合并分支的效果是这样的： ​ 其实，还有一种方法：你可以提取在 C4 中引入的补丁和修改，然后在 C3 的基础上应用一次。 在 Git 中，这种操作就叫做变基。 你可以使用 rebase 命令将提交到某一分支上的所有修改都移至另一分支上，就好像“重新播放”一样。 1234$ git checkout experiment$ git rebase master #将test分支变基到master分支First, rewinding head to replay your work on top of it...Applying: added staged command ​ 此时，C4' 指向的快照就和上面使用 merge 命令的例子中 C5 指向的快照一模一样了。 这两种整合方法的最终结果没有任何区别，但是变基使得提交历史更加整洁。 你在查看一个经过变基的分支的历史记录时会发现，尽管实际的开发工作是并行的，但它们看上去就像是串行的一样，提交历史是一条直线没有分叉。 ​ 一般我们这样做的目的是为了确保在向远程分支推送时能保持提交历史的整洁——例如向某个其他人维护的项目贡献代码时。 在这种情况下，你首先在自己的分支里进行开发，当开发完成时你需要先将你的代码变基到 origin/master 上，然后再向主项目提交修改。 这样的话，该项目的维护者就不再需要进行整合工作，只需要快进合并便可。 ​ 请注意，无论是通过变基，还是通过三方合并，整合的最终结果所指向的快照始终是一样的，只不过提交历史不同罢了。 变基是将一系列提交按照原有次序依次应用到另一分支上，而合并是把最终结果合在一起。 变基冲突解决​ 与merge操作一样，如果两个分支同时更改了某个文件同一个地方，在变基时就会产生冲突，变基操作就会自动暂停，然后系统等待我们解决冲突。 ​ 与merge解决冲突的操作一样，我们需要重新编辑冲突文件，解决文件中的冲突，然后通过add命令将冲突文件重新放入暂存区，然后执行git rebase --continue即可。 若我们无法解决变基冲突，我们就需要取消此次变基操作1git rebase --abort 合并连续多个commit的合并 ​ 我们需要将上面两个提交对象合并到d5c36d这个commit对象中，所以d5c36d的父节点为基准进行变基操作： 1git rebase -i 9b444c #-i选项表示以交互模式进行 执行上述命令后，弹出命令交互框：我们需要在命令框中输入不同的命令从而达到不同的变基操作，但是第一个命令一定是像pick和reword这类有 use commit作用的命令: ​ 由于我们需要将6bdb12e和b0f82fc合并到d5c36d0，所以我们后面两个命令要选用 squash命令（可以用简写 s），该命令用于压缩commit对象： ​ 我们使用wq命令保存这个对话框后系统会自动执行这些命令，从而达到变基操作，执行完毕后系统会自动跳到另一个对话框，在这个对话框中我们可以修改合并后的commit的提交信息，通过wq保存后就完成了变基操作。 我们通过log命令查看合并后的效果： 更改历史某个commit对象的提交信息​ 在撤销操作一文中我们提到了修改上一次的提交信息的操作，但是如果我们需要更改历史版本的提交信息，那么就不能使用commit --amend命令来实现这个功能了，我们就需要变基来实现这个功能了： ​ 比如我们想要更改下图中b8b5af这个commit对象的提交信息： 1git rebase -i 9b444c 执行该命令后进入命令交互框： ​ 根据 reword的命令的提示信息可知，该命令会在变基操作时继续使用该对象，只改变该对象的提示信息。 ​ 我们需要将pick b8b5af2改为reword b8b5af2: ​ 保存后，系统执行这些命令，然后自动进入提交信息修改的框，我们修改完成后保存即可。","link":"/article/38240.html"},{"title":"十.使用diff命令查看文件修改的细节","text":"​ 我们通过git status命令只能看到当前工作区内哪些文件被修改了，而如果我们想要查看修改的细节那就得使用git diff命令了 查看工作区中文件与暂存区文件的区别：这里需要指出的是：该命令查看的是工作区文件相对于暂存区文件的区别。 1git diff &lt;文件名&gt; #当文件名省略时，则表示查看所有修改过的文件的区别 12345678$ git diff hello.javadiff --git a/hello.java b/hello.javaindex f761ec1..b2a7546 100644--- a/hello.java+++ b/hello.java@@ -1 +1 @@-bbb #原先暂存区中的文件+ccc #工作区新做出的变更 查看暂存区中的文件与上次提交的快照的具体区别：这里需要指出的是：该命令查看的是暂存区文件相对于HEAD指针指向的快照的文件的区别。 1git diff --cached &lt;文件名&gt; #--cached的表示查看暂存区中文件与HEAD指针所指向的快照中的区别,当文件名省略时，则表示查看所有修改过的文件的区别 12345678$ git diff --cached hello.javadiff --git a/hello.java b/hello.javaindex 8f22c74..f761ec1 100644--- a/hello.java+++ b/hello.java@@ -1 +1 @@-aaa`+bbb 查看两个提交对象的具体差别1git diff &lt;base-commit&gt; &lt;commit&gt; &lt;文件名&gt;... #如果省略文件名，就是比较commit相对于base-commit的所有文件的具体区别 一定要注意：该命令显示的是后面的commit对象相对于前面的commit对象的变化 123456789101112131415161718192021git log commit c689ddfc8fdc6b689a13d3a7dd861df3230d0947 (HEAD -&gt; master) # A CommitAuthor: tianjindong &lt;tianjindong98@qq.com&gt;Date: Tue Jun 4 09:52:42 2019 +0800 dsacommit d21c01b28f1b21bffab3a64286fdc180405d6120Merge: 86984d0 8b43fc7Author: tianjindong &lt;tianjindong98@qq.com&gt;Date: Tue Jun 4 09:28:36 2019 +0800 dsadascommit 8b43fc7441ced8d19b4edd1aaa333444b931b075 # B commitAuthor: tianjindong &lt;tianjindong98@qq.com&gt;Date: Tue Jun 4 09:26:41 2019 +0800 dasdsad git diff HEAD 8b43fc744 #查看A和B commit的区别","link":"/article/57577.html"},{"title":"十六.Git标签","text":"像其他版本控制系统（VCS）一样，Git 可以给历史中的某一个提交打上标签，以示重要。 比较有代表性的是人们会使用这个功能来标记发布结点（v1.0 等等）。 在本节中，你将会学习如何列出已有的标签、如何创建新标签、以及不同类型的标签分别是什么。 查看标签1git tag 标签分类​ git标签分为两类： 轻量标签（lightweight） 一个轻量标签很像一个不会改变的分支 - 它只是一个特定提交的引用。 附注标签（annotated） 附注标签是存储在 Git 数据库中的一个完整对象。 它们是可以被校验的；其中包含打标签者的名字、电子邮件地址、日期时间；还有一个标签信息。 创建附注标签1git tag -a &lt;标签名&gt; [commit的Hash值] -m '标签信息' a选项：表示创建附注标签 m选项：表示添加一条将会存储在标签中的信息 如果省略commit Hash值就表示在当前commit上打标签，如果指定就在指定位置上打标签 创建轻量标签​ 另一种给提交打标签的方式是使用轻量标签。 轻量标签本质上是将提交校验和存储到一个文件中 - 没有保存任何其他信息。 1git tag &lt;标签名&gt; [commit的hash值] 查看标签信息与对应的提交信息1git show &lt;标签名&gt; 只有附注标签会显示标签信息，轻量标签只会显示提交信息。 发布标签到远端​ 默认情况下，git push 命令并不会传送标签到远程仓库服务器上。 在创建完标签后你必须显式地推送标签到共享服务器上。 这个过程就像共享远程分支一样 - 你可以运行 git push origin [tagname] ​ 如果想要一次性推送很多标签，也可以使用带有 --tags 选项的 git push 命令。 这将会把所有不在远程仓库服务器上的标签全部传送到那里。 删除标签1git tag -d &lt;标签名&gt; 需要注意的是上述命令并不会从任何远程仓库中移除这个标签，你必须使用 git push &lt;remote&gt; :refs/tags/&lt;tagname&gt; 来更新你的远程仓库： 123$ git push origin :refs/tags/v1.4-lwTo /git@github.com:schacon/simplegit.git - [deleted] v1.4-lw","link":"/article/53876.html"},{"title":"十一.为Git创建命令别名","text":"前言在Git系统中，有些命令需要和部分参数或选项连用才能达到效果，但过长的参数列表输入会严重影响我们的工作效率，幸运的是Git提供了和Linux系统一样的命令别名功能，这样我们就可以通过别名来调用常用命令了。 定义别名1git config --global alias.自定义别名 命令 #--global是别名的作用域，通常有local、global、system 例如：我们需要使用st代替status 1git config --global alias.st status ​ 此时git st与git status等价。 听到微笑自己比较喜欢创建一个log --all --graph的别名： 1git config --global alias.logg 'log --all --graph'","link":"/article/16523.html"},{"title":"五.创建和克隆仓库","text":"一. 把已有的项目代码纳入Git管理12$ cd 项目所在路径 #先进入项目所在文件夹$ git init #初始化仓库 二. 创建一个新的仓库123$ cd 某个文件夹$ git init 仓库名称 #git会在当前目录下，创建与项目名称相同的文件夹$ cd 仓库名称 #进入仓库 创建仓库后，git会在仓库中创建一个.git的文件夹，这个文件夹时git仓库的核心文件。 三. 克隆现有仓库​ 如果你想获得一份已经存在了的 Git 仓库的拷贝，比如说，你想为某个开源项目贡献自己的一份力，这时就要用到 git clone 命令。 如果你对其它的 VCS 系统（比如说Subversion）很熟悉，请留心一下你所使用的命令是”clone”而不是”checkout”。 这是 Git 区别于其它版本控制系统的一个重要特性，Git 克隆的是该 Git 仓库服务器上的几乎所有数据，而不是仅仅复制完成你的工作所需要文件。 当你执行 git clone 命令的时候，默认配置下远程 Git 仓库中的每一个文件的每一个版本都将被拉取下来。 事实上，如果你的服务器的磁盘坏掉了，你通常可以使用任何一个克隆下来的用户端来重建服务器上的仓库。 ​ 克隆仓库的命令格式是 git clone [url] 。 比如，要克隆 Git 的可链接库 libgit2，可以用下面的命令： 1$ git clone https://github.com/tianjindong/test [自定义仓库名] #如果本地有同名仓库，那么必须指定一个另外的文件夹名，来保存克隆下来的文件 ​ Git 支持多种数据传输协议。 上面的例子使用的是 https:// 协议，不过你也可以使用 git:// 协议或者使用 SSH 传输协议，比如 user@server:path/to/repo.git 。 ​ 或者克隆本地的git仓库： 1$ git clone 需要克隆的仓库目录 克隆后的目标文件夹名称 四. 克隆不带工作区的版本库我们可以使用--bare选项来创建一个不带工作区的版本库。这对于服务器端的版本库来说是非常有用的。 1git clone --bare &lt;url&gt; &lt;仓库名&gt;","link":"/article/33529.html"},{"title":"十二.查看Git版本演变历史","text":"在提交了若干更新，又或者克隆了某个项目之后，你也许想回顾下提交历史。 完成这个任务最简单而又有效的工具是 git log 命令。 log命令参数 不带参数： git log 会按提交时间列出所有的更新，最近的更新排在最上面。 正如你所看到的，这个命令会列出每个提交的 SHA-1 校验和、作者的名字和电子邮件地址、提交时间以及提交说明。 12345678$ git logcommit e79bd7a65a4446463af93046b2d9c12aac87be81 (HEAD -&gt; master)Author: tianjindong &lt;tianjindong98@qq.com&gt;Date: Wed Mar 20 18:55:31 2019 +0800 modified add_article.html........ --online参数：用于显示简介的日志迭代信息。 12345$ git log --onelinee79bd7a (HEAD -&gt; master) modified add_article.html407b125 dsafc8fa95 add style.css file57e9210 add index.html -n参数：n代表一个整数，表示列出最近n条日志信息 12$ git log -2$ git log --oneline -5 --all参数：默认情况下只查看当前分支的版本演变历史，加上--all参数后就会列出所有分支的版本演变历史。 1$ git log --all -5 --graph参数：在搭配--all参数显示版本演变历史时，由于很多分支在里面，无法看清分支间的关系，我们就可以使用--graph参数，进行图形化显示。 注意：最左边竖着的红线连起来的版本演变历史，就是一个分支的演变历史。 -p参数：用来显示每次提交的内容差异。 查看当前分支所有提交历史1git log &lt;branch-name&gt; 查看b分支相对于a分支到底做出了哪些修改1git log a..b a..b用来表示来自于分支b，但不属于分支a的提交。这样就能清晰的看出分支b相对于分支a到底做出了哪些改变。","link":"/article/47995.html"},{"title":"七.Git的工作区和暂存区（add、rm、commit、status命令）","text":"Git的基本工作流程​ 将文件放入暂存区（add） 第一次加入暂存区（将文件交给Git管理，加入Git跟踪） 1$ git add 文件名或文件夹名 #将指定文件或文件夹放入暂存区 修改文件后，加入暂存区如果我们前面将某个文件（或多个文件）已经加入过暂存区，也就是交给Git管理了，如果我们修改若干个文件，需要再次将其加入暂存区，只需要输入一下命令： 1$ git add -u ​ -u是update的意思，意思是将工作区当中，被Git管理了的文件一起提交到暂存区中。 ​ 交给Git管理的文件，在修改后都最好通过git add命令将其放入暂存区中，然后才能将其正式提交。 文件的移除（rm）解除对文件的跟踪，并删除工作区的文件1$ git rm 文件名 #该命令会将暂存区中的跟踪信息删除，还会删除工作区文件。 123#git rm 文件名 命令相当于rm 文件名 git add -u ​ 如果在删除之前文件已经修改并放入暂存区处于未提交状态，为了安全性，系统是不会让你删除对这个文件的跟踪的（防止数据的丢失），我们可以使用-f选项强制删除。 解除对文件的跟踪，不删除工作区的文件1git rm --cached 文件名 将暂存区文件提交（commit）1$ git commit -m \"提交信息\" #将暂存区中的文件提交 ​ 在上一次暂存区提交操作之后，所有加入暂存区中的“信息”都称为一次集合，每一次提交就是将这一个集合提交上去。 我们可以通过-a选项跳过使用暂存区域： 1$ git commit -am '提交信息' #a选项的作用是先将工作区中所有被跟踪并修改过的文件加入暂存区，然后提交 通过这种方式，工作区中修改的文件哪怕没有放入暂存区，也会被提交。 查看当前Git仓库的状态（status）1$ git status #查看当前git仓库的状态，一定要进入git仓库的工作目录下面，才能使用该命令 状态简览​ git status 命令的输出十分详细，但其用语有些繁琐。 如果你使用 git status -s 命令或 git status --short 命令，你将得到一种更为紧凑的格式输出。 运行 git status -s ，状态报告输出如下： 123456$ git status -sM READMEMM Rakefile #两个M表示在第一修改放入暂存区后，又进行了一次修改，由于放入暂存区的快照是前一个版本，所以此时，老的版本在暂存区中，新的版本还未放入暂存区，在非暂存区中，所以出现了两个M。A lib/git.rbM lib/simplegit.rb?? LICENSE.txt 符号含义如下： ？ 表示未跟踪文件 A 新添加到暂存区中的文件显示为A M 新修改的文件显示为M，出现在右边的 M 表示该文件被修改了但是还没放入暂存，出现在靠左边的 M 表示该文件被修改了并放入了暂存区。","link":"/article/52878.html"},{"title":"六.通过gitignore文件忽略工作区的文件","text":"​ 一般我们总会有些文件无需纳入 Git 的管理，也不希望它们总出现在未跟踪文件列表。 在这种情况下，我们可以创建一个名为 .gitignore 的文件（在.git文件夹同名的目录中创建），列出要忽略的文件模式。 来看一个实际的例子： 123$ cat .gitignore*.[oa]*~ ​ 第一行告诉 Git 忽略所有以 .o 或 .a 结尾的文件。一般这类对象文件和存档文件都是编译过程中出现的。 ​ 第二行告诉 Git 忽略所有以波浪符（~）结尾的文件，许多文本编辑软件（比如 Emacs）都用这样的文件名保存副本。 此外，你可能还需要忽略 log，tmp 或者 pid 目录，以及自动生成的文档等等。 要养成一开始就设置好 .gitignore 文件的习惯，以免将来误提交这类无用的文件。 文件 .gitignore 的格式规范如下： 所有空行或者以 ＃ 开头的行都会被 Git 忽略。 可以使用标准的 glob 模式匹配。 匹配模式可以以（/）开头防止递归（即使我们在该文件中写“src/”这样的路径，也会将所有包含这一名字的目录全部排出，例如：demo/src/也会被忽略，但如果我们写的是“/src/”，那么“demo/src/”不会被排除）。 匹配模式可以以（/）结尾指定目录。 要忽略指定模式以外的文件或目录，可以在模式前加上惊叹号（!）取反。 ​ 所谓的 glob 模式是指 shell 所使用的简化了的正则表达式。 星号（*）匹配零个或多个任意字符；[abc]匹配任何一个列在方括号中的字符（这个例子要么匹配一个 a，要么匹配一个 b，要么匹配一个 c）；问号（?）只匹配一个任意字符；如果在方括号中使用短划线分隔两个字符，表示所有在这两个字符范围内的都可以匹配（比如 [0-9] 表示匹配所有 0 到 9 的数字）。 使用两个星号（*) 表示匹配任意中间目录，比如 a/**/z 可以匹配 a/z , a/b/z 或 a/b/c/z 等。 ​ .gitignore文件可以存放在工作目录任意子目录中，这样一来该文件只能影响该目录下的文件和路径。例如：我们在src下创建一个.gitignore文件，那么此时在这个文件中写*.class，表示src/*.class自动忽略。 我们再看一个 .gitignore 文件的例子： 1234567891011121314151617# no .a files*.a# but do track lib.a, even though you're ignoring .a files above!lib.a# only ignore the TODO file in the current directory, not subdir/TODO/TODO# ignore all files in the build/ directorybuild/# ignore doc/notes.txt, but not doc/server/arch.txtdoc/*.txt# ignore all .pdf files in the doc/ directorydoc/**/*.pdf 注意：添加.gitignore的目的就是避免我们在使用git status命令时，部分文件总是出现在“未跟踪列表”中。这个文件并不是解除Git对这些文件的监控，如果一个文件已经被Git跟踪，是不能通过.gitignore文件解除跟踪的。","link":"/article/54773.html"},{"title":"八.给文件重命名","text":"​ 通常情况，如果我们需要将已经被Git管理的文件重命名，那么我们该怎么做呢？ 方式一：传统方法第一步：将文件重命名1$ mv 原文件名 新文件名 第二步：将重命名的文件加入Git管理1$ git add 新文件名 第三步：解除Git对原文件名的管理1$ git rm 原文件名 示例： 方式二：git命令重命名（推荐）1$ git mv 原文件名 新文件名 示例： 注：mv命令和Linux系统一样实际上是用于移动文件的，但是在不移动文件位置时，也就有了重命名的效果了。","link":"/article/60525.html"},{"title":"二十.Git贮藏工具","text":"有时，当你在项目的一部分上已经工作一段时间后，所有东西都进入了混乱的状态，而这时你想要切换到另一个分支做一点别的事情。 问题是，你不想仅仅因为过会儿回到这一点而为做了一半的工作创建一次提交。 针对这个问题的答案是 git stash 命令。 ​ git stash命令相当于将工作区中的工作树状态保存到一个堆栈中，此时我们就可以切换到另一个分支去进行其他工作了。 将当前工作区状态存入栈中1git stash 查看堆栈中保存的状态1git stash list 将堆栈中的状态应用到工作区中（不删除栈中保存的状态）1git stash apply &lt;贮藏名&gt; ​ 如果想要应用其中一个更旧的储藏，可以通过名字指定它，像这样：git stash apply stash@{2}。 如果不指定一个储藏，Git 认为指定的是最近的储藏： 弹栈操作​ 与apply不同的是，它将状态应用到工作区后，会删除栈顶元素。 1git stash pop 删除栈中保存的某个状态1git stash drop &lt;贮藏名&gt;","link":"/article/26141.html"},{"title":"十七.远程仓库的使用","text":"Git是一个分布式版本控制系统，那么自然会有其他的服务器对我们当前版本库进行备份。通常我们会搭建Git私服或者使用第三方平台（例如：Github、Gitee、Coding等）作为自己本地版本库的备份；或者多个成员在进行协作开发时也需要有一个“中心”服务器作为开发的协作工具，以解决协作开发时代码冲突的解决。 查看远程仓库查看远程服务器的简称： 1git remote 查看远程仓库简写与其对应的 URL： 1git remote -v 查看某一个远程仓库的详细信息： 1git remote show &lt;remote-name&gt; #remote-name是远程服务器的简称，是我们添加远程服务器的时候指定的 添加远程仓库1git remote add &lt;remote-name&gt; &lt;url&gt; #remote-name是这个远程仓库的别名 例如： 1git remote add TestGit https://github.com/tianjindong/TestGit.git ​ 如果你使用 clone 命令克隆了一个仓库，命令会自动将其添加为远程仓库并默认以 “origin” 为简写。 远程仓库的移除与重命名重命名： 1git remote rename &lt;old-name&gt; &lt;new-name&gt; 移除： 1git remote rm &lt;remote-name&gt; 从远程仓库中抓取与拉取1git fetch [remote-name] #remote-name就是我们在添加远程仓库时指定的仓库别名 ​ 必须注意 git fetch 命令会将数据拉取到你的本地仓库 - 它并不会自动合并或修改你当前的工作。 当准备好时你必须手动将其合并入你的工作。 推送到远程仓库1git push &lt;remote-name&gt; &lt;branch-name&gt; #remote-name表示远程仓库的简称，branch-name是分支名 ​ 只有当你有所克隆服务器的写入权限，并且之前没有人推送过时，这条命令才能生效。 当你和其他人在同一时间克隆，他们先推送到上游然后你再推送到上游，你的推送就会毫无疑问地被拒绝。 你必须先将他们的工作拉取下来并将其合并进你的工作后才能推送。","link":"/article/33539.html"},{"title":"九.撤销和回退操作","text":"撤销Git对文件的跟踪有时候我们需要取消Git对某个文件或文件夹的跟踪，那么我们可以使用下列命令： 1git rm --cached 文件名 将暂存区恢复成和HEAD指针快照一样（撤销放入暂存区的修改）​ 如果你已经修改了两个文件并且想要将它们作为两次独立的修改提交，但是却意外地输入了 git add * 暂存了它们两个，你可以撤销暂存区中其中的某一个文件： 1git reset HEAD &lt;文件名&gt; #文件名省略时，表示将暂存区所有文件恢复成和HEAD一样 将工作区的文件恢复成和暂存区一样（撤销工作区的修改）​ 有时候我们修改工作区的某个文件后，如果我们想要回到修改之前（暂存区保存的快照），那么我们使用下面这个命令就能做到： 1git checkout -- 文件名 撤销上一次提交以及上一次提交的提交信息（message）有时候我们提交过后我们发现还有一个东西未提交，需要将新文件纳入上一次提交中： 我们需要将需要将变更放入暂存区： 1git add -u 然后执行： 1git commit --amend #该命令是取消上一次提交，并将上一次提交与这一次提交合并后再重新提交。 ​ 该命令会取消上一次提交，并将暂存区中的文件与上一次提交合并后再提交。 ​ 如果我们只是想修改上一次的提交的备注信息（message），那么我们只需要保证暂存区干净，然后使用该命令即可。因为这样能保证没有新的文件更新加入到上次提交中去，我们只需要修改提交信息即可。** ​ 如果需要修改历史中某一个commit对象的，就需要用变基操作完成：变基 将当前分支指针强制回退到某一次提交​ 需要注意的是：如果回退到指定位置后，原先的commit对象没有被其它分支所指向，那么这些commit对象就会丢失。 1git reset --hard &lt;commit的hash值&gt; 选项： –soft 完全不触及索引文件或工作树。即，暂存区和工作区都不重置（也就是说在回退之间commit对象所包含的操作，都会显示在“Changes to be commited”中），只是将当前分支指针和HEAD指针进行移动。 –mixed 默认值，重置索引，但不重置工作树。即，重置暂存区信息（也就是说在回退之间commit对象所包含的操作，都会显示在“Changes not staged for commit”或“Untracked files”中），不重置工作区文件（工作区文件与原commit保存一置）。 –hard 重置索引和工作树。自以来对工作树中跟踪文件的任何更改都将被丢弃。跟踪文件的任何更改都将被丢弃。","link":"/article/36345.html"},{"title":"十八.远程分支","text":"什么是远程跟踪分支​ 远程跟踪分支是远程分支状态的引用。 它们是你不能移动的本地引用，当你做任何网络通信操作时，它们会自动移动。 远程跟踪分支像是你上次连接到远程仓库时，那些分支所处状态的书签。 ​ 它们以 (remote)/(branch) 形式命名。 例如，如果你想要看你最后一次与远程仓库 origin 通信时 master分支的状态，你可以查看 origin/master 分支。 ​ 远程跟踪分支实质上是用于表示当前远端服务器中分支的状态，但是由于远程跟踪分支不会自动同步远程服务器的内容，也就会导致远程跟踪分支落后于远端服务器对应分支的实际进度。此时我们需要通过fetch命令，抓取远程的最新内容，从而更新远程跟踪分支的状态。 ​ 远程跟踪分支会在用户执行git fetch时全部拉取下来，如果远端服务器没有我们需要分支，换言之，当我们需要在远端创建一个分支时，我们直接在本地分支中通过git push &lt;remote名&gt; &lt;远程分支名&gt;，直接在远端创建一个新的分支。 本地跟踪分支​ 从一个远程跟踪分支检出一个本地分支会自动创建所谓的 “跟踪分支”（它跟踪的分支叫做 “上游分支”）。 跟踪分支是与远程分支有直接关系的本地分支。 如果在一个跟踪分支上输入 git pull，Git 能自动地识别去哪个服务器上抓取、合并到哪个分支。 创建本地跟踪分支： 123git branch &lt;跟踪分支名&gt; &lt;远程跟踪分支名&gt;#或者git checkout -b &lt;本地跟踪分支名&gt; &lt;远程跟踪分支&gt; 如何查看我们的本地分支跟踪哪一个“远程跟踪分支”呢？ 1git branch --vv ​ ​ 在上图中，本地分支master跟踪的就是远程跟踪分支origin/master，可以看到behind 1关键字，你相对于服务器落后一个提交，也就是服务器有一次提交没有合并到本地；如果是ahead 1则表示本地领先远端一次提交，也就是说本地有一次提交未push到远端。ahead和behind的关键词同时出现，表示远端跟踪分支与本地跟踪分支出现了分叉，需要合并两个分支才能提交。 ​ 例如：在上图中的状态远程跟踪分支origin/master和本地跟踪分支master，此时执行git branch -vv就会显示为ahead 2 bhind 2 ​ 需要重点注意的一点是这些数字的值来自于你从每个服务器上最后一次抓取的数据。 这个命令并没有连接服务器，它只会告诉你关于本地缓存的服务器数据。 如果想要统计最新的领先与落后数字，需要在运行此命令前抓取所有的远程仓库。 可以像这样做：$ git fetch --all; git branch -vv 设置已有的本地分支跟踪一个刚刚拉取下来的远程分支： 1git branch --set-upstream-to=&lt;远程跟踪分支名&gt; 拉取操作（fetch,pull）​ git fetch 命令从服务器上抓取本地没有的数据时，它并不会修改工作目录中的内容。 它只会获取数据然后让你自己合并。 然而，有一个命令叫作 git pull 在大多数情况下它的含义是一个 git fetch 紧接着一个 git merge 命令。 如果有一个像之前章节中演示的设置好的跟踪分支，不管它是显式地设置还是通过 clone 或 checkout 命令为你创建的，git pull 都会查找当前分支所跟踪的服务器与分支，从服务器上抓取数据然后尝试合并入那个远程分支。 ​ pull = fetch + merge. 推送操作（push）当我们需要将自己本地跟踪分支，推送到远端时，我们直接使用： 1git push 由于我们在创建本地跟踪分支时，绑定了远端跟踪分支，所以系统就知道将当前分支的最新内容与远端的那个分支进行合并。 例： 在上图中，我们在master分支中执行git push后分支树就变成： 新建远端分支如果是我们新创建的一个本地分支，在远端服务器没有对应的分支，此时我们需要在远端创建一个分支，然后把提交推送到这个新分支上去： 1git push &lt;remote-name&gt; &lt;branch-name&gt; #执行该命令，系统会自动查看remote远程服务器是否存在名为&lt;branch-name&gt;的分支，如果不存在则自动创建一个，然后将本地分支的内容push到远端。 需要注意的是：branch-name通常要与本地分支名相同。 删除远程分支​ 如果我们想要删除远端的某个分支，可以运行带有 --delete 选项的 git push 命令来删除一个远程分支。 如果想要从服务器上删除 serverfix 分支，运行下面的命令： 1git push origin --delete serverfix 删除远程分支后，本机会直接显示远程分支的删除，但是在别的主机上，是无法通过fetch操作发现这个远程分支被删除的，我们必须使用： 1git remote prune &lt;remote-name&gt; 远端服务器的具体操作演示第一步：抓取远程跟踪分支1git fetch 第二步：查看远程跟踪分支1234git branch -avremotes/origin/master 341eeb7 Update README.md file #以remotes开头的分支就是远程跟踪分支remotes/origin/test2 cc73e38 Create a.txt file 第三步：使本地分支跟踪远程跟踪分​ 需要明确的是，只有本地跟踪分支才能提交到远端，如果要将本地分支变成本地跟踪分支有多种方式： 1234#方式一：直接设置远程跟踪分支git branch --set-upstream-to=origin/xxx#方式二：push本地分支时，指定远程服务器名和远程分支名，这样系统会自动使当前本地分支跟踪这个远程分支git push &lt;remote-name&gt; &lt;branch-name&gt; 第四步：push到远端在下图中，origin/master是“远程跟踪分支”，而master是“本地跟踪分支”，本地跟踪分支进行两步操作后指向893cf。 但是此时另一个用户提交了自己本地跟踪分支的内容，使得实际提交历史成了这样： 如果我们现在去git push服务器会拒绝我们的请求，因为此时远端分支不是“快进”操作了。 此时我们需要将master分支与origin/master分支进行合并，然后重新push到远端。","link":"/article/42736.html"},{"title":"十九.禁用push-f操作","text":"通常情况下，我们都是禁用push -f操作的，因为它可能导致远端服务器的提交历史丢失。 例如在远端此时的情况如上图所示，如果我们使用git reset --hard 0b743使得本地跟踪分支回退到历史的某个版本，也就是变成了这样： 如果我们此时执行git push -f &lt;remote-name&gt; &lt;branch-name&gt; 进行强行推送的话，那么此时远端的master分支也会回退到0b743： 1$ git push -f origin master","link":"/article/62026.html"},{"title":"一.什么是版本控制","text":"集中式版本控制系统（CVCS）​ 集中化的版本控制系统（Centralized Version Control Systems，简称 CVCS）应运而生。 这类系统，诸如 CVS、Subversion 以及 Perforce 等，都有一个单一的集中管理的服务器，保存所有文件的修订版本，而协同工作的人们都通过客户端连到这台服务器，取出最新的文件或者提交更新。 多年以来，这已成为版本控制系统的标准做法。 ​ 优点：每个人都可以在一定程度上看到项目中的其他人正在做些什么。 而管理员也可以轻松掌控每个开发者的权限，并且管理一个 CVCS 要远比在各个客户端上维护本地数据库来得轻松容易。 ​ 缺点：中央服务器存在单点故障。 如果宕机一小时，那么在这一小时内，谁都无法提交更新，也就无法协同工作。 如果中心数据库所在的磁盘发生损坏，又没有做恰当备份，毫无疑问你将丢失所有数据——包括项目的整个变更历史，只剩下人们在各自机器上保留的单独快照。 本地版本控制系统也存在类似问题，只要整个项目的历史记录被保存在单一位置，就有丢失所有历史更新记录的风险。 分布式版本控制系统（DVCS）​ 分布式版本控制系统（Distributed Version Control System，简称 DVCS）面世了。 在这类系统中，像 Git、Mercurial、Bazaar 以及 Darcs 等，客户端并不只提取最新版本的文件快照，而是把代码仓库完整地镜像下来。 这么一来，任何一处协同工作用的服务器发生故障，事后都可以用任何一个镜像出来的本地仓库恢复。 因为每一次的克隆操作，实际上都是一次对代码仓库的完整备份。 SVN版本控制系统与Git的区别 SVN是集中式版本控制系统，Git是分布式版本控制系统，也就是每一个Git节点都保存相同的版本库，理论上来说所有Git仓库都是平等的，但是在实际开发中会有一个主仓库。 SVN的版本控制是通过增量记录来实现的，而Git是通过保存更改文件快照实现的。","link":"/article/57074.html"},{"title":"二.Git的基本概念","text":"版本控制采用：直接记录快照，而非差异比较​ Git 和其它版本控制系统（包括 Subversion 和近似工具）的主要差别在于 Git 对待数据的方法。 概念上来区分，其它大部分系统以文件变更列表的方式存储信息。 这类系统（CVS、Subversion、Perforce、Bazaar 等等）将它们保存的信息看作是一组基本文件和每个文件随时间逐步累积的差异。 ​ 差异式比较示意图： ​ Git 不按照以上方式对待或保存数据。 反之，Git 更像是把数据看作是对小型文件系统的一组快照。 每次你提交更新，或在 Git 中保存项目状态时，它主要对当时的全部文件制作一个快照并保存这个快照的索引。 为了高效，如果文件没有修改，Git 不再重新存储该文件，而是只保留一个链接指向之前存储的文件。 Git 对待数据更像是一个 快照流。 ​ 快照记录示意图： Git 保证完整性​ Git 中所有数据在存储前都计算校验和，然后以校验和来引用。 这意味着不可能在 Git 不知情时更改任何文件内容或目录内容。 这个功能建构在 Git 底层，是构成 Git 哲学不可或缺的部分。 若你在传送过程中丢失信息或损坏文件，Git 就能发现。 ​ Git 用以计算校验和的机制叫做 SHA-1 散列（hash，哈希）。 这是一个由 40 个十六进制字符（0-9 和 a-f）组成字符串，基于 Git 中文件的内容或目录结构计算出来。 SHA-1 哈希看起来是这样： 124b9da6552252987aa493b52f8696cd6d3b00373 ​ Git 中使用这种哈希值的情况很多，你将经常看到这种哈希值。 实际上，Git 数据库中保存的信息都是以文件内容的哈希值来索引，而不是文件名。 Git 一般只添加数据​ 你执行的 Git 操作，几乎只往 Git 数据库中增加数据。 很难让 Git 执行任何不可逆操作，或者让它以任何方式清除数据。 同别的 VCS 一样，未提交更新时有可能丢失或弄乱修改的内容；但是一旦你提交快照到 Git 中，就难以再丢失数据，特别是如果你定期的推送数据库到其它仓库的话。 Git的三种状态​ Git 有三种状态，你的文件可能处于其中之一：已提交（committed）、已修改（modified）和已暂存（staged）。 已提交表示数据已经安全的保存在本地数据库中。 已修改表示修改了文件，但还没保存到数据库中。 已暂存表示对一个已修改文件的当前版本做了标记，使之包含在下次提交的快照中。 Git 仓库目录（工作目录） ：Git 用来保存项目的元数据和对象数据库的地方。 这是 Git 中最重要的部分，从其它计算机克隆仓库时，拷贝的就是这里的数据。 工作目录：工作目录是对项目的某个版本独立提取出来的内容。 这些从 Git 仓库的压缩数据库中提取出来的文件，放在磁盘上供你使用或修改。 暂存区域：暂存区域是一个文件，保存了下次将提交的文件列表信息，一般在 Git 仓库目录中。 有时候也被称作`‘索引’’，不过一般说法还是叫暂存区域。 Git工作流程 在工作目录中修改文件。 暂存文件，将文件的快照放入暂存区域。 提交更新，找到暂存区域的文件，将快照永久性存储到 Git 仓库目录。 ​ 如果 Git 目录中保存着的特定版本文件，就属于已提交状态。 如果作了修改并已放入暂存区域，就属于已暂存状态。 如果自上次取出后，作了修改但还没有放到暂存区域，就是已修改状态。","link":"/article/64185.html"},{"title":"三.Git的初始配置","text":"​ 当安装完 Git 应该做的第一件事就是设置你的用户名称与邮件地址。 这样做很重要，因为每一个 Git 的提交都会使用这些信息，并且它会写入到你的每一次提交中，不可更改。 通过config命令设置用户信息：12$ git config --global user.name \"John Doe\"$ git config --global user.email johndoe@example.com ​ 如果使用了 --global 选项，那么该命令只需要运行一次，因为之后无论你在该系统上做任何事情， Git 都会使用那些信息。 当你想针对特定项目使用不同的用户名称与邮件地址时，可以在那个项目目录下运行没有 --global 选项的命令来配置（也就是--local的作用域设置）。 ​ 注意：很多 GUI 工具都会在第一次运行时帮助你配置这些信息。 更改Git默认调用的编辑器环境​ 默认情况下，Git 会调用环境变量（$VISUAL 或 $EDITOR）设置的任意文本编辑器，如果没有设置，会调用 vi 来创建和编辑你的提交以及标签信息。 你可以使用 core.editor 选项来修改默认的编辑器： 1$ git config --global core.editor vim config的三个作用域： --local local只对某一个仓库有效（默认） --global global对当前用户的所有仓库有效 --system system对系统登录的所有用户都有效 如果设置时不指定作用域，默认就是--local作用域 config命令的参数：​ 在命令行中直接输入git config就能看到config命令的所有参数了。 查看设置的信息一. 查看当前所有配置信息12#查看所有配置项$ git config --list 同时还可在命令后面加上“作用域”选项： 123$ git config --list --local$ git config --list --global$ git config --list --system 示例： 二. 查看某一项配置信息你还可以通过输入git config &lt;key&gt;来查看某一项的配置值： 12#查看user.name的值$ git config user.name","link":"/article/46264.html"},{"title":"四.获取Git命令帮助","text":"在日常使用中，我们不可能记住所有的命令和参数，我们可以通过一些命令查看Git命令的帮助说明。 123$ git help &lt;verb&gt;$ git &lt;verb&gt; --help$ man git-&lt;verb&gt; 获取系统所有命令帮助如果我们想查看Git所有命令的粗略作用，则使用git help即可： 获取特定命令帮助如果我们想要想获得某个命令的 详细手册，例如获取config命令的帮组，执行： 12$ git help config$ git config --help #两种方式都可以 执行命令后，不会在命令行中显示文档，而是会通过系统默认浏览器弹出config命令的帮助文档：","link":"/article/7307.html"},{"title":"一文看懂Spring事务的七种传播行为（转）","text":"什么叫事务传播行为？听起来挺高端的，其实很简单。 即然是传播，那么至少有两个东西，才可以发生传播。单体不存在传播这个行为。 事务传播行为（propagation behavior）指的就是当一个事务方法被另一个事务方法调用时，这个事务方法应该如何进行。 例如：methodA事务方法调用methodB事务方法时，methodB是继续在调用者methodA的事务中运行呢，还是为自己开启一个新事务运行，这就是由methodB的事务传播行为决定的。 Spring定义了七种传播行为 PROPAGATION_REQUIRED如果存在一个事务，则支持当前事务。如果没有事务则开启一个新的事务。 可以把事务想像成一个胶囊，在这个场景下方法B用的是方法A产生的胶囊（事务）。 举例有两个方法： 12345678910@Transactional(propagation = Propagation.REQUIRED)public void methodA() { methodB(); // do something}@Transactional(propagation = Propagation.REQUIRED)public void methodB() { // do something} 单独调用methodB方法时，因为当前上下文不存在事务，所以会开启一个新的事务。 调用methodA方法时，因为当前上下文不存在事务，所以会开启一个新的事务。当执行到methodB时，methodB发现当前上下文有事务，因此就加入到当前事务中来。 PROPAGATION_SUPPORTS​ 如果存在一个事务，支持当前事务。如果没有事务，则非事务的执行。但是对于事务同步的事务管理器，PROPAGATION_SUPPORTS与不使用事务有少许不同。 举例有两个方法： 1234567891011@Transactional(propagation = Propagation.REQUIRED)public void methodA() { methodB(); // do something}// 事务属性为SUPPORTS@Transactional(propagation = Propagation.SUPPORTS)public void methodB() { // do something} 单纯的调用methodB时，methodB方法是非事务的执行的。当调用methdA时,methodB则加入了methodA的事务中,事务地执行。 PROPAGATION_MANDATORY如果已经存在一个事务，支持当前事务。如果没有一个活动的事务，则抛出异常。 1234567891011@Transactional(propagation = Propagation.REQUIRED)public void methodA() { methodB(); // do something}// 事务属性为MANDATORY@Transactional(propagation = Propagation.MANDATORY)public void methodB() { // do something} 当单独调用methodB时，因为当前没有一个活动的事务，则会抛出异常throw new IllegalTransactionStateException(“Transaction propagation ‘mandatory’ but no existing transaction found”);当调用methodA时，methodB则加入到methodA的事务中，事务地执行。 PROPAGATION_REQUIRES_NEW 使用PROPAGATION_REQUIRES_NEW,需要使用 JtaTransactionManager作为事务管理器。 它会开启一个新的事务。如果一个事务已经存在，则先将这个存在的事务挂起。 12345678910111213@Transactional(propagation = Propagation.REQUIRED)public void methodA() { doSomeThingA(); methodB(); doSomeThingB(); // do something else}// 事务属性为REQUIRES_NEW@Transactional(propagation = Propagation.REQUIRES_NEW)public void methodB() { // do something} 当调用 123public static void main(String[] args){ methodA();} 相当于调用： 1234567891011121314151617181920212223242526272829public static void main(String[] args){ TransactionManager tm = null; try{ //获得一个JTA事务管理器 tm = getTransactionManager(); tm.begin();//开启一个新的事务 Transaction ts1 = tm.getTransaction(); doSomeThing(); tm.suspend();//挂起当前事务 try{ tm.begin();//重新开启第二个事务 Transaction ts2 = tm.getTransaction(); methodB(); ts2.commit();//提交第二个事务 } Catch(RunTimeException ex) { ts2.rollback();//回滚第二个事务 } finally { //释放资源 } //methodB执行完后，恢复第一个事务 tm.resume(ts1); doSomeThingB(); ts1.commit();//提交第一个事务 } catch(RunTimeException ex) { ts1.rollback();//回滚第一个事务 } finally { //释放资源 }} ​ 在这里，我把ts1称为外层事务，ts2称为内层事务。从上面的代码可以看出，ts2与ts1是两个独立的事务，互不相干。Ts2是否成功并不依赖于 ts1。如果methodA方法在调用methodB方法后的doSomeThingB方法失败了，而methodB方法所做的结果依然被提交。而除了 methodB之外的其它代码导致的结果却被回滚了。 PROPAGATION_NOT_SUPPORTEDPROPAGATION_NOT_SUPPORTED 总是非事务地执行，并挂起任何存在的事务。使用PROPAGATION_NOT_SUPPORTED,也需要使用JtaTransactionManager作为事务管理器。 PROPAGATION_NEVER总是非事务地执行，如果存在一个活动事务，则抛出异常。 PROPAGATION_NESTED 如果一个活动的事务存在，则运行在一个嵌套的事务中。 如果没有活动事务, 则按TransactionDefinition.PROPAGATION_REQUIRED 属性执行。 这是一个嵌套事务,使用JDBC 3.0驱动时,仅仅支持DataSourceTransactionManager作为事务管理器。 需要JDBC 驱动的java.sql.Savepoint类。使用PROPAGATION_NESTED，还需要把PlatformTransactionManager的nestedTransactionAllowed属性设为true(属性值默认为false)。 这里关键是嵌套执行。 1234567891011@Transactional(propagation = Propagation.REQUIRED)public void methodA(){ doSomeThingA(); methodB(); doSomeThingB();}@Transactional(propagation = Propagation.NEWSTED)public void methodB(){ ……} 如果单独调用methodB方法，则按REQUIRED属性执行。如果调用methodA方法，相当于下面的效果： 1234567891011121314151617181920212223public static void main(String[] args){ Connection con = null; Savepoint savepoint = null; try{ con = getConnection(); con.setAutoCommit(false); doSomeThingA(); savepoint = con.setSavepoint(); try{ methodB(); } catch(RuntimeException ex) { con.rollback(savepoint); } finally { //释放资源 } doSomeThingB(); con.commit(); } catch(RuntimeException ex) { con.rollback(); } finally { //释放资源 }} 当methodB方法调用之前，调用setSavepoint方法，保存当前的状态到savepoint。如果methodB方法调用失败，则恢复到之前保存的状态。但是需要注意的是，这时的事务并没有进行提交，如果后续的代码(doSomeThingB()方法)调用失败，则回滚包括methodB方法的所有操作。嵌套事务一个非常重要的概念就是内层事务依赖于外层事务。外层事务失败时，会回滚内层事务所做的动作。而内层事务操作失败并不会引起外层事务的回滚。 PROPAGATION_NESTED 与PROPAGATION_REQUIRES_NEW的区别它们非常类似,都像一个嵌套事务，如果不存在一个活动的事务，都会开启一个新的事务。使用 PROPAGATION_REQUIRES_NEW时，内层事务与外层事务就像两个独立的事务一样，一旦内层事务进行了提交后，外层事务不能对其进行回滚。两个事务互不影响。两个事务不是一个真正的嵌套事务。同时它需要JTA事务管理器的支持。 使用PROPAGATION_NESTED时，外层事务的回滚可以引起内层事务的回滚。而内层事务的异常并不会导致外层事务的回滚，它是一个真正的嵌套事务。DataSourceTransactionManager使用savepoint支持PROPAGATION_NESTED时，需要JDBC 3.0以上驱动及1.4以上的JDK版本支持。其它的JTATrasactionManager实现可能有不同的支持方式。 PROPAGATION_REQUIRES_NEW 启动一个新的, 不依赖于环境的 “内部” 事务. 这个事务将被完全 commited 或 rolled back 而不依赖于外部事务, 它拥有自己的隔离范围, 自己的锁, 等等. 当内部事务开始执行时, 外部事务将被挂起, 内务事务结束时, 外部事务将继续执行。 另一方面, PROPAGATION_NESTED 开始一个 “嵌套的” 事务, 它是已经存在事务的一个真正的子事务. 潜套事务开始执行时, 它将取得一个 savepoint. 如果这个嵌套事务失败, 我们将回滚到此 savepoint. 潜套事务是外部事务的一部分, 只有外部事务结束后它才会被提交。 由此可见, PROPAGATION_REQUIRES_NEW 和 PROPAGATION_NESTED 的最大区别在于, PROPAGATION_REQUIRES_NEW 完全是一个新的事务, 而 PROPAGATION_NESTED 则是外部事务的子事务, 如果外部事务 commit, 嵌套事务也会被 commit, 这个规则同样适用于 roll back。 本文转载至：https://blog.csdn.net/soonfly/article/details/70305683","link":"/article/51207.html"},{"title":"MyBatis抛出BindingException异常可能是你忘了配置资源拷贝","text":"最近博主在搭建Mybatis项目时遇到了一问题，在一切配置妥当后，开始运行测试代码，但是此时控制台无情的抛出了异常： 开始寻找问题根源咦？难道是我哪里写错，我的第一反应是我的xml配置文件哪里写错了，我开始检查下面几个点： XML文件是否与Mapper代码在通过一个包中。 XML配置文件文件名是否与Mapper代码文件名相同。（在使用设置mapper配置文件位置时，必须要保证第一点和第二点） Mapper.xml文件中的namespace与mapper接口的类路径相同。 Mapper接口方法名和Mapper.xml中定义的每个statement的id相同 。 Mapper接口方法的输入参数类型和mapper.xml中定义的每个sql 的parameterType的类型相同。 Mapper接口方法的输出参数类型和mapper.xml中定义的每个sql的resultType的类型相同。 ​ 检查完成后，我发现都没有问题，那就奇怪了，这问题到底出在哪了呢？此时我看来一下编译后的目标文件夹中，发现我的mapper配置文件并没有被编译进去。 原来在典型的maven工程中，目录结构有：src/main/java和src/main/resources，前者是用来存放java源代码的，后者则是存放一些资源文件，比如配置文件等，在默认的情况下maven打包的时候，对于src/main/java目录只打包源代码，而不会打包其他文件。所以此时如果把对应的mapper文件放到src/main/java目录下时，不会打包到最终的jar文件夹中，也不会输出到target文件夹中，由于在进行单元测试的时候执行的是/target目录下/test-classes下的代码，所以在测试的时候也不会成功。 如何解决资源拷贝问题方式一：默认Maven构建为了实现在maven默认环境下打包时，Mybatis的接口和mapper文件在同一包中，可以通过将接口文件放在src/main/java某个包中，而在src/main/resources目录中建立同样的包，这是一种约定优于配置的方式，这样在maven打包的时候就会将src/main/java和src/main/resources相同包下的文件合并到同一包中。 方式二：更改Maven的构建配置我们需要在Maven项目的pom.xml文件中配置下列代码： 1234567891011&lt;build&gt; &lt;resources&gt; &lt;resource&gt; &lt;directory&gt;src/main/java&lt;/directory&gt; &lt;includes&gt; &lt;include&gt;**/*.xml&lt;/include&gt; &lt;/includes&gt; &lt;filtering&gt;false&lt;/filtering&gt; &lt;/resource&gt; &lt;/resources&gt;&lt;/build&gt; 采用上述方法中任意一个，然后重新运行测试代码，BindingException异常的问题就解决了。","link":"/article/54018.html"},{"title":"MyBatis与Spring整合步骤","text":"在前段的工作学习中，自己大多数都在使用MyBatis，但是最近有独立搭建SSM环境的需求，所以博主就想着写一篇框架整合的博文，对整合过程进行一个总结： 第一步：引入jar包123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119&lt;properties&gt; &lt;junit.version&gt;4.12&lt;/junit.version&gt; &lt;spring.version&gt;4.2.4.RELEASE&lt;/spring.version&gt; &lt;mybatis.version&gt;3.2.8&lt;/mybatis.version&gt; &lt;mybatis.spring.version&gt;1.2.2&lt;/mybatis.spring.version&gt; &lt;mybatis.paginator.version&gt;1.2.15&lt;/mybatis.paginator.version&gt; &lt;pagehelper.version&gt;3.4.2-fix&lt;/pagehelper.version&gt; &lt;mysql.version&gt;5.1.32&lt;/mysql.version&gt; &lt;druid.version&gt;1.0.9&lt;/druid.version&gt; &lt;jedis.version&gt;2.7.2&lt;/jedis.version&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;!-- Spring --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-context&lt;/artifactId&gt; &lt;version&gt;${spring.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-beans&lt;/artifactId&gt; &lt;version&gt;${spring.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-jdbc&lt;/artifactId&gt; &lt;version&gt;${spring.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-test&lt;/artifactId&gt; &lt;version&gt;${spring.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-aspects&lt;/artifactId&gt; &lt;version&gt;${spring.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-jms&lt;/artifactId&gt; &lt;version&gt;${spring.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-context-support&lt;/artifactId&gt; &lt;version&gt;${spring.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-tx&lt;/artifactId&gt; &lt;version&gt;${spring.version}&lt;/version&gt; &lt;/dependency&gt; &lt;!-- log4j --&gt; &lt;dependency&gt; &lt;groupId&gt;log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j&lt;/artifactId&gt; &lt;version&gt;1.2.17&lt;/version&gt; &lt;/dependency&gt; &lt;!-- Mybatis --&gt; &lt;dependency&gt; &lt;groupId&gt;org.mybatis&lt;/groupId&gt; &lt;artifactId&gt;mybatis&lt;/artifactId&gt; &lt;version&gt;${mybatis.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.mybatis&lt;/groupId&gt; &lt;artifactId&gt;mybatis-spring&lt;/artifactId&gt; &lt;version&gt;${mybatis.spring.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.github.pagehelper&lt;/groupId&gt; &lt;artifactId&gt;pagehelper&lt;/artifactId&gt; &lt;version&gt;${pagehelper.version}&lt;/version&gt; &lt;/dependency&gt; &lt;!--MyBatisPlus--&gt; &lt;dependency&gt; &lt;groupId&gt;com.baomidou&lt;/groupId&gt; &lt;artifactId&gt;mybatis-plus&lt;/artifactId&gt; &lt;version&gt;3.1.2&lt;/version&gt; &lt;/dependency&gt; &lt;!-- MySql --&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;${mysql.version}&lt;/version&gt; &lt;/dependency&gt; &lt;!-- 连接池 --&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;druid&lt;/artifactId&gt; &lt;version&gt;${druid.version}&lt;/version&gt; &lt;/dependency&gt; &lt;!--lombok--&gt; &lt;dependency&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &lt;version&gt;1.18.8&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;!-- 加载配置文件 --&gt; &lt;resources&gt; &lt;resource&gt; &lt;directory&gt;src/main/java&lt;/directory&gt; &lt;includes&gt; &lt;include&gt;**/*.xml&lt;/include&gt; &lt;/includes&gt; &lt;/resource&gt; &lt;/resources&gt; &lt;/build&gt; 注意最后&lt;resource&gt;标签的配置是非常必要的，在默认构建模式下，放在mapper包中的配置文件并不会编译到目标目录中去，就会导致MyBatis搭建错误，详见：MyBatis抛出BindingException异常可能是你忘了配置资源拷贝 第二步：编写实体类与Mapper接口 第三步：配置Mapper配置文件Mapper配置文件需要满足一下要求： Mapper配置文件与Mapper接口在一个包中 Mapper配置文件名与Mapper接口名相同 Mapper配置文件中namespace填写为Mapper接口的全限定类名 Mapper配置文件配置sql语句的标签的id要与Mapper接口中方法名相同 1234567&lt;?xml version=\"1.0\" encoding=\"UTF-8\" ?&gt;&lt;!DOCTYPE mapper PUBLIC \"-//mybatis.org//DTD Mapper 3.0//EN\" \"http://mybatis.org/dtd/mybatis-3-mapper.dtd\" &gt;&lt;mapper namespace=\"com.tjd.spring_mybatis_plus.mapper.UserMapper\"&gt; &lt;select id=\"getUserById\" resultType=\"com.tjd.spring_mybatis_plus.pojo.User\" parameterType=\"long\"&gt; select * from user where uid = #{id} &lt;/select&gt;&lt;/mapper&gt; 第四步：配置MyBatis核心配置文件在resource/目录下创建SqlMapConfig.xml文件，SqlMapConfig.xml配置文件具体内容参考：MyBatis官网 1234567891011121314&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;!DOCTYPE configuration PUBLIC \"-//mybatis.org//DTD Config 3.0//EN\" \"http://mybatis.org/dtd/mybatis-3-config.dtd\"&gt;&lt;configuration&gt; &lt;!--开启日志输出--&gt; &lt;settings&gt; &lt;setting name=\"logImpl\" value=\"STDOUT_LOGGING\" /&gt; &lt;/settings&gt; &lt;!--配置类别名，配置后在Mapper配置文件（通常我们将编写SQL语句的配置文件成为Mapper配置文件）中需要使用pojo包中的类时，使用简单类名即可--&gt; &lt;typeAliases&gt; &lt;package name=\"com.tjd.spring_mybatis_plus.pojo\"/&gt; &lt;/typeAliases&gt;&lt;/configuration&gt; 第五步：配置Spring核心配置文件在resource/目录下创建applicationContext.xml文件 12345678910111213141516171819202122232425262728293031323334&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;beans xmlns=\"http://www.springframework.org/schema/beans\" xmlns:context=\"http://www.springframework.org/schema/context\" xmlns:p=\"http://www.springframework.org/schema/p\" xmlns:aop=\"http://www.springframework.org/schema/aop\" xmlns:tx=\"http://www.springframework.org/schema/tx\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-4.2.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context-4.2.xsd http://www.springframework.org/schema/aop http://www.springframework.org/schema/aop/spring-aop-4.2.xsd http://www.springframework.org/schema/tx http://www.springframework.org/schema/tx/spring-tx-4.2.xsd http://www.springframework.org/schema/util http://www.springframework.org/schema/util/spring-util-4.2.xsd\"&gt; &lt;!--配置包扫描器，配置后会自动扫描指定包下面，由注解注册的Bean--&gt; &lt;context:component-scan base-package=\"com.tjd.spring_mybatis_plus\"&gt;&lt;/context:component-scan&gt; &lt;!-- 数据库连接池 --&gt; &lt;bean id=\"dataSource\" class=\"com.alibaba.druid.pool.DruidDataSource\" destroy-method=\"close\"&gt; &lt;property name=\"url\" value=\"jdbc:mysql://localhost:3306/test\" /&gt; &lt;property name=\"username\" value=\"root\" /&gt; &lt;property name=\"password\" value=\"xxx\" /&gt; &lt;property name=\"driverClassName\" value=\"com.mysql.jdbc.Driver\" /&gt; &lt;/bean&gt; &lt;!-- 让spring管理sqlsessionfactory 使用mybatis和spring整合包中的 --&gt; &lt;bean id=\"sqlSessionFactory\" class=\"org.mybatis.spring.SqlSessionFactoryBean\"&gt; &lt;!-- 数据库连接池 --&gt; &lt;property name=\"dataSource\" ref=\"dataSource\" /&gt; &lt;!-- 加载mybatis的全局配置文件 --&gt; &lt;property name=\"configLocation\" value=\"classpath:SqlMapConfig.xml\" /&gt; &lt;/bean&gt; &lt;!--BeanDefinitionRegistryPostProcessor，它从一个基本包开始递归地搜索接口，并将它们注册为MapperFactoryBean。--&gt; &lt;bean class=\"org.mybatis.spring.mapper.MapperScannerConfigurer\"&gt; &lt;property name=\"basePackage\" value=\"com.tjd.spring_mybatis_plus.mapper\" /&gt; &lt;/bean&gt;&lt;/beans&gt; 需要注意的是，在applicationContext.xml中最后配置的MapperScannerConfigurer，它的作用是从指定的包开始递归地搜索接口，并将它们注册为MapperFactoryBean。注意，只有具有至少一个方法的接口才会被注册;具体类将被忽略。并且会根据Mapper接口找到对应的映射文件，这也就是为什么前面要求： Mapper配置文件与Mapper接口在一个包中 Mapper配置文件名与Mapper接口名相同 因为只有这样才能让MapperScannerConfigurer在找到接口时，顺利的找到接口对应的配置文件。 什么是MapperFactoryBean那什么是MapperFactoryBean，它是我们Mapper接口的实质代理实现类，如果不在applicationContext.xml不配置MapperScannerConfigurer，那么我们就需要手动配置UserMapper接口的实现类，示例如下： 1234&lt;bean id=\"userMapper\" class=\"org.mybatis.spring.mapper.MapperFactoryBean\"&gt; &lt;property name=\"sqlSessionFactory\" ref=\"sqlSessionFactory\"&gt;&lt;/property&gt; &lt;property name=\"mapperInterface\" value=\"com.tjd.spring_mybatis_plus.mapper.UserMapper\"&gt;&lt;/property&gt;&lt;/bean&gt; 但是如果采用这种配置方式，那么每个接口都需要配置一次，会很麻烦，如果我们配置MapperScannerConfigurer那么只需要指定包名，就能将该包下所有接口注册为MapperFactoryBean。 如何使接口与映射配置文件分离由于我们在spring配置文件中配置了MapperScannerConfigurer，实质上我们并不需要在MyBatis核心配置文件中指定映射文件的路径，因为他找到接口后自然就能找到配置文件；但是如果想要使得Mapper配置文件与接口分开，我们就需要进行额外的配置： 方式一在Spring配置文件配置SqlSessionFactoryBean时通过mapperLocations属性指定映射文件路径 123456789&lt;!-- 让spring管理sqlsessionfactory 使用mybatis和spring整合包中的 --&gt;&lt;bean id=\"sqlSessionFactory\" class=\"org.mybatis.spring.SqlSessionFactoryBean\"&gt; &lt;!-- 数据库连接池 --&gt; &lt;property name=\"dataSource\" ref=\"dataSource\" /&gt; &lt;!-- 加载mybatis的全局配置文件 --&gt; &lt;property name=\"configLocation\" value=\"classpath:SqlMapConfig.xml\" /&gt; &lt;!--指定--&gt; &lt;property name=\"mapperLocations\" value=\"com/tjd/spring_mybatis_plus/pojo/*.xml\"&gt;&lt;/property&gt;&lt;/bean&gt; 方式二那么可以在MyBatis核心配置文件中采用&lt;mapper resource&gt;或&lt;mapper url&gt;载入配置文件。 123&lt;mappers&gt; &lt;mapper resource=\"com/tjd/spring_mybatis_plus/pojo/UserMapper.xml\"&gt;&lt;/mapper&gt;&lt;/mappers&gt; 配置文件与接口分离后，配置文件名也就没有与接口名相同的需求了。","link":"/article/41666.html"},{"title":"MyBatis延迟加载策略","text":"延迟加载就是在需要用到数据的时候才进行加载，不需要用到数据的时候就不加载数据。延迟加载也称为懒加载。 优点：在使用关联对象时，才从数据库中查询关联数据，大大降低数据库不必要开销。 缺点：因为只有当需要用到数据时，才会进行数据库查询，这样在大批量数据查询时，因为查询工作也需要耗费时间，所以可能造成用户等待时间变长，造成用户体验下降。 数据库模型准备下面我们给出的就是一个数据库关系模型，在后面的例子中一这两个表为基础讲解MyBatis延迟加载。我们假定Article（文章）与Tag（标签）是一对多的关系。 注意面这段话的表述：表的关联关系大致可以分为四种：一对一、多对一、一对多、多对多，但是实质上从单独一个表的角度上来看只存在一对一和一对多关系；而一对一和一对多的关系都能通过下列两个表来表示，以Article表的角度上来看，一个Article数据可以由多个Tag数据行对应，这就是一对多的关系；而一个Tag数据只能与一个Article关联，这就是一对一的关系。所以了解了MyBatis一对多的延迟加载的配置（双向）也就学会了四种关联模式的配置。 第一步：配置MyBatis核心配置文件如果想使用延迟加载策略，就需要在MyBatis全局配置文件中开启延迟加载策略： 参数详情参考官方文档：settings配置 我们在MyBatis全局配置文件（SqlMapConfig.xml）中添加下列代码： 1234&lt;settings&gt; &lt;setting name=\"lazyLoadingEnabled\" value=\"true\"/&gt; &lt;setting name=\"aggressiveLazyLoading\" value=\"true\"/&gt;&lt;/settings&gt; 第二步：配置映射文件预先定义ArticleMapper和TagMapper接口ArticleMapper接口： 123public interface ArticleMapper { Article getArticleById(Long id);} TagMapper接口： 1234public interface TagMapper { Tag getTagById(Long id); List&lt;Tag&gt; getTagsByArticleId(Long id);} 配置ArticleMapper.xml映射文件（一对多配置）在不使用延迟加载的情况下，我们通常使用的是关联查询，直接查出关联对象的数据： 123456789101112131415161718192021222324252627&lt;mapper namespace=\"com.tjd.spring_mybatis_plus.mapper.ArticleMapper\"&gt; &lt;resultMap id=\"articleMap\" type=\"Article\"&gt; &lt;id column=\"id\" property=\"id\"&gt;&lt;/id&gt; &lt;result column=\"title\" property=\"title\"&gt;&lt;/result&gt; &lt;result column=\"content\" property=\"content\"&gt;&lt;/result&gt; &lt;collection property=\"tags\" ofType=\"Tag\"&gt; &lt;id column=\"tid\" property=\"id\"&gt;&lt;/id&gt; &lt;result column=\"tcontent\" property=\"content\"&gt;&lt;/result&gt; &lt;/collection&gt; &lt;/resultMap&gt; &lt;select id=\"getArticleById\" resultMap=\"articleMap\" parameterType=\"long\"&gt; SELECT a.id id, a.title title, a.content content, t.id tid, t.content tcontent FROM article a LEFT JOIN tag t ON a.id = t.article_id WHERE a.id = #{id} &lt;/select&gt;&lt;/mapper&gt; 如果要想使用延迟加载策略，那么映射文件配置就不能采用连接查询了，因为这样一旦SQL语句执行了，关联数据也就查询出来了，所以我们要将原来的关联查询，转换成单表查询： 12345678910111213&lt;mapper namespace=\"com.tjd.spring_mybatis_plus.mapper.ArticleMapper\"&gt; &lt;resultMap id=\"articleMap\" type=\"Article\"&gt; &lt;id column=\"id\" property=\"id\"&gt;&lt;/id&gt; &lt;result column=\"title\" property=\"title\"&gt;&lt;/result&gt; &lt;result column=\"content\" property=\"content\"&gt;&lt;/result&gt; &lt;collection property=\"tags\" ofType=\"Tag\" column=\"id\" fetchType=\"lazy\" select=\"com.tjd.spring_mybatis_plus.mapper.TagMapper.getTagsByArticleId\" &gt;&lt;/collection&gt; &lt;/resultMap&gt; &lt;select id=\"getArticleById\" resultMap=\"articleMap\" parameterType=\"long\"&gt; select * from article where id=#{id} &lt;/select&gt;&lt;/mapper&gt; collection标签属性: ofType：用于指定集合元素的数据类型。 select：指定用于查询关联表数据SQL语句的ID。 column：是用于指定使用哪个字段的值作为条件查询。 在TagMapper.xml映射文件中定义如下语句： 1234&lt;!--在前面定义的ArticleMapper.xml映射文件中collection标签中select属性就是指向的这个SQL语句--&gt;&lt;select id=\"getTagsByArticleId\" parameterType=\"long\" resultType=\"Article\"&gt; select * from tag where article_id=#{article_id}&lt;/select&gt; 做到这里，一对多的延迟加载配置就完成了，在执行ArticleMapper中ID为getArticleById的SQL语句时，并不会立即执行TagMapper中的getTagsByArticleId，而是在需要时再执行getTagsByArticleId，从而达到了延迟加载的目的。 配置TagMapper.xml映射文件（一对一 或 多对一 配置）1234567891011121314151617&lt;mapper namespace=\"com.tjd.spring_mybatis_plus.mapper.TagMapper\"&gt; &lt;resultMap id=\"tagMap\" type=\"Tag\"&gt; &lt;id column=\"id\" property=\"id\"&gt;&lt;/id&gt; &lt;result column=\"content\" property=\"content\"&gt;&lt;/result&gt; &lt;association property=\"article\" column=\"article_id\" javaType=\"Article\" fetchType=\"lazy\" select=\"com.tjd.spring_mybatis_plus.mapper.ArticleMapper.getArticleById\"&gt;&lt;/association&gt; &lt;/resultMap&gt; &lt;!--并没有采用关联查询--&gt; &lt;select id=\"getTagById\" parameterType=\"long\" resultMap=\"tagMap\"&gt; select * from tag where id=#{id} &lt;/select&gt; &lt;select id=\"getTagsByArticleId\" parameterType=\"long\" resultMap=\"Article\"&gt; select * from tag where article_id=#{article_id} &lt;/select&gt;&lt;/mapper&gt; association标签属性: ofType：用于指定集合元素的数据类型。 select：指定用于查询关联表数据SQL语句的ID。 第三步：测试1234567891011121314151617@RunWith(SpringJUnit4ClassRunner.class)@ContextConfiguration(\"classpath:applicationContext.xml\")public class LazyLoadTest { @Autowired private ArticleMapper articleMapper; @Autowired private TagMapper tagMapper; @Test public void test(){ Tag tag = tagMapper.getTagById(1L); Article article = tag.getArticle(); List&lt;Tag&gt; tags = article.getTags(); }} 如果想要验证延迟加载策略，我们推荐采用Debug，开启MyBatis SQL日志功能，然后每执行一条语句就会发现控制台输出一条SQL语句，这就表明它的关联数据是延迟查询的。","link":"/article/13726.html"},{"title":"MyBatis动态代理开发的理解","text":"MyBatis动态代理开发时我们开发中常用的一种形式，但是他与传统MyBatis开发有哪些区别呢？而导致这些区别的原因是什么呢？ MyBatis非动态代理开发在日常开发中我们或许都使用动态代理的方式，但是动态代理开发并不是MyBatis最基本的开发模式： 传统开发流程第一步：在resource/目录下新建SqlMapConfig.xml文件：1234567891011121314151617181920&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;!DOCTYPE configuration PUBLIC \"-//mybatis.org//DTD Config 3.0//EN\" \"http://mybatis.org/dtd/mybatis-3-config.dtd\"&gt;&lt;configuration&gt; &lt;environments default=\"development\"&gt; &lt;environment id=\"development\"&gt; &lt;transactionManager type=\"JDBC\"/&gt; &lt;dataSource type=\"POOLED\"&gt; &lt;property name=\"driver\" value=\"com.mysql.jdbc.Driver\"/&gt; &lt;property name=\"url\" value=\"jdbc:mysql://localhost:3306/test\"/&gt; &lt;property name=\"username\" value=\"root\"/&gt; &lt;property name=\"password\" value=\"980613\"/&gt; &lt;/dataSource&gt; &lt;/environment&gt; &lt;/environments&gt; &lt;mappers&gt; &lt;mapper resource=\"com/tjd/mapper/UserMapper.xml\"&gt;&lt;/mapper&gt; &lt;/mappers&gt;&lt;/configuration&gt; 第二步：在com.tjd.mapper包下创建映射文件：12345678910&lt;?xml version=\"1.0\" encoding=\"UTF-8\" ?&gt;&lt;!DOCTYPE mapper PUBLIC \"-//mybatis.org//DTD Mapper 3.0//EN\" \"http://mybatis.org/dtd/mybatis-3-mapper.dtd\" &gt;&lt;mapper namespace=\"com.tjd.mapper.UserMapper\"&gt; &lt;select id=\"getUserById\" resultType=\"com.tjd.pojo.User\" parameterType=\"long\"&gt; select * from user where uid = #{id} &lt;/select&gt; &lt;select id=\"getUserByName\" resultType=\"com.tjd.pojo.User\" parameterType=\"string\" &gt; select * from user where uname = #{name} &lt;/select&gt;&lt;/mapper&gt; 走到这一步，MyBatis环境就搭建完成了，此时我们就需要开始进行数据库操作： 第三步：使用MyBatis提供的API进行数据库操作123456789public class MyBatisTest { @Test public void test() throws IOException { InputStream resource = Resources.getResourceAsStream(\"SqlMapConfig.xml\"); SqlSessionFactory sessionFactory = new SqlSessionFactoryBuilder().build(resource); SqlSession session = sessionFactory.openSession(); Object o = session.selectOne(\"com.tjd.mapper.UserMapper.getUserById\",1L); }} 理解传统开发流程​ 在非动态代理开发时，并不需要写什么Mapper接口，只需要配置映射文件，然后获取的SqlSeesion对象，通过该对象的selectOne或selectList方法进行数据库查询即可，而这些方法只需要传入SQL语句的ID即可执行相应的SQL语句，然后将结果映射为实体类。（注：通常我们会在ID前面加上映射文件的namespace，但是如果ID没有重复的情况下，是可以直接通过ID调用SQL语句）。 ​ 需要注意的是，不进行动态代理开发时，在SqlMapConfig文件中只能使用&lt;mapper resource&gt;或&lt;mapper url&gt;进行映射文件的加载。而由于使用resource指定的映射文件位置，所以映射文件实际上放在哪都无所谓，只要resource指向映射文件即可。 ​ 通常在这种开发模式中，我们仍然会创建Dao层代码，对数据库操作进行封装，但是其本质并没有变，就是MyBatis框架将映射文件的SQL语句加载进系统，然后通过获取的SqlSession对象执行指定的SQL语句，所以在这种开发模式中，映射配置文件的位置、文件名、namespace、statement_id的填写格式并没有强制要求，唯一的要求就是每一个namespace+id可以唯一定位一个SQL语句。 MyBatis动态代理开发在传统开发中，我们需要指定映射文件配置的SQL语句的ID才能执行SQL语句： 1User user = (User)session.selectOne(\"com.tjd.mapper.UserMapper.getUserById\",1L); 诚然，这种方式能够正常工作，并且对于使用旧版本 MyBatis 的用户来说也比较熟悉。不过现在有了一种更简洁的方式 ——动态代理开发，这种开发模式不仅可以执行更清晰和类型安全的代码，而且还不用担心易错的字符串字面值以及强制类型转换。 动态代理开发流程第一步：创建Mapper接口但是如果想要进行动态代理开发就需要满足动态代理开发的要求了，在传统开发模式中有没有Mapper接口都无所谓，但是由于采用的是动态代理开发，所以必须要一个接口： 12345package com.tjd.mapper;public interface UserMapper { User getUserById(Long id); User getUserByName(String name);} 第二步：创建映射配置文件12345678910&lt;?xml version=\"1.0\" encoding=\"UTF-8\" ?&gt;&lt;!DOCTYPE mapper PUBLIC \"-//mybatis.org//DTD Mapper 3.0//EN\" \"http://mybatis.org/dtd/mybatis-3-mapper.dtd\" &gt;&lt;mapper namespace=\"com.tjd.mapper.UserMapper\"&gt; &lt;select id=\"getUserById\" resultType=\"com.tjd.pojo.User\" parameterType=\"long\"&gt; select * from user where uid = #{id} &lt;/select&gt; &lt;select id=\"getUserByName\" resultType=\"com.tjd.pojo.User\" parameterType=\"string\"&gt; select * from user where uname = #{name} &lt;/select&gt;&lt;/mapper&gt; 使用动态代理开发时，映射文件的namespace必须是Mapper接口的全限定类名，而SQL语句的ID必须与接口中的方法名一一对应。但是映射文件名和映射文件的位置并没有强制要求，具体细节在后面讲解。 第三步：配置MyBatis核心配置文件1234567891011121314151617181920212223242526272829&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;!DOCTYPE configuration PUBLIC \"-//mybatis.org//DTD Config 3.0//EN\" \"http://mybatis.org/dtd/mybatis-3-config.dtd\"&gt;&lt;configuration&gt; &lt;settings&gt; &lt;setting name=\"logImpl\" value=\"STDOUT_LOGGING\" /&gt; &lt;/settings&gt; &lt;typeAliases&gt; &lt;package name=\"com.tjd.spring_mybatis_plus.pojo\"/&gt; &lt;/typeAliases&gt; &lt;environments default=\"development\"&gt; &lt;environment id=\"development\"&gt; &lt;transactionManager type=\"JDBC\"/&gt; &lt;dataSource type=\"POOLED\"&gt; &lt;property name=\"driver\" value=\"com.mysql.jdbc.Driver\"/&gt; &lt;property name=\"url\" value=\"jdbc:mysql://localhost:3306/test\"/&gt; &lt;property name=\"username\" value=\"root\"/&gt; &lt;property name=\"password\" value=\"980613\"/&gt; &lt;/dataSource&gt; &lt;/environment&gt; &lt;/environments&gt; &lt;mappers&gt; &lt;mapper resource=\"com/tjd/spring_mybatis_plus/mapper/UserMapper.xml\"&gt;&lt;/mapper&gt; &lt;!--&lt;mapper class=\"com.tjd.spring_mybatis_plus.mapper.UserMapper\"&gt;&lt;/mapper&gt;--&gt; &lt;/mappers&gt;&lt;/configuration&gt; 在使用动态代理开发时，指定映射文件位置可以有四种模式：&lt;mapper resource&gt;、&lt;mapper url&gt;、&lt;mapper class&gt;、&lt;package&gt;，但是这四种配置模式对映射文件名和映射文件位置有着不同的影响，具体来说： 在使用&lt;mapper resource&gt;、&lt;mapper url&gt;时，映射文件可以放在任意地方，映射文件名可以任意取； 在使用&lt;mapper class&gt;和&lt;package&gt;时，映射文件必须与Mapper接口处于相同的包中，并且映射文件名和接口名相同。 第四步：获取代理对象执行数据库操作1234567891011public class MyBatisTest { @Test public void test() throws IOException { InputStream resource = Resources.getResourceAsStream(\"SqlMapConfig.xml\"); SqlSessionFactory sessionFactory = new SqlSessionFactoryBuilder().build(resource); SqlSession session = sessionFactory.openSession(); //获取代理对象 UserMapper mapper = session.getMapper(UserMapper.class); User userById = mapper.getUserById(1L); }} 理解MyBatis动态代理开发​ 虽说是动态代理开发，但是实质的原理非常简单，就是MyBatis生成代理对象，在代理对象的方法中，会自动根据当前执行的方法名，然后定位到映射配置文件中配置的SQL语句。这就避免了我们使用字符串常量来调用SQL语句了，也避免了最后结果的强制类型转换。 ​ 在这种开发模式中，对映射配置文件有相应的要求： Mapper接口方法名和Mapper.xml中定义的SQL语句的ID一一对应。 映射配置文件的namespace与接口的全限定类名相同。 Mapper接口方法的输入参数类型和mapper.xml中定义的每个sql 的parameterType的类型相同。 Mapper接口方法的输出参数类型和mapper.xml中定义的每个sql的resultType的类型相同。 但是对于映射配置文件位置和文件名的要求，取决于采用哪种模式加载配置文件(resource、url、class、package)。 Spring与MyBatis结合的动态代理开发参考博主的另一篇文章：MyBatis与Spring整合步骤","link":"/article/30374.html"},{"title":"MyBatis两级缓存机制详解","text":"​ 缓存是提高软硬件系统性能的一种重要手段；硬件层面，现代先进CPU有三级缓存，而MyBatis也提供了缓存机制，通过缓存机制可以大大提高我们查询性能。 一级缓存​ Mybatis对缓存提供支持，但是在没有配置的默认情况下，它只开启一级缓存，一级缓存只是相对于同一个SqlSession而言，一级缓存又叫本地缓存。所以在参数和SQL完全一样的情况下，我们使用同一个SqlSession对象调用一个Mapper方法，往往只执行一次SQL，因为使用SelSession第一次查询后，MyBatis会将其放在缓存中，以后再查询的时候，如果没有声明需要刷新，并且缓存没有超时的情况下，SqlSession都会取出当前缓存的数据，而不会再次发送SQL到数据库。 为什么要使用一级缓存，不用多说也知道个大概。但是还有几个问题我们要注意一下。 一级缓存的生命周期 MyBatis在开启一个数据库会话时，会创建一个新的SqlSession对象，SqlSession对象中会有一个新的Executor对象。Executor对象中持有一个新的PerpetualCache对象；当会话结束时，SqlSession对象及其内部的Executor对象还有PerpetualCache对象也一并释放掉。 如果SqlSession调用了close()方法，会释放掉一级缓存PerpetualCache对象，一级缓存将不可用。 如果SqlSession调用了clearCache()，会清空PerpetualCache对象中的数据，但是该对象仍可使用。- SqlSession中执行了任何一个update操作(update()、delete()、insert()) ，都会清空PerpetualCache对象的数据，但是该对象可以继续使用。 如何判断两次查询是完全相同的呢mybatis认为，对于两次查询，如果以下条件都完全一样，那么就认为它们是完全相同的两次查询。 传入的statementId 查询时要求的结果集中的结果范围 这次查询所产生的最终要传递给JDBC java.sql.Preparedstatement的Sql语句字符串（boundSql.getSql() ） 传递给java.sql.Statement要设置的参数值 一级缓存的测试123456789101112public class FirstCachedTest { @Test public void test() throws Exception { InputStream resource = Resources.getResourceAsStream(\"SqlMapConfig.xml\"); SqlSessionFactory sessionFactory = new SqlSessionFactoryBuilder().build(resource); SqlSession session = sessionFactory.openSession(); ArticleMapper mapper = session.getMapper(ArticleMapper.class); Article article1 = mapper.getArticleById(1L); Article article2 = mapper.getArticleById(1L); System.out.println(article1 == article2); //输出true }} 执行结果： 需要注意的是，这是在单独使用MyBatis时进行的以及缓存测试，如果MyBatis与Spring整合，那么MyBatis的一级缓存可能会失效，详情参见 https://blog.csdn.net/ctwy291314/article/details/81938882 二级缓存​ MyBatis的二级缓存是Application级别的缓存，它可以提高对数据库查询的效率，以提高应用的性能。 SqlSessionFactory层面上的二级缓存默认是不开启的，二级缓存的开启需要进行配置，实现二级缓存的时候，MyBatis要求返回的POJO必须是可序列化的。 二级缓存的配置步骤第一步：配置SqlMapConfig.xml（可省略）我们实质上需要在全局配置文件中开启配置文件中的所有映射器已经配置的任何缓存，也就是cacheEnabled属性，但是这个属性默认值为true，所以实际上我们可以省略该步骤。 123&lt;settings&gt; &lt;setting name=\"cacheEnabled\" value=\"true\"/&gt;&lt;/settings&gt; 第二步：配置映射文件cache标签配置若要要启用全局的二级缓存，只需要在你的 SQL 映射文件中添加一行： 1&lt;cache/&gt; 这个简单语句的效果如下: 映射语句文件中的所有 select 语句的结果将会被缓存。 映射语句文件中的所有 insert、update 和 delete 语句会刷新缓存。 缓存会使用最近最少使用算法（LRU, Least Recently Used）算法来清除不需要的缓存。 缓存不会定时进行刷新（也就是说，没有刷新间隔）。 缓存会保存列表或对象（无论查询方法返回哪种）的 1024 个引用。 缓存会被视为读/写缓存，这意味着获取到的对象并不是共享的，可以安全地被调用者修改，而不干扰其他调用者或线程所做的潜在修改。 cache标签的属性： eviction：设置缓存的清除策略，默认值为LRU LRU – 最近最少使用：移除最长时间不被使用的对象。 FIFO – 先进先出：按对象进入缓存的顺序来移除它们。 SOFT – 软引用：基于垃圾回收器状态和软引用规则移除对象。 WEAK – 弱引用：更积极地基于垃圾收集器状态和弱引用规则移除对象。 flushInterval：（刷新间隔）属性可以被设置为任意的正整数，设置的值应该是一个以毫秒为单位的合理时间量。 默认情况是不设置，也就是没有刷新间隔，缓存仅仅会在调用语句时刷新。 size：（引用数目）属性可以被设置为任意正整数，要注意欲缓存对象的大小和运行环境中可用的内存资源。默认值是 1024。 readOnly：（只读）属性可以被设置为 true 或 false（默认值）。只读的缓存会给所有调用者返回缓存对象的相同实例。 因此这些对象不能被修改。这就提供了可观的性能提升。而可读写的缓存会（通过反序列化）返回缓存对象的拷贝。 速度上会慢一些，但是更安全，因此默认值是 false。 SQL语句标签配置​ 缓存的配置和缓存实例会被绑定到 SQL 映射文件的命名空间中。 因此，同一命名空间中的所有语句和缓存将通过命名空间绑定在一起。每条语句可以自定义与缓存交互的方式，或将它们完全排除于缓存之外，这可以通过在每条语句上使用两个简单属性来达成。 默认情况下，语句会这样来配置： 1234&lt;select ... flushCache=\"false\" useCache=\"true\"/&gt;&lt;insert ... flushCache=\"true\"/&gt;&lt;update ... flushCache=\"true\"/&gt;&lt;delete ... flushCache=\"true\"/&gt; ​ 鉴于这是默认行为，显然你永远不应该以这样的方式显式配置一条语句。但如果你想改变默认的行为，只需要设置 flushCache 和 useCache 属性。比如，某些情况下你可能希望特定 select 语句的结果排除于缓存之外，或希望一条 select 语句清空缓存。类似地，你可能希望某些 update 语句执行时不要刷新缓存。 123&lt;select id=\"getArticleById\" resultMap=\"articleMap\" parameterType=\"long\" useCache=\"true\" flushCache=\"false\"&gt; select * from article where id = #{id}&lt;/select&gt; 第三步：让实体类实现Serializable接口由于&lt;cache/&gt;标签readOnly标签默认是false，所以MyBatis在读写缓存是通过序列化与反序列化完成的。 二级缓存测试12345678910111213141516public class FirstCachedTest { @Test public void test() throws Exception { InputStream resource = Resources.getResourceAsStream(\"SqlMapConfig.xml\"); SqlSessionFactory sessionFactory = new SqlSessionFactoryBuilder().build(resource); SqlSession session = sessionFactory.openSession(); ArticleMapper mapper = session.getMapper(ArticleMapper.class); Article article1 = mapper.getArticleById(1L); session.close(); //开启一个新的SqlSession，在不同的SqlSession中才会读取二级缓存（全局缓存） session=sessionFactory.openSession(); ArticleMapper mapper2 = session.getMapper(ArticleMapper.class); Article article2 = mapper2.getArticleById(1L); System.out.println(article1 == article2); }} 执行结果： 二级缓存需要注意的地方​ 对于查询多commit少且用户对查询结果实时性要求不高，此时采用mybatis二级缓存技术降低数据库访问量，提高访问速度。但不能滥用二级缓存，二级缓存也有很多弊端，从MyBatis默认二级缓存是关闭的就可以看出来。二级缓存是建立在同一个namespace下的，如果对表的操作查询可能有多个namespace，那么得到的数据就是错误的。 ​ 举例来说：文章和标签，ArticleMapper、TagMapper。在查询文章时我们需要把文章对应的标签也查询出来，那么这个标签信息被二级缓存在ArticleMapper对应的namespace下，这个时候有人要修改Tag的基本信息，那就是在TagMapper的namespace下修改，他是不会影响到ArticleMapper的缓存的，那么你再次查找文章数据时，拿到的是缓存的数据，这个数据其实已经是过时的。 二级缓存数据过期问题测试Article和Tag是一对多的关系，其中Aticle是一，Tag是多。 ArticleMapper 123456789101112&lt;mapper namespace=\"com.tjd.spring_mybatis_plus.mapper.ArticleMapper\"&gt; &lt;cache /&gt; &lt;resultMap id=\"articleMap\" type=\"Article\"&gt; &lt;id column=\"id\" property=\"id\"&gt;&lt;/id&gt; &lt;result column=\"title\" property=\"title\"&gt;&lt;/result&gt; &lt;result column=\"content\" property=\"content\"&gt;&lt;/result&gt; &lt;collection property=\"tags\" ofType=\"Tag\" column=\"id\" select=\"com.tjd.spring_mybatis_plus.mapper.TagMapper.getTagsByArticleId\"&gt;&lt;/collection&gt; &lt;/resultMap&gt; &lt;select id=\"getArticleById\" resultMap=\"articleMap\" parameterType=\"long\" useCache=\"true\"&gt; select * from article where id = #{id} &lt;/select&gt;&lt;/mapper&gt; TagMapper 1234567891011121314&lt;mapper namespace=\"com.tjd.spring_mybatis_plus.mapper.TagMapper\"&gt; &lt;cache/&gt; &lt;resultMap id=\"tagMap\" type=\"Tag\"&gt; &lt;id column=\"id\" property=\"id\"&gt;&lt;/id&gt; &lt;result column=\"content\" property=\"content\"&gt;&lt;/result&gt; &lt;association property=\"article\" column=\"article_id\" javaType=\"Article\" select=\"com.tjd.spring_mybatis_plus.mapper.ArticleMapper.getArticleById\"&gt;&lt;/association&gt; &lt;/resultMap&gt; &lt;update id=\"updateTag\" parameterType=\"Tag\"&gt; update tag set content=#{content} where id=#{id} &lt;/update&gt; &lt;select id=\"getTagsByArticleId\" parameterType=\"long\" resultMap=\"tagMap\"&gt; select * from tag where article_id=#{article_id} &lt;/select&gt;&lt;/mapper&gt; 测试代码 123456789101112131415161718192021222324@RunWith(SpringJUnit4ClassRunner.class)@ContextConfiguration(\"classpath:applicationContext.xml\")public class SecondCachedErrorTest { @Autowired private ArticleMapper articleMapper; @Autowired private TagMapper tagMapper; @Test public void test() throws IOException { //第一次查询，查询出来的aticle对象被缓存在ArticleMapper的namespace下 Article article = articleMapper.getArticleById(1L); Tag tag = article.getTags().get(0); tag.setContent(\"dasdas\"); //更新Tag，那么TagMapper下的二级缓存被刷新（清空） tagMapper.updateTag(tag); //再次查询Article，此时获得是缓存数据，而关联的tag数据已经过时 Article article2 = articleMapper.getArticleById(1L); //TagMapper对应的namespace下的缓存由于在更新时被刷新（清空），所以查询的结果是正确的 List&lt;Tag&gt; tags = tagMapper.getTagsByArticleId(1L); }} ​ 根据以上测试，我们明白想要使用二级缓存时需要想好两个问题： 对该表的操作与查询都在同一个namespace下，其他的namespace如果有操作，就会发生缓存数据过期的问题。 对关联表的查询，关联的所有表的操作都必须在同一个namespace。 在有多表查询的情况下建议不使用二级缓存。 两级缓存的优先级如果两级缓存同时开启，那么二级缓存比一级缓存优先级高，也就是在执行数据库查询操作时，优先读取二级缓存中的内容。 文章参考至： https://blog.csdn.net/ctwy291314/article/details/81938882 https://www.cnblogs.com/happyflyingpig/p/7739749.html https://www.cnblogs.com/yuluoxingkong/p/8205858.html http://www.mybatis.org/mybatis-3/zh/sqlmap-xml.html","link":"/article/51583.html"},{"title":"Maven到底是什么","text":"Maven 是一个项目管理工具，它最主要的两个功能就是：依赖管理和项目构建。 何为依赖管理​ 在传统项目中，我们的项目如果需要第三方提供的库就必须得去官网上下载，有了Maven我们只需要在pom文件中配置对应库的坐标，Maven则会自动的去中央仓库下载对应的第三方库，这就是Maven的依赖管理。 何为项目构建​ 依赖管理可能比较好理解，那到底什么是项目构建呢？ ​ 在工作中，除了需要编写源代码以外，我们每天有相当一部分时间花在了项目编译、运行单元测试、生成文档、打包和部署等繁琐且不起眼的工作上，这就是构建。如果我们还去手工的完成这一部分工作，那么效率就太低了，Maven的出现使得这一整套的动作向一条流水线一样，通过一个简单的命令就能完成。 Maven的核心思想约定优于配置（convention over configuration），Maven通过超级pom约定了很多通用的配置（例如：目录结构），这样做省去了我们繁琐的配置，我们只需要按照Maven的约定进行项目的创建即可。 超级POM的位置1${Maven_Home}/lib/maven-model-builder-3.3.9.jar/org/apache/maven/model/pom4.0.0.xml 上面截取的是超级POM的一部分，它定义了Maven项目的目录结构、编译后的输出目录、资源目录等。 Maven的目录结构 坐标和依赖何为坐标在一个平面中只要给定一个直角坐标系，所有的点都能通过一个（x,y）来表示；同样的Maven也需要定义一种坐标形式来定位众多的构件。 项目坐标定义任何一个构件都必须明确定义自己的坐标，而坐标是通过以下元素来定义的：groupId、artifactId、version、packaging、classfier。 groupId：定义当前Maven项目隶属的实际项目。首先Maven项目和实际项目并不是一一对应的，因为有些大的项目会分成很多模块，例如SpringFramework项目（实际项目）被分成spring-core、spring-beans、spring-context等模块。 1234&lt;groupId&gt;org.springframework&lt;/groupId&gt;&lt;artifactId&gt;spring-beans&lt;/artifactId&gt;&lt;version&gt;5.1.9.RELEASE&lt;/version&gt;&lt;packaging&gt;jar&lt;/packaging&gt; artifactId：该元素定义实际项目中的一个Maven项目（模块） version：该元素定义的Maven项目所处的版本 packaging：该元素定义Maven项目的打包方式，当不定义packaging时Maven会使用默认值jar。 classfier：该元素用于定义构件输出的一些附属构件。附属构件与主构件对应，例如主构件是nexus-indexer-2.0.0.jar的项目还可以通过使用一些插件来生成例如nexus-indexer-2.0.0-javadoc.jar、nexus-indexer-2.0.0-sources.jar这样一些附属构件。 在上面五个元素中，groupId、artifactId、version是必须定义的，而packaging是可选的（默认为jar），而classfier是不能直接定义的。 依赖的配置依赖管理时Maven的两大核心功能之一，它定义了一套成熟的依赖管理模式： 123456789101112131415&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;...&lt;/groupId&gt; &lt;artifactId&gt;...&lt;/artifactId&gt; &lt;version&gt;...&lt;/version&gt; &lt;type&gt;...&lt;/type&gt; &lt;scope&gt;...&lt;/scope&gt; &lt;optional&gt;...&lt;/optional&gt; &lt;exclusions&gt; &lt;exclusion&gt;....&lt;/exclusion&gt; ... &lt;/exclusions&gt; &lt;/dependency&gt; ...&lt;/dependencies&gt; groupId、artifactId、version：依赖的基本坐标，对任何一个依赖来说，基本坐标是最重要的。 type：依赖的类型，对应项目坐标定义中的packaging属性。 scope：依赖的范围。 optional：标记依赖是否可选 exclusions：用于排除传递性依赖。 依赖范围​ Maven在编译主代码的时候会使用一套classpath；其次Maven在编译和执行测试的时候会使用另外一套classpath；最后，实际运行Maven项目的时候，又会使用一套classpath。而依赖范围就是用来控制依赖于这三种classpath（编译classpath、测试classpath、运行classpath）。 ​ Maven有以下几种依赖范围： compile：编译依赖范围，如果未指定scope属性，那么该依赖默认范围是compile。compile也是我们项目中运用最多的依赖范围。 test：测试依赖范围，只在测试classpath中有效，在编译主代码时不能使用该此依赖。 provided：已提供依赖范围，例如servlet-api，我们将项目部署到服务器时，服务器会自动的提供这些依赖环境，但是我们在编译时又必须使用该依赖，否则编译不了，所以就有了provided依赖范围。 runtime：运行时依赖范围，有些依赖我们只需要在运行时加进去就可以了，例如MySQL驱动，由于MySQL驱动完全是按照java提供的规范编写，所有在Java自带的API中有对应的接口，我们面向接口编程不需要显示调用驱动中的代码，所以在代码编译时就不需要提供。 system：系统依赖范围。该依赖与三种classpath的关系和provided依赖范围完全一致。但是使用system范围的依赖时必须通过systemPath元素显示的指定依赖文件的路径。由于此依赖不是通过Maven仓库解析的，，而且往往与本机系统弄个绑定，可能造成构件的不可移植，因此需谨慎使用。 1234567&lt;dependency&gt; &lt;groupId&gt;javax.sql&lt;/groupId&gt; &lt;artifactId&gt;jdbc-stdext&lt;/artifactId&gt; &lt;version&gt;2.0&lt;/version&gt; &lt;scope&gt;system&lt;/scope&gt; &lt;systemPath&gt;${java.home}/lib/rt.jar&lt;/systemPath&gt;&lt;/dependency&gt; 依赖的传递何为依赖传递Maven的依赖传递大大减轻了我们对项目依赖文件的管理难度，如果不使用Maven如果我们要在自己的项目中搭建Spring环境，那么我们就需要去下载Spring的jar包，引入到本地项目中，但是Spring仍然需要依赖其它第三方的jar包，此时我们就必须查阅相关文档手动引入Spring所依赖的环境，这个非常繁琐。 Maven的依赖传递机制就很好的解决了这一问题，如果你在pom文件中引入spring-core那么Maven就会将spring-core和spring-core所依赖的环境都引入到工程中。 依赖范围影响依赖的传递​ 依赖范围不仅可以控制依赖于三种classpath的关系，还对传递性依赖产生影响。假设A依赖于B，B依赖于C，我们说A对于B是第一直接依赖，B对于C是第二直接依赖，A对于C是间接依赖。 ​ 下图中，最左边一列代表第一直接依赖，最上面一行代表第二直接依赖： ​ 见上图我们可发现这样一个规律：当第二直接依赖为compile时，传递依赖与第一直接依赖一致；当第二直接依赖为test时，依赖不会得到传递；当第二直接依赖是provided时，只传递第一直接依赖为provided的依赖，且传递依赖依然为provided；当第二直接依赖是runtime时，传递依赖除compile以外与第一传递依赖一致。 ​ 表格的具体理解： ​ 对于这张图，A-&gt;B和A-&gt;D就是第一直接依赖，B-&gt;C和D-&gt;E是第二直接依赖。第一和第二的概念是相对于现在所处的项目来说的。假如A项目使我们此时正在进行的项目，B、D都是本项目直接依赖的jar，那么C、E分别是B、D的直接依赖，那他们是否会随着B和D的引入而引入A项目呢？ ​ 根据表格得知，compile-test是无效的，也就是C根本不会被引入到A项目中；而compile-runtime是runtime也就是说E会被引入A项目，并且在A项目中E的scope是runtime。 依赖仲裁三原则 版本声明原则 ​ 优先按照依赖管理&lt;dependencyManagement&gt;元素中指定的版本声明进行仲裁，此时下面的两个原则都无效了。 最短路径原则 ​ 在此种依赖关系下，gupao-web到底依赖的是1.1版本还是1.0版本的gupao-common-lib。 ​ 根据最短路径原则，在gupao-web依赖的是1.0版本的，Maven会自动过滤掉1.1版本的gupao-common-lib 最先声明原则 若最短路径原则无法做出仲裁，那么就需要使用最先声明原则了，在这个案例中，gupao-common-lib到底依赖哪个版本取决于，gupao-biz和gupao-dal谁先声明。 依赖冲突何为依赖冲突大多数的依赖冲突发生的原因是因为maven的传递依赖会引入很多隐式的依赖，这些依赖可能会和我们显示依赖版本不一致。 如图，我们显示依赖了 spring-boot1.5.9，和spring-core4.0.8（当然这种情况在正常情况下不会发生）在这种情况，根据Maven的最短依赖路径原则，会使用spring-core4.0.8,当在启动项目的时候会报错。这是因为spring-boot1.5.9运行所需要的spring-core版本是4.3.13，但是项目中编译的spring-core版本是4.0.8。 ​ Maven冲突的实质是：不同版本的jar中会有部分API不一样，例如A依赖B的1.4版本中的某些新特性，但是系统根据仲裁法则选择了1.3版本的B，此时A所依赖的新特性用不了，这就导致项目无法运行。 如何解决依赖冲突​ 冲突导致项目无法运行的原因是因为系统按照“三大依赖仲裁法则”留下的jar包的版本不适用于所有需求者，那么我们需要通过仲裁法则或者标签来解决冲突。 解决冲突的最终目的就是将我们需要的版本留下来，让系统忽略不兼容的版本。 ​ 解决依赖有两大种方法： 利用依赖仲裁三原则选出我们需要的版本 版本声明原则：我们在&lt;dependencyManagement&gt;锁定版本号。 最短路径原则：我们直接在项目中显示引入需要的jar包版本即可。 优先声明原则：我们将需要的版本放在前面声明 利用&lt;exclution&gt;标签添加排除 在IDEA中就会显示servlet-api冲突，（omitted for duplicate：省略重复的） ​ 在当前这个情况中，我们以添加进行排除 12345678910111213141516&lt;dependency&gt; &lt;groupId&gt;javax.servlet&lt;/groupId&gt; &lt;artifactId&gt;servlet-api&lt;/artifactId&gt; &lt;scope&gt;provided&lt;/scope&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;javax.servlet&lt;/groupId&gt; &lt;artifactId&gt;jsp-api&lt;/artifactId&gt; &lt;scope&gt;provided&lt;/scope&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;javax.servlet&lt;/groupId&gt; &lt;artifactId&gt;servlet-api&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt;&lt;/dependency&gt;","link":"/article/22321.html"},{"title":"Mybatis中各种数据的映射类型","text":"Mybatis对应的java和数据库的数据类型： 文章转载至：异或随心 https://www.cnblogs.com/zhuangfei/p/9492915.html","link":"/article/49653.html"},{"title":"Maven生命周期与插件","text":"除了坐标、依赖以及仓库以外，Maven另外两个核心概念是生命周期和插件。 什么是生命周期​ Maven生命周期是为了对所有的构建过程进行抽象和统一。Maven从大量项目和构建工具中学习和反思，然后总结了一套高度完善的、易扩展的生命周期。这个生命周期包含了项目的清理、初始化、编译、测试、打包、集成测试、验证部署和站点生成等几乎所有步骤。也就是说，几乎所有项目的构建，都能映射到这样一个生命周期中上。 ​ Maven的生命周期是一个抽象的概念，这意味着生命周期本身不做任何实际的工作，在Maven的设计中，实际的任务（如编译源码）都交给插件来完成。这种设计思想与设计模式中模板模式（Template Pattern）非常类似。 1234567891011121314151617public abstract class AbstrctBuild { public void build() { initialize(); compile(); test(); packagee(); integrationTest(); deploy(); } protected abstract void initialize(); protected abstract void compile(); protected abstract void test(); protected abstract void packagee(); protected abstract void integrationTest(); protected abstract void deploy();} ​ 模板模式在父类中定义了算法的整体结构，子类可以通过实现或者重写父类的方法来控制实际的行为，这样既保证了算法有足够的扩展性，又能够严格控制算法的整体结构。 ​ 由于生命周期只是对项目构建过程的一种抽象，那么项目构建还是需要实际的程序来构建，这就引入了插件的概念。在Maven的生命周期中，每个构建步骤都可以绑定一个或多个插件行为，而且Maven为绝大多数构建步骤都提供了默认的插件绑定，这样大多数用户在使用Maven时不需要进行任何的插件配置即可完成项目的构建工作。 生命周期详解三套生命周期​ Maven拥有三套相互独立的生命周期，他们分别是：clean、default、site。clean生命周期的目的是清理项目，default生命周期的目的是为了构建项目，而site生命周期的目的是建立项目站点。 ​ Maven生命周期包含一些阶段（phase），这些阶段是有序的，并且后面的阶段依赖于前面的阶段，用户和Maven最直接的交互方式就是调用这些生命周期阶段。以clean生命周期为例，它包含的阶段有pre-clean、clean、post-clean。当用户调用pre-clean时只有pre-clean这个阶段会执行；当用户调用clean阶段时，pre-clean和clean阶段会得以执行；当用户调用post-clean阶段时，clean生命周期的三个阶段都会执行。 ​ 较之于生命周期阶段的前后依赖关系，三套生命周期本身是相互独立的。用户仅调用clean生命周期的某个阶段并不会对其他生命周期产生影响。例如用户调用clean生命周期中的clean阶段时，并不会触发default以及Site生命周期中的任何阶段。 clean生命周期clean生命周期的目的是清理项目，它包含三个阶段： clean生命周期阶段 任务 pre-clean 执行一些清理前需要完成的操作 clean 清理上一次构建生成的文件 post-clean 执行一些清理后需要完成的操作 default生命周期​ default生命周期定义了真正构建时所需要执行的所有步骤，它是三套生命周期中最核心的部分，default生命周期包含以下阶段： validate initialize generate-sources process-sources：处理项目的主资源文件。一般来说，是对src/main/resources目录内的内容进行变量替换等工作后，复制到项目输出的主classpath下。 generate-resources process-resources compile：编译项目的主源码。一般来说，是编译src/main/java目录下的Java文件至项目输出的主classpath中。 process-classes generate-test-sources process-test-sources：处理项目测试的资源文件。 generate-test-resources process-test-resources test-compile：编译项目的测试代码 process-test-classes test：使用单元测试框架进行测试，测试代码不会被打包或部署 prepare-package package：接受编译好的代码，将其打包为可发布的格式，如JAR pre-integration-test integration-test post-integration-test verify install：将打包好的文件安装到本地仓库中，方便其他Maven项目使用。 deploy：将最终的包复制到远程仓库，供其他开发人员或Maven项目使用 site生命周期​ site生命周期的目的是建立和发布项目的站点，Maven能够基于POM所包含的信息，自动生成一个友好的站点，方便团队交流和发布项目信息。该生命周期包含如下阶段： site生命周期阶段 任务 pre-site 执行一些在生成项目站点之前需要完成的工作 site 生成项目站点 post-site 执行一些在生成项目站点之后需要完成的工作 site-deploy 将生成的项目站点发布到服务器上 Maven命令与生命周期​ 从命令行执行Maven任务的最主要方式就是调用Maven的生命周期阶段。下面我们就以一些常用的命令为例，解释其执行的生命周期阶段： mvn clean：调用clean生命周期中clean阶段，实际执行的阶段是pre-clean和clean阶段。 mvn test：该命令调用default生命周期test阶段。实际上执行的是default生命周期中validate到test的所有阶段 mvn clean install：该命令调用clean生命周期的clean阶段，以及default生命周期中的install阶段。实际执行的阶段为clean生命周期的pre-clean、clean阶段，以及default生命周期的从validate至install的所有阶段。 插件目标​ 对于插件本身，为了能够复用代码，他往往能够完成多个任务。例如maven-dependency-plugin，它能够基于项目依赖做很多事情。它能够分析项目的依赖，帮助找到潜在的无用依赖；它能够列出项目的依赖树；帮助分析依赖来源；它能够列出项目所有已解析的依赖，等等。为每个这样的功能编写一个插件是很不可取的，因为这些任务背后有很多可以复用的代码，因此，这些功能聚集在一个插件里，每个功能就是一个插件目标。 ​ maven-dependency-plugin有十多个目标，每个目标对应一个功能，上述几个功能分别对应的插件目标为dependency:analyze、dependency:tree、dependency:list。这是一种通用的写法，冒号前面写插件的前缀，冒号后面是该插件的目标。 插件绑定​ Maven的生命周期与插件相互绑定，用以完成实际的构建任务。具体而言，是生命周期的阶段与插件的目标相互绑定，以完成某个具体的构建任务。例如项目编译这一任务，它对应了default生命周期的compile阶段，而maven-dependency-plugin插件的compile目标能够完成该任务。因此，将它们绑定，就能实现项目编译的目的。 内置绑定为了让用户几乎不用任何配置就能构架Maven项目，Maven默认为一些主要的生命周期阶段绑定了对应插件目标，具体的绑定见下表： clean生命周期： 生命周期阶段 插件目标 pre-clean — clean maven-clean-plugin:clean post-clean — site生命周期： 生命周期阶段 插件目标 pre-site — site maven-site-plugin:site post-site — site-deploy maven-site-plugin:deploy default生命周期： 生命周期阶段 插件目标 执行任务 process-resources maven-resources-plugin:resources 复制主资源文件至主输出目录 compile maven-compiler-plugin:compile 编译主代码至主输出目录 process-test-resources maven-resources-plugin:testResources 复制测试资源文件至测试输出目录 test-compile maven-compiler-plugin:testCompile 编译测试代码值测试输出目录 test maven-surefire-plugin:test 执行测试用例 package maven-jar-plugin:jar 将项目打包为jar，如果打包为war则不是该插件目标 install maven-install-plugin:install 将项目输出构建安装到本地仓库 deploy maven-deploy-plugin:deploy 将项目输出构建部署到远程仓库 ​ 读者可以从Maven命令行输出中看到项目构建过程中执行了哪些插件目标，例如我们执行 mvn clean package命令输出如下信息： 自定义绑定​ 除了内置绑定以外，用户还能够自己选择将某个插件目标绑定到生命周期的某个阶段上。 ​ 一个常见的例子是创建项目的源码JAR包，内置的插件绑定关系中并没有涉及到当前任务，因此需要用户自行配置。maven-source-plugin可以帮助我们完成这个任务，它的jar-no-fork目标能够将项目的主代码打包成jar文件，我们可以将其绑定到default生命周期的verify阶段上，在执行完成测试后和安装构件前创建源码jar包： 123456789101112131415161718&lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-source-plugin&lt;/artifactId&gt; &lt;version&gt;2.1.1&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;attach-sources&lt;/id&gt; &lt;phase&gt;verify&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;jar-no-fork&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;/build&gt; ​ groupId、artifactId、version仍然是定位插件的基本坐标，而&lt;executions&gt;是用于插件执行配置，executions下每一个execution子元素可以用来配置一个任务。该例中配置了一个id为attach-sources的任务，通过phase配置，将其绑定到verify生命周期阶段上，再通过&lt;goals&gt;配置指定要执行的插件目标。 ​ 需要指出的是：有时候即使不指定phase元素配置生命周期阶段，插件目标仍能绑定到生命周期中去。这是因为有些插件的目标在编写时已经定义了默认的生命周期阶段。 ​ 我们都知道，当插件绑定到不同的生命周期阶段中时，它的执行顺序由生命周期阶段顺序决定；如果多个目标被绑定到同一个生命周期阶段，那么它们生命的先后顺序就决定了执行顺序。 插件配置​ 完成插件和生命周期的绑定后，用户还可以配置插件目标的参数，进一步调整插件目标所执行的任务，以满足项目需求。几乎所有Maven插件的目标都有一些可配置的参数，用户可以通过命令行和POM配置等方式来配置这些参数。 命令行插件的配置​ 在日常的Maven使用中，我们会经常从命令行输入并执行Maven命令，用户可以在Maven命令中使用-D参数，并伴随一个“参数键=参数值”的形式，来配置插件目标的参数。 ​ 例如，maven-surefire-plugin提供了一个maven.test.skip参数，当其值为true的时候，就会跳过执行测试。 1$ mvn install -Dmaven.test.skip=true ​ 参数-D是Java自带的，其功能是通过命令行设置一个Java系统属性，Maven简单的重用了该参数，在准备插件时检查系统属性，便实现了插件参数配置。 POM中插件全局配置​ 并不是所有参数都适合用命令行参数配置，有些参数不会经常改变，甚至根本不会改变，若每次使用maven命令都带上参数，显得特别繁琐。此时我们可以在POM中进行插件的全局配置。 ​ 用户可以在声明插件时，对插件进行一个全局配置。也就是说所有基于该插件目标的任务，都会使用这些配置，例如maven-compiler-plugin告诉它编译Java1.5版本的源文件，生成JVM1.5兼容的字节码文件： 12345678910&lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;version&gt;3.7.0&lt;/version&gt; &lt;configuration&gt; &lt;source&gt;1.8&lt;/source&gt; &lt;target&gt;1.8&lt;/target&gt; &lt;encoding&gt;UTF-8&lt;/encoding&gt; &lt;/configuration&gt;&lt;/plugin&gt; 使用命令行调用插件​ 有些插件不适合绑定到生命周期上，例如maven-dependency-plugin插件，它使用来分析项目的依赖的插件，我们不需要在构建项目时显示依赖树，我们只需要在需要的时候调用它即可。从命令行调用插件的格式如下： 1$ mvn [options] [&lt;goal(s)&gt;] [&lt;phase(s)&gt;] ​ 就拿maven-dependency-plugin为例： 1$ mvn org.apache.maven.plugins:maven-dependency-plugin:2.1:tree ​ 这条命令非常清晰，里面写明了插件的groupId、artifactId、version、goal，它的意思是调用maven-dependency-plugin 2.1版本插件的tree目标，显示项目的依赖树。 ​ 但是上面这条命令显得过于繁琐，我们可以执行下列命令，到达同样的效果： 1$ mvn dependency:tree ​ dependency是什么呢？它是看上去不是groupId、artifactId、version、goal中任意一个，这里引入目标前缀（Goal Prefix）的概念，dependency就是maven-dependency-plugin的前缀，有了目标前缀，Maven就能找到对应插件的artifactId。 查看插件的描述信息​ 前面我们提到了目标前缀的概念，我们到底如何查看插件的目标前缀呢？我们可以借助maven-help-plugin插件来实现这个任务。 12# 注意：在描述插件时可以省略版本信息，让Maven自动获取最新版本来进行表述$ mvn help:describe -D plugin=org.apache.maven.plugins:maven-dependency-plugin:2.1","link":"/article/56692.html"},{"title":"IDEA炫酷插件推荐","text":"好的插件可以大大提升我们的开发效率，下面我就推荐几款好用的插件。 Key Promotor XIDEA很多操作都是有快捷键的，但是我们无法短时间记住所有的快捷键，安装一个key Promoter X插件，它可以提示你每次操作对应的快捷键。 Maven Helper 此插件可用来方便显示maven的依赖树，在没有此插件时，如果想看maven的依赖树需要输入命令行： mvn dependency:tree 才可查看依赖。如果想看是否有依赖包冲突的话也需要输入命令行等等的操作。而如果安装Maven Helper插件就可免去命令行困扰。通过界面即可操作完成。 安装这个插件后还可以右击项目名，来运行命令： TranslationTranslation插件是一个翻译插件，有时候我们在查看源码时，英文注释某些单词不熟悉直接划词翻译，不用另外打开其他的翻译插件： 该插件默认使用的是Google翻译，可以配置国内的有道翻译和百度翻译，翻译内容会更加详细。 Rainbow Brackets用于对括号进行着色： 高亮效果(默认快捷键 mac : command+鼠标右键单击, windows : ctrl+鼠标右键单击)： 选中部分外暗淡效果（默认快捷键 alt+鼠标右键单击）： MyBatisX可以在Mapper.java和Mapper.xml互相跳转，并在编写接口方法后，生成配置文件对应的标签： MyBatis Log PluginMybatis现在是java中操作数据库的首选，在开发的时候，我们都会把Mybatis的脚本直接输出在console中，但是默认的情况下，输出的脚本不是一个可以直接执行的。MyBatis Log Plugin 这款插件是直接将Mybatis执行的sql脚本显示出来，无需处理，可以直接复制出来执行的。 安装完后默认不会显示Mybatis Log框，我们只需要进行如下操作: Grep Console根据日志不同等级信息进行上色： 其它插件Lombok Plugin Lombok 功能辅助插件 （省略Getter和Setter方法） GsonFormat 把 JSON 字符串转换为Java类 RestfulToolkit 根据 URL 直接跳转到对应的方法定义等功能","link":"/article/24820.html"},{"title":"真正理解Maven的聚合与继承","text":"在这个技术飞速发展的时代，软件开发的工程量也与日俱增，分模块开发的概念也逐渐成为了主流。分模块开发带来了更好的设计思想，以及更高的代码重用性。 分模块开发​ 以一个简单的博客系统为例，在传统开发中我们将所有代码写在同一个项目中，实际上我们可以将一个WEB项目分为三个子模块： ​ 一般来说，一个项目的子模块应该使用同样的groupId，如果他们一起开发和发布，还应该使用同样的version，此外，它们的artifactId还应该使用相同的前缀，以方便同其他项目区分。 blog-web的pom.xml文件： 123456789&lt;project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.wennry.blog&lt;/groupId&gt; &lt;artifactId&gt;blog-web&lt;/artifactId&gt; &lt;packaging&gt;war&lt;/packaging&gt; &lt;name&gt;Blog Dao&lt;/name&gt; &lt;version&gt;1.0.0-SNAPSHOT&lt;/version&gt;&lt;/project&gt; blog-service的pom.xml文件： 123456789&lt;project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.wennry.blog&lt;/groupId&gt; &lt;artifactId&gt;blog-service&lt;/artifactId&gt; &lt;packaging&gt;jar&lt;/packaging&gt; &lt;name&gt;Blog Service&lt;/name&gt; &lt;version&gt;1.0.0-SNAPSHOT&lt;/version&gt;&lt;/project&gt; blog-dao的pom.xml文件： 12345678910&lt;project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.wennry.blog&lt;/groupId&gt; &lt;artifactId&gt;blog-dao&lt;/artifactId&gt; &lt;packaging&gt;jar&lt;/packaging&gt; &lt;!--在使用Maven命令进行构建，输出时会以name属性输出信息，使得输出更加直观--&gt; &lt;name&gt;Blog Dao&lt;/name&gt; &lt;version&gt;1.0.0-SNAPSHOT&lt;/version&gt;&lt;/project&gt; ​ 此时三个项目相互独立，如果我们此时想要一次构建三个项目该怎么办呢，而不是分别执行三次Maven命令。Maven聚合就是为这一任务诞生的。 聚合​ 为了能够实现一条命令构建三个模块的效果，我们需要创建一个额外的名为blog-aggregator的模块，然后通过该模块构建整个项目的所有模块。blog-aggregator本身是一个Maven项目，它必须要有自己的POM，同时作为一个聚合项目，其POM文件又有特殊的地方。如下为blog-aggregator的POM： 1234567891011121314&lt;project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.wennry.blog&lt;/groupId&gt; &lt;artifactId&gt;blog-aggregator&lt;/artifactId&gt; &lt;packaging&gt;pom&lt;/packaging&gt; &lt;name&gt;Blog Aggregator&lt;/name&gt; &lt;version&gt;1.0.0-SNAPSHOT&lt;/version&gt; &lt;modules&gt; &lt;module&gt;blog-web&lt;/module&gt; &lt;module&gt;blog-service&lt;/module&gt; &lt;module&gt;blog-dao&lt;/module&gt; &lt;/modules&gt;&lt;/project&gt; ​ 对于聚合项目来说，其打包方式packaging的值必须为pom，不然无法构建。用户在POM中通过任意数量的module元素来实现模块聚合。这里每个module的值都是一个当前POM的相对目录。例如blog-aggregator的POM路径为：D:/…./blog-aggregator/pom.xml，那么blog-web的目录就为：D:/…./blog-aggregator/blog-web/。blog-web里面又是一个完整的Maven项目结构。离开聚合项目后依然可以独立构建。 ​ 一般来说，为了方便定位内容，artifactId应与当前目录名称一致，不过这不是Maven的要求，用户可以将blog-web项目放在web-blog目录下，这时聚合项目的配置就应该改为&lt;module&gt;web-blog&lt;/module&gt;。 ​ 为了方便用户构建项目，通常将聚合项目放在最顶层，其他模块则作为聚合项目的子目录存在。但是这仍然不是Maven的规定，我们仍然可以使用平行结构，但是聚合模块的POM也需要做出相应的修改： 12345&lt;modules&gt; &lt;module&gt;../blog-web&lt;/module&gt; &lt;module&gt;../blog-service&lt;/module&gt; &lt;module&gt;../blog-dao&lt;/module&gt;&lt;/modules&gt; 继承​ 聚合解决了分模块开发需要多次构建的问题，但是分模块开发中还有一个问题需要解决，那就是POM重复配置的问题，例如web、service、dao三个模块中groupId、version以及各模块所依赖的JAR以及插件都有一定的重复。重复往往就意味着更多的劳动和更多的潜在问题。在面向对象的世界中可以使用类继承在一定程度上消除重复。在Maven的世界中，也有类似的继承机制让我们抽出重复的配置。 定义父模块​ 我们需要创建一个父POM来对重复配置进行抽取，以实现一次声明多处使用的目的。我们继续以上面的项目结构为基础，在blog-aggregator下创建一个名为blog-parent的子目录，在blog-parent目录下创建pom.xml。由于父模块只是为了帮助消除配置的重复，因此他本并不包含除POM之外的项目文件，也就不需要src/main/java之类的文件夹了。 123456789&lt;project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.wennry.blog&lt;/groupId&gt; &lt;artifactId&gt;blog-parent&lt;/artifactId&gt; &lt;packaging&gt;pom&lt;/packaging&gt; &lt;name&gt;Blog Parent&lt;/name&gt; &lt;version&gt;1.0.0-SNAPSHOT&lt;/version&gt;&lt;/project&gt; 此时整个项目的目录结构如下： 有了父模块之后我们需要让子模块继承父模块，我们就以blog-web模块为例： 12345678910111213&lt;project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;parent&gt; &lt;groupId&gt;com.wennry.blog&lt;/groupId&gt; &lt;artifactId&gt;blog-parent&lt;/artifactId&gt; &lt;version&gt;1.0.0-SNAPSHOT&lt;/version&gt; &lt;relativePath&gt;../blog-parent/pom.xml&lt;/relativePath&gt; &lt;/parent&gt; &lt;artifactId&gt;blog-web&lt;/artifactId&gt; &lt;packaging&gt;war&lt;/packaging&gt; &lt;name&gt;Blog Dao&lt;/name&gt;&lt;/project&gt; ​ 在子模块中我们需要使用parent标签指定父模块，以达到继承的效果。需要注意的是，relativePath表示父模块POM的相对路径。当项目构件时，Maven会首先根据relativePath检查父POM，如果找不到再去本地仓库查找。relativePath的默认值为../pom.xml。 ​ 正确设置relativePath非常重要。考虑这一情况，开发团队的新成员从源码库迁出一个包含父模块关系的Maven项目。由于只关心其中的某一个子模块是，它就直接到该模块的目录下进行构建，但是此时父模块并没有安装到本地仓库，因此如果子模块没有正确设置relativePath属性，Maven无法找到父POM，这将直接导致构建失败。 ​ 在子模块中如果继承了父模块，那么通常就不要定义groupId、version了，它会默认继承父POM的两个属性。 可继承的POM元素在上面我们提到了groupId、version是可以被继承的，那么还有哪些元素时可以被继承的呢？ 元素 作用 groupId 项目组ID version 项目版本 description 项目描述信息 organization 项目的组织信息 inceptionYear 项目的创始年份 url 项目的URL developers 项目的开发者信息 contributors 项目的贡献者信息 distributionManagement 项目的部署配置 issueManagement 项目的缺陷跟踪信息 ciManagement 项目的持续集成系统信息 scm 项目的版本控制系统信息 mailingLists 项目的邮件列表信息 properties 自定义Maven属性 dependencies 项目的依赖配置 dependencyManagement 项目的依赖管理配置 reporsitories 项目的仓库配置 build 包括项目的源码目录配置、输出目录配置、插件管理配置 reporting 包括项目的报告输出目录配置、报告插件配置等 ​ 依赖管理​ dependencies元素时可以继承的，所以我们可以将公有的依赖（例如Spring的依赖）在父POM中定义，但是这样又有一个问题，我们不能保证以后所有的项目都会依赖这些JAR，例如我们根据需求产生了blog-pojo模块，它专门用于对数据库进行实体封装。它就不需要Spring的依赖，如果直接在父POM中定义dependencies是不合理的。 ​ Maven提供了dependencyManagement元素既能然子模块继承父模块的依赖配置，又能保证子模块依赖使用的灵活性。在dependencyManagement元素下的依赖声明并不会引入实际的JAR，不过它能够约束dependencies下的依赖使用。dependencyManagement声明依赖能够统一项目范围中依赖的版本，当依赖版本在父POM中声明以后，子模块声明依赖时无需声明版本，也就不会发生多个模块使用依赖版本不一致的问题。 在blog-parent的POM进行依赖管理： 12345678910111213141516171819&lt;project&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.wennry.blog&lt;/groupId&gt; &lt;artifactId&gt;blog-parent&lt;/artifactId&gt; &lt;packaging&gt;pom&lt;/packaging&gt; &lt;name&gt;Blog Parent&lt;/name&gt; &lt;version&gt;1.0.0-SNAPSHOT&lt;/version&gt; &lt;properties&gt; &lt;spring.version&gt;2.5.6&lt;/spring.version&gt; &lt;/properties&gt; &lt;dependencyManagement&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-core&lt;/artifactId&gt; &lt;version&gt;${spring.version}&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencyManagement&gt;&lt;/project&gt; blog-web中继承父POM的声明信息，就不需要声明依赖版本号： 12345678910111213141516171819&lt;project&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;parent&gt; &lt;groupId&gt;com.wennry.blog&lt;/groupId&gt; &lt;artifactId&gt;blog-parent&lt;/artifactId&gt; &lt;version&gt;1.0.0-SNAPSHOT&lt;/version&gt; &lt;relativePath&gt;../blog-parent/pom.xml&lt;/relativePath&gt; &lt;/parent&gt; &lt;artifactId&gt;blog-web&lt;/artifactId&gt; &lt;packaging&gt;war&lt;/packaging&gt; &lt;name&gt;Blog Dao&lt;/name&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-core&lt;/artifactId&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/project&gt; 插件管理​ 类似地，Maven也提供了pluginManagement元素帮助管理插件。在该元素中配置插件并不会造成实际的插件调用行为，当POM中配置了真正的plugin元素，并且其groupId和artifactId与pluginManagement中配置的插件匹配时，pluginManagement的配置才会实际影响插件的行为。 父POM中进行插件管理： 1234567891011121314151617181920&lt;build&gt; &lt;pluginManagement&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-source-plugin&lt;/artifactId&gt; &lt;version&gt;2.1.1&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;attach-sources&lt;/id&gt; &lt;phase&gt;verify&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;jar-no-fork&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/pluginManagement&gt;&lt;/build&gt; 当子模块需要生成源码包时候，只需要进行如下简单的配置： 12345678&lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-source-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;/build&gt; ​ 当项目中的多个模块有同样的插件配置时，就应当使用pluginManagement进行统一管理。即使各个模块对于同一个插件的具体不尽相同，也应当使用父POM的pluginManagement统一插件版本。 聚合和继承的关系​ 初学者容易将聚合和继承搞混，要说关系他们其实没有任何关系，聚合主要是为了方便快速的构建项目，继承主要是为了消除一些重复的配置，这也就是为什么笔者在上面的案例中，刻意将聚合项目和用于继承的父项目分开的原因，而在日常开发中我们将通常将聚合项目与被继承的父项目放在一起，这样做容易混淆大家对聚合与继承的概念区分。 ​ 对于聚合模块来说，它知道有哪些子模块被聚合，但那些子模块并不知道自己被别人“聚合”了。 ​ 对于继承关系的父POM来说，它不知道有哪些子模块继承于它，但那些子模块都必须知道自己的父POM是什么。 ​ 如果非要说这两个特性的共同点，那么可以看到，聚合POM与继承关系中的父POM的packaging都必须是pom，同时，聚合模块与继承关系中的父模块除了POM之外都没有实际的内容。","link":"/article/17488.html"}],"tags":[{"name":"clone方法","slug":"clone方法","link":"/tags/clone方法/"},{"name":"LinkedHashMap","slug":"LinkedHashMap","link":"/tags/LinkedHashMap/"},{"name":"HashMap","slug":"HashMap","link":"/tags/HashMap/"},{"name":"Tampermonkey插件","slug":"Tampermonkey插件","link":"/tags/Tampermonkey插件/"},{"name":"事务","slug":"事务","link":"/tags/事务/"},{"name":"单例模式","slug":"单例模式","link":"/tags/单例模式/"},{"name":"设计模式","slug":"设计模式","link":"/tags/设计模式/"},{"name":"委派模式","slug":"委派模式","link":"/tags/委派模式/"},{"name":"策略模式","slug":"策略模式","link":"/tags/策略模式/"},{"name":"模板模式","slug":"模板模式","link":"/tags/模板模式/"},{"name":"原型模式","slug":"原型模式","link":"/tags/原型模式/"},{"name":"代理模式","slug":"代理模式","link":"/tags/代理模式/"},{"name":"一致性型Hash","slug":"一致性型Hash","link":"/tags/一致性型Hash/"},{"name":"B-树","slug":"B-树","link":"/tags/B-树/"},{"name":"B+树","slug":"B-树","link":"/tags/B-树/"},{"name":"Git","slug":"Git","link":"/tags/Git/"},{"name":"Git内部原理","slug":"Git内部原理","link":"/tags/Git内部原理/"},{"name":"PullRequest","slug":"PullRequest","link":"/tags/PullRequest/"},{"name":"分支","slug":"分支","link":"/tags/分支/"},{"name":"Git标签","slug":"Git标签","link":"/tags/Git标签/"},{"name":"Git工具","slug":"Git工具","link":"/tags/Git工具/"},{"name":"Git远程仓库","slug":"Git远程仓库","link":"/tags/Git远程仓库/"},{"name":"Spring事务","slug":"Spring事务","link":"/tags/Spring事务/"},{"name":"MyBatis","slug":"MyBatis","link":"/tags/MyBatis/"},{"name":"Spring","slug":"Spring","link":"/tags/Spring/"},{"name":"缓存","slug":"缓存","link":"/tags/缓存/"},{"name":"Maven","slug":"Maven","link":"/tags/Maven/"},{"name":"IDEA插件","slug":"IDEA插件","link":"/tags/IDEA插件/"}],"categories":[{"name":"Java基础","slug":"Java基础","link":"/categories/Java基础/"},{"name":"科技生活","slug":"科技生活","link":"/categories/科技生活/"},{"name":"数据库","slug":"数据库","link":"/categories/数据库/"},{"name":"设计模式","slug":"设计模式","link":"/categories/设计模式/"},{"name":"数据结构","slug":"数据结构","link":"/categories/数据结构/"},{"name":"Git","slug":"Git","link":"/categories/Git/"},{"name":"Git内部原理","slug":"Git/Git内部原理","link":"/categories/Git/Git内部原理/"},{"name":"Git分支","slug":"Git/Git分支","link":"/categories/Git/Git分支/"},{"name":"Github","slug":"Git/Github","link":"/categories/Git/Github/"},{"name":"Git基础","slug":"Git/Git基础","link":"/categories/Git/Git基础/"},{"name":"Git工具","slug":"Git/Git工具","link":"/categories/Git/Git工具/"},{"name":"Git远程仓库","slug":"Git/Git远程仓库","link":"/categories/Git/Git远程仓库/"},{"name":"起步","slug":"Git/起步","link":"/categories/Git/起步/"},{"name":"常用框架","slug":"常用框架","link":"/categories/常用框架/"},{"name":"工具使用","slug":"工具使用","link":"/categories/工具使用/"},{"name":"Spring","slug":"常用框架/Spring","link":"/categories/常用框架/Spring/"},{"name":"MyBatis","slug":"常用框架/MyBatis","link":"/categories/常用框架/MyBatis/"},{"name":"Maven","slug":"工具使用/Maven","link":"/categories/工具使用/Maven/"},{"name":"IDEA","slug":"工具使用/IDEA","link":"/categories/工具使用/IDEA/"}]}